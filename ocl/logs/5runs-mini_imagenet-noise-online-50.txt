Namespace(num_runs=5, seed=0, val_size=0.1, num_val=3, num_runs_val=3, error_analysis=True, verbose=True, store=False, save_path=None, agent='ER', update='random', retrieve='MIR', optimizer='SGD', learning_rate=0.1, epoch=1, batch=10, test_batch=128, weight_decay=0, num_tasks=10, fix_order=False, plot_sample=False, data='mini_imagenet', cl_type='ni', ns_factor=[0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6], ns_type='noise', ns_task=(1, 1, 2, 2, 2, 2), online=True, mem_size=5000, eps_mem_batch=10, lambda_=100, alpha=0.9, fisher_update_after=50, subsample=50, gss_mem_strength=10, gss_batch_size=10, k=5, aser_type='asvm', n_smp_cls=2.0, stm_capacity=1000, classifier_chill=0.01, log_alpha=-300, minlr=0.0005, clip=10.0, mem_epoch=70, labels_trick=False, separated_softmax=False, kd_trick=False, kd_trick_star=False, review_trick=False, ncm_trick=False, mem_iters=1, min_delta=0.0, patience=0, cumulative_delta=False, temp=0.07, buffer_tracker=False, warmup=4, head='mlp', budget=0.5, cuda=True)
Setting up data stream
data setup time: 1.5712227821350098
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 0 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.366927, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.513958, running mem acc: 0.100
[0.009 0.011 0.01  0.015 0.009 0.017 0.008 0.009 0.011 0.008]
no ratio: 0.0
on ratio: 0.0
[(0, 991, 0, 0)]
[-0.02334323337674141]
[0]
[nan]
[-7.530926814069971e-05]
[nan]
[-0.001238350523635745]
-----------run 0 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.725953, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.122943, running mem acc: 0.550
[0.013 0.016 0.016 0.016 0.022 0.017 0.015 0.012 0.017 0.02 ]
no ratio: 0.0
on ratio: 0.9998986931415257
[(0, 991, 0, 0), (0, 984, 0, 987)]
[-0.02334323337674141, -0.03905413690209389]
[0, nan]
[nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05]
[nan, nan]
[-0.001238350523635745, -0.0012383506400510669]
-----------run 0 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.523331, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.519404, running mem acc: 0.250
[0.009 0.012 0.009 0.012 0.016 0.018 0.007 0.011 0.014 0.009]
no ratio: 0.0
on ratio: 0.9999494719822142
[(0, 991, 0, 0), (0, 984, 0, 987), (0, 991, 0, 1979)]
[-0.02334323337674141, -0.03905413690209389, -0.005985401742160321]
[0, nan, nan]
[nan, nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05, -7.53093117964454e-05]
[nan, nan, nan]
[-0.001238350523635745, -0.0012383506400510669, -0.0012383508728817105]
-----------run 0 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.167481, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.695930, running mem acc: 0.300
[0.01  0.016 0.01  0.012 0.017 0.019 0.009 0.012 0.013 0.011]
no ratio: 0.0
on ratio: 0.9999662629465943
[(0, 991, 0, 0), (0, 984, 0, 987), (0, 991, 0, 1979), (0, 988, 0, 2964)]
[-0.02334323337674141, -0.03905413690209389, -0.005985401742160321, -0.008337552703917027]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05, -7.53093117964454e-05, -7.530943548772484e-05]
[nan, nan, nan, nan]
[-0.001238350523635745, -0.0012383506400510669, -0.0012383508728817105, -0.0012383514549583197]
-----------run 0 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.205442, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.343518, running mem acc: 0.100
[0.01  0.017 0.015 0.012 0.013 0.018 0.011 0.019 0.009 0.008]
no ratio: 0.0
on ratio: 0.9999746585236056
[(0, 991, 0, 0), (0, 984, 0, 987), (0, 991, 0, 1979), (0, 988, 0, 2964), (0, 987, 0, 3946)]
[-0.02334323337674141, -0.03905413690209389, -0.005985401742160321, -0.008337552703917027, -0.012134575270116328]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05, -7.53093117964454e-05, -7.530943548772484e-05, -7.530942821176723e-05]
[nan, nan, nan, nan, nan]
[-0.001238350523635745, -0.0012383506400510669, -0.0012383508728817105, -0.0012383514549583197, -0.0012383514549583197]
-----------run 0 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.925653, running train acc: 0.100
==>>> it: 1, mem avg. loss: 3.260471, running mem acc: 0.200
[0.007 0.018 0.01  0.009 0.006 0.011 0.015 0.007 0.012 0.005]
no ratio: 0.0
on ratio: 0.9999797983879113
[(0, 991, 0, 0), (0, 984, 0, 987), (0, 991, 0, 1979), (0, 988, 0, 2964), (0, 987, 0, 3946), (0, 989, 0, 4950)]
[-0.02334323337674141, -0.03905413690209389, -0.005985401742160321, -0.008337552703917027, -0.012134575270116328, 0.16928542268276214]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05, -7.53093117964454e-05, -7.530943548772484e-05, -7.530942821176723e-05, -7.530947186751291e-05]
[nan, nan, nan, nan, nan, nan]
[-0.001238350523635745, -0.0012383506400510669, -0.0012383508728817105, -0.0012383514549583197, -0.0012383514549583197, -0.0012383514549583197]
-----------run 0 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.422665, running train acc: 0.050
==>>> it: 1, mem avg. loss: 4.375006, running mem acc: 0.050
[0.022 0.022 0.022 0.028 0.025 0.023 0.018 0.024 0.021 0.03 ]
no ratio: 0.0
on ratio: 0.9999829296188183
[(0, 991, 0, 0), (0, 984, 0, 987), (0, 991, 0, 1979), (0, 988, 0, 2964), (0, 987, 0, 3946), (0, 989, 0, 4950), (0, 982, 0, 5858)]
[-0.02334323337674141, -0.03905413690209389, -0.005985401742160321, -0.008337552703917027, -0.012134575270116328, 0.16928542268276214, -0.007167766790837049]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05, -7.53093117964454e-05, -7.530943548772484e-05, -7.530942821176723e-05, -7.530947186751291e-05, -7.530953735113144e-05]
[nan, nan, nan, nan, nan, nan, nan]
[-0.001238350523635745, -0.0012383506400510669, -0.0012383508728817105, -0.0012383514549583197, -0.0012383514549583197, -0.0012383514549583197, -0.0012383516877889633]
-----------run 0 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.546732, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.694364, running mem acc: 0.150
[0.016 0.023 0.017 0.016 0.025 0.029 0.019 0.02  0.018 0.022]
no ratio: 0.0
on ratio: 0.9999854123207539
[(0, 991, 0, 0), (0, 984, 0, 987), (0, 991, 0, 1979), (0, 988, 0, 2964), (0, 987, 0, 3946), (0, 989, 0, 4950), (0, 982, 0, 5858), (0, 980, 0, 6855)]
[-0.02334323337674141, -0.03905413690209389, -0.005985401742160321, -0.008337552703917027, -0.012134575270116328, 0.16928542268276214, -0.007167766790837049, -0.021293604522943498]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05, -7.53093117964454e-05, -7.530943548772484e-05, -7.530942821176723e-05, -7.530947186751291e-05, -7.530953735113144e-05, -7.530959555879235e-05]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.001238350523635745, -0.0012383506400510669, -0.0012383508728817105, -0.0012383514549583197, -0.0012383514549583197, -0.0012383514549583197, -0.0012383516877889633, -0.001238351920619607]
-----------run 0 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.854026, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.194403, running mem acc: 0.100
[0.008 0.007 0.007 0.006 0.017 0.005 0.012 0.01  0.013 0.015]
no ratio: 0.0
on ratio: 0.9999873866374036
[(0, 991, 0, 0), (0, 984, 0, 987), (0, 991, 0, 1979), (0, 988, 0, 2964), (0, 987, 0, 3946), (0, 989, 0, 4950), (0, 982, 0, 5858), (0, 980, 0, 6855), (0, 987, 0, 7928)]
[-0.02334323337674141, -0.03905413690209389, -0.005985401742160321, -0.008337552703917027, -0.012134575270116328, 0.16928542268276214, -0.007167766790837049, -0.021293604522943498, -0.0058072455190122125]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05, -7.53093117964454e-05, -7.530943548772484e-05, -7.530942821176723e-05, -7.530947186751291e-05, -7.530953735113144e-05, -7.530959555879235e-05, -7.530963193858042e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.001238350523635745, -0.0012383506400510669, -0.0012383508728817105, -0.0012383514549583197, -0.0012383514549583197, -0.0012383514549583197, -0.0012383516877889633, -0.001238351920619607, -0.0012383521534502506]
-----------run 0 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.852559, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.827393, running mem acc: 0.250
[0.016 0.028 0.024 0.028 0.022 0.023 0.023 0.019 0.015 0.022]
no ratio: 0.0
on ratio: 0.9999886390747662
[(0, 991, 0, 0), (0, 984, 0, 987), (0, 991, 0, 1979), (0, 988, 0, 2964), (0, 987, 0, 3946), (0, 989, 0, 4950), (0, 982, 0, 5858), (0, 980, 0, 6855), (0, 987, 0, 7928), (0, 978, 0, 8802)]
[-0.02334323337674141, -0.03905413690209389, -0.005985401742160321, -0.008337552703917027, -0.012134575270116328, 0.16928542268276214, -0.007167766790837049, -0.021293604522943498, -0.0058072455190122125, -0.006909429401159286]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-7.530926814069971e-05, -7.530929724453017e-05, -7.53093117964454e-05, -7.530943548772484e-05, -7.530942821176723e-05, -7.530947186751291e-05, -7.530953735113144e-05, -7.530959555879235e-05, -7.530963193858042e-05, -7.530963193858042e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.001238350523635745, -0.0012383506400510669, -0.0012383508728817105, -0.0012383514549583197, -0.0012383514549583197, -0.0012383514549583197, -0.0012383516877889633, -0.001238351920619607, -0.0012383521534502506, -0.0012383521534502506]
-----------run 0-----------avg_end_acc 0.022-----------train time 222.62553191184998
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 1 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 7.315961, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.824119, running mem acc: 0.200
[0.016 0.015 0.017 0.008 0.01  0.022 0.012 0.012 0.013 0.011]
no ratio: 0.0
on ratio: 0.0
[(0, 984, 0, 0)]
[-0.015919684797525407]
[0]
[nan]
[-9.303479600930586e-05]
[nan]
[0.001365345437079668]
-----------run 1 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.337512, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.677365, running mem acc: 0.250
[0.008 0.012 0.013 0.011 0.013 0.012 0.008 0.012 0.01  0.01 ]
no ratio: 0.0
on ratio: 0.9998992037093035
[(0, 984, 0, 0), (0, 988, 0, 992)]
[-0.015919684797525407, -0.0068415741994977]
[0, nan]
[nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05]
[nan, nan]
[0.001365345437079668, 0.001365345437079668]
-----------run 1 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.540942, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.804296, running mem acc: 0.150
[0.011 0.01  0.012 0.008 0.013 0.013 0.009 0.007 0.012 0.014]
no ratio: 0.0
on ratio: 0.9999494719822142
[(0, 984, 0, 0), (0, 988, 0, 992), (0, 988, 0, 1979)]
[-0.015919684797525407, -0.0068415741994977, -0.0034521730821579695]
[0, nan, nan]
[nan, nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05, -9.303492697654292e-05]
[nan, nan, nan]
[0.001365345437079668, 0.001365345437079668, 0.001365345437079668]
-----------run 1 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.584449, running train acc: 0.050
==>>> it: 1, mem avg. loss: 4.266122, running mem acc: 0.000
[0.011 0.009 0.012 0.011 0.013 0.013 0.009 0.01  0.013 0.017]
no ratio: 0.0
on ratio: 0.9999663084127893
[(0, 984, 0, 0), (0, 988, 0, 992), (0, 988, 0, 1979), (0, 989, 0, 2968)]
[-0.015919684797525407, -0.0068415741994977, -0.0034521730821579695, -0.014089994594454765]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05, -9.303492697654292e-05, -9.303492697654292e-05]
[nan, nan, nan, nan]
[0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668]
-----------run 1 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.261235, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.300716, running mem acc: 0.050
[0.009 0.011 0.012 0.009 0.011 0.012 0.008 0.005 0.01  0.014]
no ratio: 0.0
on ratio: 0.9999747417342325
[(0, 984, 0, 0), (0, 988, 0, 992), (0, 988, 0, 1979), (0, 989, 0, 2968), (0, 989, 0, 3959)]
[-0.015919684797525407, -0.0068415741994977, -0.0034521730821579695, -0.014089994594454765, -0.011904364496469498]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05]
[nan, nan, nan, nan, nan]
[0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668]
-----------run 1 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.835886, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.938483, running mem acc: 0.000
[0.01  0.014 0.012 0.009 0.011 0.013 0.009 0.007 0.012 0.013]
no ratio: 0.0
on ratio: 0.9999797738718876
[(0, 984, 0, 0), (0, 988, 0, 992), (0, 988, 0, 1979), (0, 989, 0, 2968), (0, 989, 0, 3959), (0, 987, 0, 4944)]
[-0.015919684797525407, -0.0068415741994977, -0.0034521730821579695, -0.014089994594454765, -0.011904364496469498, -0.010716059777885676]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05]
[nan, nan, nan, nan, nan, nan]
[0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668]
-----------run 1 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.089733, running train acc: 0.050
==>>> it: 1, mem avg. loss: 4.724429, running mem acc: 0.000
[0.01  0.011 0.012 0.01  0.012 0.012 0.01  0.006 0.013 0.015]
no ratio: 0.0
on ratio: 0.9999831454045945
[(0, 984, 0, 0), (0, 988, 0, 992), (0, 988, 0, 1979), (0, 989, 0, 2968), (0, 989, 0, 3959), (0, 987, 0, 4944), (0, 990, 0, 5933)]
[-0.015919684797525407, -0.0068415741994977, -0.0034521730821579695, -0.014089994594454765, -0.011904364496469498, -0.010716059777885676, -0.012064749233424663]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05]
[nan, nan, nan, nan, nan, nan, nan]
[0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668]
-----------run 1 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.226270, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.760637, running mem acc: 0.000
[0.01  0.012 0.013 0.01  0.012 0.012 0.008 0.006 0.013 0.015]
no ratio: 0.0
on ratio: 0.9999855556037035
[(0, 984, 0, 0), (0, 988, 0, 992), (0, 988, 0, 1979), (0, 989, 0, 2968), (0, 989, 0, 3959), (0, 987, 0, 4944), (0, 990, 0, 5933), (0, 994, 0, 6923)]
[-0.015919684797525407, -0.0068415741994977, -0.0034521730821579695, -0.014089994594454765, -0.011904364496469498, -0.010716059777885676, -0.012064749233424663, -0.010501733772456646]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05]
[nan, nan, nan, nan, nan, nan, nan, nan]
[0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668]
-----------run 1 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.395588, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.790603, running mem acc: 0.000
[0.01  0.012 0.012 0.008 0.012 0.012 0.008 0.006 0.012 0.013]
no ratio: 0.0
on ratio: 0.9999873738967942
[(0, 984, 0, 0), (0, 988, 0, 992), (0, 988, 0, 1979), (0, 989, 0, 2968), (0, 989, 0, 3959), (0, 987, 0, 4944), (0, 990, 0, 5933), (0, 994, 0, 6923), (0, 988, 0, 7920)]
[-0.015919684797525407, -0.0068415741994977, -0.0034521730821579695, -0.014089994594454765, -0.011904364496469498, -0.010716059777885676, -0.012064749233424663, -0.010501733772456646, -0.009058310031890868]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668]
-----------run 1 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.960889, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.936073, running mem acc: 0.000
[0.011 0.01  0.012 0.01  0.011 0.011 0.008 0.007 0.012 0.016]
no ratio: 0.0
on ratio: 0.999988774261627
[(0, 984, 0, 0), (0, 988, 0, 992), (0, 988, 0, 1979), (0, 989, 0, 2968), (0, 989, 0, 3959), (0, 987, 0, 4944), (0, 990, 0, 5933), (0, 994, 0, 6923), (0, 988, 0, 7920), (0, 984, 0, 8908)]
[-0.015919684797525407, -0.0068415741994977, -0.0034521730821579695, -0.014089994594454765, -0.011904364496469498, -0.010716059777885676, -0.012064749233424663, -0.010501733772456646, -0.009058310031890868, -0.01700439913570881]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-9.303479600930586e-05, -9.303483966505155e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05, -9.303492697654292e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668, 0.001365345437079668]
-----------run 1-----------avg_end_acc 0.010799999999999999-----------train time 229.61856508255005
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 2 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.266582, running train acc: 0.000
==>>> it: 1, mem avg. loss: 1.978112, running mem acc: 0.100
[0.015 0.011 0.012 0.011 0.008 0.006 0.01  0.014 0.011 0.012]
no ratio: 0.0
on ratio: 0.0
[(0, 985, 0, 0)]
[0.01199389473348856]
[0]
[nan]
[8.980821439763531e-05]
[nan]
[-0.003715540748089552]
-----------run 2 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.509680, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.463222, running mem acc: 0.300
[0.007 0.007 0.014 0.012 0.006 0.01  0.007 0.015 0.007 0.012]
no ratio: 0.0
on ratio: 0.9998993052059209
[(0, 985, 0, 0), (0, 993, 0, 993)]
[0.01199389473348856, -0.0011331360191106797]
[0, nan]
[nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05]
[nan, nan]
[-0.003715540748089552, -0.0037155409809201956]
-----------run 2 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.102605, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.337640, running mem acc: 0.250
[0.011 0.009 0.018 0.014 0.011 0.007 0.014 0.016 0.01  0.009]
no ratio: 0.0
on ratio: 0.9999494975001263
[(0, 985, 0, 0), (0, 993, 0, 993), (0, 982, 0, 1980)]
[0.01199389473348856, -0.0011331360191106797, -0.005490877140313387]
[0, nan, nan]
[nan, nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05, 8.98081561899744e-05]
[nan, nan, nan]
[-0.003715540748089552, -0.0037155409809201956, -0.003715540748089552]
-----------run 2 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.822338, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.922871, running mem acc: 0.150
[0.021 0.02  0.024 0.024 0.022 0.005 0.021 0.017 0.019 0.014]
no ratio: 0.0
on ratio: 0.9999659296105755
[(0, 985, 0, 0), (0, 993, 0, 993), (0, 982, 0, 1980), (0, 976, 0, 2935)]
[0.01199389473348856, -0.0011331360191106797, -0.005490877140313387, -0.0035209376867860556]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05, 8.98081561899744e-05, 8.980813436210155e-05]
[nan, nan, nan, nan]
[-0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.003715540748089552]
-----------run 2 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.157027, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.290024, running mem acc: 0.300
[0.014 0.013 0.012 0.011 0.008 0.009 0.011 0.022 0.011 0.009]
no ratio: 0.0
on ratio: 0.9999746841852105
[(0, 985, 0, 0), (0, 993, 0, 993), (0, 982, 0, 1980), (0, 976, 0, 2935), (0, 992, 0, 3950)]
[0.01199389473348856, -0.0011331360191106797, -0.005490877140313387, -0.0035209376867860556, -0.005933883365243673]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05, 8.98081561899744e-05, 8.980813436210155e-05, 8.980809070635587e-05]
[nan, nan, nan, nan, nan]
[-0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.003715540748089552, -0.0037155409809201956]
-----------run 2 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.277910, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.241761, running mem acc: 0.250
[0.011 0.017 0.009 0.017 0.006 0.013 0.014 0.013 0.015 0.008]
no ratio: 0.0
on ratio: 0.9999797574947875
[(0, 985, 0, 0), (0, 993, 0, 993), (0, 982, 0, 1980), (0, 976, 0, 2935), (0, 992, 0, 3950), (0, 987, 0, 4940)]
[0.01199389473348856, -0.0011331360191106797, -0.005490877140313387, -0.0035209376867860556, -0.005933883365243673, 0.0012678096499294043]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05, 8.98081561899744e-05, 8.980813436210155e-05, 8.980809070635587e-05, 8.980808343039826e-05]
[nan, nan, nan, nan, nan, nan]
[-0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.003715540748089552, -0.0037155409809201956, -0.003715540748089552]
-----------run 2 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.192775, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.676518, running mem acc: 0.100
[0.012 0.016 0.013 0.015 0.009 0.01  0.016 0.02  0.014 0.011]
no ratio: 0.0
on ratio: 0.9999831226477189
[(0, 985, 0, 0), (0, 993, 0, 993), (0, 982, 0, 1980), (0, 976, 0, 2935), (0, 992, 0, 3950), (0, 987, 0, 4940), (0, 984, 0, 5925)]
[0.01199389473348856, -0.0011331360191106797, -0.005490877140313387, -0.0035209376867860556, -0.005933883365243673, 0.0012678096499294043, -0.0022695189621299507]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05, 8.98081561899744e-05, 8.980813436210155e-05, 8.980809070635587e-05, 8.980808343039826e-05, 8.980802522273734e-05]
[nan, nan, nan, nan, nan, nan, nan]
[-0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.0037155409809201956]
-----------run 2 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.030552, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.299636, running mem acc: 0.100
[0.025 0.025 0.023 0.02  0.023 0.014 0.026 0.03  0.022 0.018]
no ratio: 0.0
on ratio: 0.9999853888750895
[(0, 985, 0, 0), (0, 993, 0, 993), (0, 982, 0, 1980), (0, 976, 0, 2935), (0, 992, 0, 3950), (0, 987, 0, 4940), (0, 984, 0, 5925), (0, 970, 0, 6844)]
[0.01199389473348856, -0.0011331360191106797, -0.005490877140313387, -0.0035209376867860556, -0.005933883365243673, 0.0012678096499294043, -0.0022695189621299507, 0.0011912692468613387]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05, 8.98081561899744e-05, 8.980813436210155e-05, 8.980809070635587e-05, 8.980808343039826e-05, 8.980802522273734e-05, 8.980802522273734e-05]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.0037155409809201956, -0.0037155412137508392]
-----------run 2 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.990002, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.984578, running mem acc: 0.150
[0.022 0.023 0.021 0.027 0.019 0.01  0.019 0.02  0.02  0.011]
no ratio: 0.0
on ratio: 0.9999872434335574
[(0, 985, 0, 0), (0, 993, 0, 993), (0, 982, 0, 1980), (0, 976, 0, 2935), (0, 992, 0, 3950), (0, 987, 0, 4940), (0, 984, 0, 5925), (0, 970, 0, 6844), (0, 980, 0, 7839)]
[0.01199389473348856, -0.0011331360191106797, -0.005490877140313387, -0.0035209376867860556, -0.005933883365243673, 0.0012678096499294043, -0.0022695189621299507, 0.0011912692468613387, -0.0050051877424120905]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05, 8.98081561899744e-05, 8.980813436210155e-05, 8.980809070635587e-05, 8.980808343039826e-05, 8.980802522273734e-05, 8.980802522273734e-05, 8.980798156699166e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.0037155409809201956, -0.0037155412137508392, -0.0037155416794121265]
-----------run 2 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.753036, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.374133, running mem acc: 0.200
[0.019 0.021 0.02  0.02  0.021 0.006 0.016 0.014 0.015 0.015]
no ratio: 0.0
on ratio: 0.9999886981385834
[(0, 985, 0, 0), (0, 993, 0, 993), (0, 982, 0, 1980), (0, 976, 0, 2935), (0, 992, 0, 3950), (0, 987, 0, 4940), (0, 984, 0, 5925), (0, 970, 0, 6844), (0, 980, 0, 7839), (0, 985, 0, 8848)]
[0.01199389473348856, -0.0011331360191106797, -0.005490877140313387, -0.0035209376867860556, -0.005933883365243673, 0.0012678096499294043, -0.0022695189621299507, 0.0011912692468613387, -0.0050051877424120905, -0.0063878801688551905]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[8.980821439763531e-05, 8.980817801784724e-05, 8.98081561899744e-05, 8.980813436210155e-05, 8.980809070635587e-05, 8.980808343039826e-05, 8.980802522273734e-05, 8.980802522273734e-05, 8.980798156699166e-05, 8.980797429103404e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.003715540748089552, -0.0037155409809201956, -0.003715540748089552, -0.0037155409809201956, -0.0037155412137508392, -0.0037155416794121265, -0.003715542145073414]
-----------run 2-----------avg_end_acc 0.016700000000000003-----------train time 175.40273666381836
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 3 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.758037, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.059593, running mem acc: 0.200
==>>> it: 101, avg. loss: 5.684032, running train acc: 0.004
==>>> it: 101, mem avg. loss: 4.962614, running mem acc: 0.022
==>>> it: 201, avg. loss: 5.488453, running train acc: 0.007
==>>> it: 201, mem avg. loss: 5.034246, running mem acc: 0.017
==>>> it: 301, avg. loss: 5.442267, running train acc: 0.006
==>>> it: 301, mem avg. loss: 5.135401, running mem acc: 0.012
==>>> it: 401, avg. loss: 5.425427, running train acc: 0.006
==>>> it: 401, mem avg. loss: 5.187755, running mem acc: 0.010
[0.012 0.013 0.013 0.011 0.01  0.011 0.008 0.011 0.014 0.013]
no ratio: 0.0
on ratio: 0.0
[(0, 988, 0, 0)]
[0.024855268269777298]
[0]
[nan]
[0.00014464760897681117]
[nan]
[0.000930305162910372]
-----------run 3 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.808862, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.206040, running mem acc: 0.000
==>>> it: 101, avg. loss: 5.363147, running train acc: 0.012
==>>> it: 101, mem avg. loss: 5.281255, running mem acc: 0.017
==>>> it: 301, avg. loss: 5.370817, running train acc: 0.010
==>>> it: 301, mem avg. loss: 5.355500, running mem acc: 0.018
==>>> it: 401, avg. loss: 5.375633, running train acc: 0.010
==>>> it: 401, mem avg. loss: 5.356743, running mem acc: 0.016
[0.011 0.015 0.024 0.014 0.017 0.018 0.009 0.013 0.02  0.014]
no ratio: 0.0
on ratio: 0.99989889798807
[(0, 988, 0, 0), (0, 985, 0, 989)]
[0.024855268269777298, 0.03128104412555695]
[0, nan]
[nan, nan]
[0.00014464760897681117, 0.00014464760897681117]
[nan, nan]
[0.000930305162910372, 0.000930305162910372]
-----------run 3 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.816851, running train acc: 0.050
==>>> it: 1, mem avg. loss: 5.442880, running mem acc: 0.000
==>>> it: 101, avg. loss: 5.475415, running train acc: 0.012
==>>> it: 101, mem avg. loss: 5.272077, running mem acc: 0.019
==>>> it: 201, avg. loss: 5.463835, running train acc: 0.011
==>>> it: 201, mem avg. loss: 5.376434, running mem acc: 0.013
[0.013 0.015 0.023 0.011 0.018 0.011 0.012 0.012 0.015 0.012]
no ratio: 0.0
on ratio: 0.9999492926322195
[(0, 988, 0, 0), (0, 985, 0, 989), (0, 977, 0, 1972)]
[0.024855268269777298, 0.03128104412555695, 0.032513487637043]
[0, nan, nan]
[nan, nan, nan]
[0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117]
[nan, nan, nan]
[0.000930305162910372, 0.000930305162910372, 0.000930305162910372]
-----------run 3 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.410600, running train acc: 0.050
==>>> it: 1, mem avg. loss: 5.235274, running mem acc: 0.050
==>>> it: 201, avg. loss: 5.224252, running train acc: 0.031
==>>> it: 201, mem avg. loss: 5.368978, running mem acc: 0.012
==>>> it: 301, avg. loss: 5.263827, running train acc: 0.022
==>>> it: 301, mem avg. loss: 5.362554, running mem acc: 0.013
==>>> it: 401, avg. loss: 5.288708, running train acc: 0.021
==>>> it: 401, mem avg. loss: 5.354223, running mem acc: 0.015
[0.011 0.015 0.023 0.013 0.015 0.015 0.009 0.011 0.017 0.014]
no ratio: 0.0
on ratio: 0.9999661143302497
[(0, 988, 0, 0), (0, 985, 0, 989), (0, 977, 0, 1972), (0, 987, 0, 2951)]
[0.024855268269777298, 0.03128104412555695, 0.032513487637043, 0.02683965790271759]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117]
[nan, nan, nan, nan]
[0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372]
-----------run 3 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.405914, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.024622, running mem acc: 0.050
==>>> it: 101, avg. loss: 5.409514, running train acc: 0.017
==>>> it: 101, mem avg. loss: 5.478333, running mem acc: 0.032
==>>> it: 201, avg. loss: 5.393511, running train acc: 0.016
==>>> it: 201, mem avg. loss: 5.495173, running mem acc: 0.024
==>>> it: 301, avg. loss: 5.398326, running train acc: 0.011
==>>> it: 301, mem avg. loss: 5.509036, running mem acc: 0.025
==>>> it: 401, avg. loss: 5.388361, running train acc: 0.018
==>>> it: 401, mem avg. loss: 5.537162, running mem acc: 0.022
[0.013 0.015 0.017 0.011 0.014 0.011 0.015 0.007 0.014 0.009]
no ratio: 0.0
on ratio: 0.9999746456732842
[(0, 988, 0, 0), (0, 985, 0, 989), (0, 977, 0, 1972), (0, 987, 0, 2951), (0, 986, 0, 3944)]
[0.024855268269777298, 0.03128104412555695, 0.032513487637043, 0.02683965790271759, 0.03857512930035591]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117]
[nan, nan, nan, nan, nan]
[0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372]
-----------run 3 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.762749, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.189863, running mem acc: 0.050
==>>> it: 101, avg. loss: 5.416737, running train acc: 0.020
==>>> it: 101, mem avg. loss: 5.536427, running mem acc: 0.012
==>>> it: 201, avg. loss: 5.376426, running train acc: 0.020
==>>> it: 201, mem avg. loss: 5.562091, running mem acc: 0.013
==>>> it: 301, avg. loss: 5.447853, running train acc: 0.014
==>>> it: 301, mem avg. loss: 5.571309, running mem acc: 0.013
[0.013 0.015 0.021 0.012 0.016 0.013 0.011 0.01  0.015 0.01 ]
no ratio: 0.0
on ratio: 0.9999796875952143
[(0, 988, 0, 0), (0, 985, 0, 989), (0, 977, 0, 1972), (0, 987, 0, 2951), (0, 986, 0, 3944), (0, 987, 0, 4923)]
[0.024855268269777298, 0.03128104412555695, 0.032513487637043, 0.02683965790271759, 0.03857512930035591, 0.03329746171832085]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117]
[nan, nan, nan, nan, nan, nan]
[0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372]
-----------run 3 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.269296, running train acc: 0.100
==>>> it: 1, mem avg. loss: 5.408564, running mem acc: 0.000
==>>> it: 101, avg. loss: 5.120988, running train acc: 0.043
==>>> it: 101, mem avg. loss: 5.483682, running mem acc: 0.010
==>>> it: 201, avg. loss: 5.166392, running train acc: 0.031
==>>> it: 201, mem avg. loss: 5.461985, running mem acc: 0.011
==>>> it: 301, avg. loss: 5.193000, running train acc: 0.026
==>>> it: 301, mem avg. loss: 5.466680, running mem acc: 0.012
==>>> it: 401, avg. loss: 5.217994, running train acc: 0.020
==>>> it: 401, mem avg. loss: 5.458747, running mem acc: 0.013
[0.014 0.015 0.02  0.015 0.018 0.011 0.015 0.01  0.017 0.012]
no ratio: 0.0
on ratio: 0.99998307121938
[(0, 988, 0, 0), (0, 985, 0, 989), (0, 977, 0, 1972), (0, 987, 0, 2951), (0, 986, 0, 3944), (0, 987, 0, 4923), (0, 985, 0, 5907)]
[0.024855268269777298, 0.03128104412555695, 0.032513487637043, 0.02683965790271759, 0.03857512930035591, 0.03329746171832085, 0.032261793553829195]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117]
[nan, nan, nan, nan, nan, nan, nan]
[0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372]
-----------run 3 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.391533, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.207165, running mem acc: 0.050
==>>> it: 201, avg. loss: 5.345255, running train acc: 0.005
==>>> it: 201, mem avg. loss: 5.669508, running mem acc: 0.016
==>>> it: 301, avg. loss: 5.376647, running train acc: 0.004
==>>> it: 301, mem avg. loss: 5.636324, running mem acc: 0.017
==>>> it: 401, avg. loss: 5.378589, running train acc: 0.009
==>>> it: 401, mem avg. loss: 5.649184, running mem acc: 0.016
[0.016 0.017 0.022 0.014 0.014 0.011 0.015 0.009 0.015 0.011]
no ratio: 0.0
on ratio: 0.9999854885286819
[(0, 988, 0, 0), (0, 985, 0, 989), (0, 977, 0, 1972), (0, 987, 0, 2951), (0, 986, 0, 3944), (0, 987, 0, 4923), (0, 985, 0, 5907), (0, 991, 0, 6891)]
[0.024855268269777298, 0.03128104412555695, 0.032513487637043, 0.02683965790271759, 0.03857512930035591, 0.03329746171832085, 0.032261793553829195, 0.034628702938556674]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117]
[nan, nan, nan, nan, nan, nan, nan, nan]
[0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372]
-----------run 3 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.869190, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.526636, running mem acc: 0.100
[0.016 0.016 0.021 0.013 0.018 0.014 0.014 0.011 0.019 0.01 ]
no ratio: 0.0
on ratio: 0.9999873049726422
[(0, 988, 0, 0), (0, 985, 0, 989), (0, 977, 0, 1972), (0, 987, 0, 2951), (0, 986, 0, 3944), (0, 987, 0, 4923), (0, 985, 0, 5907), (0, 991, 0, 6891), (0, 981, 0, 7877)]
[0.024855268269777298, 0.03128104412555695, 0.032513487637043, 0.02683965790271759, 0.03857512930035591, 0.03329746171832085, 0.032261793553829195, 0.034628702938556674, 0.025028052121400832]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372]
-----------run 3 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.188712, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.349136, running mem acc: 0.000
[0.012 0.012 0.019 0.011 0.015 0.009 0.011 0.003 0.013 0.013]
no ratio: 0.0
on ratio: 0.9999887578554485
[(0, 988, 0, 0), (0, 985, 0, 989), (0, 977, 0, 1972), (0, 987, 0, 2951), (0, 986, 0, 3944), (0, 987, 0, 4923), (0, 985, 0, 5907), (0, 991, 0, 6891), (0, 981, 0, 7877), (0, 987, 0, 8895)]
[0.024855268269777298, 0.03128104412555695, 0.032513487637043, 0.02683965790271759, 0.03857512930035591, 0.03329746171832085, 0.032261793553829195, 0.034628702938556674, 0.025028052121400832, 0.04401888132095337]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117, 0.00014464760897681117]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372, 0.000930305162910372]
-----------run 3-----------avg_end_acc 0.0118-----------train time 29875.01758956909
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 4 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.756481, running train acc: 0.050
==>>> it: 1, mem avg. loss: 2.709008, running mem acc: 0.100
[0.013 0.011 0.01  0.017 0.019 0.021 0.019 0.014 0.015 0.008]
no ratio: 0.0
on ratio: 0.0
[(0, 987, 0, 0)]
[-0.039692534327507016]
[0]
[nan]
[-0.00010921036300715059]
[nan]
[-0.0010263307485729456]
-----------run 4 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.606383, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.258339, running mem acc: 0.250
[0.012 0.012 0.009 0.011 0.01  0.007 0.009 0.012 0.008 0.015]
no ratio: 0.0
on ratio: 0.9998987956684546
[(0, 987, 0, 0), (0, 988, 0, 988)]
[-0.039692534327507016, -0.010900319337844848]
[0, nan]
[nan, nan]
[-0.00010921036300715059, -0.00010921035573119298]
[nan, nan]
[-0.0010263307485729456, -0.0010263309814035892]
-----------run 4 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.481408, running train acc: 0.050
==>>> it: 1, mem avg. loss: 5.252129, running mem acc: 0.000
[0.011 0.008 0.011 0.01  0.014 0.009 0.008 0.011 0.014 0.014]
no ratio: 0.0
on ratio: 0.999949522992277
[(0, 987, 0, 0), (0, 988, 0, 988), (0, 989, 0, 1981)]
[-0.039692534327507016, -0.010900319337844848, -0.016099681228399276]
[0, nan, nan]
[nan, nan, nan]
[-0.00010921036300715059, -0.00010921035573119298, -0.00010921037755906582]
[nan, nan, nan]
[-0.0010263307485729456, -0.0010263309814035892, -0.001026330515742302]
-----------run 4 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.788820, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.286634, running mem acc: 0.250
[0.01  0.016 0.014 0.01  0.008 0.009 0.008 0.016 0.011 0.015]
no ratio: 0.0
on ratio: 0.9999662173575218
[(0, 987, 0, 0), (0, 988, 0, 988), (0, 989, 0, 1981), (0, 990, 0, 2960)]
[-0.039692534327507016, -0.010900319337844848, -0.016099681228399276, -0.013449028484523297]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[-0.00010921036300715059, -0.00010921035573119298, -0.00010921037755906582, -0.00010921039938693866]
[nan, nan, nan, nan]
[-0.0010263307485729456, -0.0010263309814035892, -0.001026330515742302, -0.0010263306321576238]
-----------run 4 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.463151, running train acc: 0.100
==>>> it: 1, mem avg. loss: 3.470602, running mem acc: 0.050
[0.012 0.012 0.009 0.011 0.013 0.005 0.009 0.012 0.012 0.011]
no ratio: 0.0
on ratio: 0.999974722580319
[(0, 987, 0, 0), (0, 988, 0, 988), (0, 989, 0, 1981), (0, 990, 0, 2960), (0, 987, 0, 3956)]
[-0.039692534327507016, -0.010900319337844848, -0.016099681228399276, -0.013449028484523297, -0.017040452003479003]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[-0.00010921036300715059, -0.00010921035573119298, -0.00010921037755906582, -0.00010921039938693866, -0.00010921047942247242]
[nan, nan, nan, nan, nan]
[-0.0010263307485729456, -0.0010263309814035892, -0.001026330515742302, -0.0010263306321576238, -0.0010263307485729456]
-----------run 4 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.178963, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.245674, running mem acc: 0.200
[0.012 0.013 0.009 0.018 0.023 0.006 0.013 0.016 0.013 0.018]
no ratio: 0.0
on ratio: 0.9999796958437391
[(0, 987, 0, 0), (0, 988, 0, 988), (0, 989, 0, 1981), (0, 990, 0, 2960), (0, 987, 0, 3956), (0, 994, 0, 4925)]
[-0.039692534327507016, -0.010900319337844848, -0.016099681228399276, -0.013449028484523297, -0.017040452003479003, -0.018331249594688414]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[-0.00010921036300715059, -0.00010921035573119298, -0.00010921037755906582, -0.00010921039938693866, -0.00010921047942247242, -0.00010921046487055719]
[nan, nan, nan, nan, nan, nan]
[-0.0010263307485729456, -0.0010263309814035892, -0.001026330515742302, -0.0010263306321576238, -0.0010263307485729456, -0.0010263309814035892]
-----------run 4 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.186727, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.927104, running mem acc: 0.150
[0.013 0.013 0.013 0.018 0.02  0.012 0.017 0.017 0.015 0.017]
no ratio: 0.0
on ratio: 0.9999830826749674
[(0, 987, 0, 0), (0, 988, 0, 988), (0, 989, 0, 1981), (0, 990, 0, 2960), (0, 987, 0, 3956), (0, 994, 0, 4925), (0, 983, 0, 5911)]
[-0.039692534327507016, -0.010900319337844848, -0.016099681228399276, -0.013449028484523297, -0.017040452003479003, -0.018331249594688414, -0.019763897016644476]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[-0.00010921036300715059, -0.00010921035573119298, -0.00010921037755906582, -0.00010921039938693866, -0.00010921047942247242, -0.00010921046487055719, -0.00010921050852630287]
[nan, nan, nan, nan, nan, nan, nan]
[-0.0010263307485729456, -0.0010263309814035892, -0.001026330515742302, -0.0010263306321576238, -0.0010263307485729456, -0.0010263309814035892, -0.001026331796310842]
-----------run 4 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 5.446948, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.826175, running mem acc: 0.250
[0.017 0.016 0.013 0.015 0.02  0.011 0.019 0.012 0.019 0.018]
no ratio: 0.0
on ratio: 0.9999854843158031
[(0, 987, 0, 0), (0, 988, 0, 988), (0, 989, 0, 1981), (0, 990, 0, 2960), (0, 987, 0, 3956), (0, 994, 0, 4925), (0, 983, 0, 5911), (0, 988, 0, 6889)]
[-0.039692534327507016, -0.010900319337844848, -0.016099681228399276, -0.013449028484523297, -0.017040452003479003, -0.018331249594688414, -0.019763897016644476, 0.004140503088012338]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00010921036300715059, -0.00010921035573119298, -0.00010921037755906582, -0.00010921039938693866, -0.00010921047942247242, -0.00010921046487055719, -0.00010921050852630287, -0.00010921049397438765]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0010263307485729456, -0.0010263309814035892, -0.001026330515742302, -0.0010263306321576238, -0.0010263307485729456, -0.0010263309814035892, -0.001026331796310842, -0.001026331214234233]
-----------run 4 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.762013, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.455703, running mem acc: 0.200
[0.021 0.02  0.017 0.022 0.025 0.014 0.02  0.018 0.014 0.019]
no ratio: 0.0
on ratio: 0.9999872499394372
[(0, 987, 0, 0), (0, 988, 0, 988), (0, 989, 0, 1981), (0, 990, 0, 2960), (0, 987, 0, 3956), (0, 994, 0, 4925), (0, 983, 0, 5911), (0, 988, 0, 6889), (0, 986, 0, 7843)]
[-0.039692534327507016, -0.010900319337844848, -0.016099681228399276, -0.013449028484523297, -0.017040452003479003, -0.018331249594688414, -0.019763897016644476, 0.004140503088012338, -0.007266157940030098]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00010921036300715059, -0.00010921035573119298, -0.00010921037755906582, -0.00010921039938693866, -0.00010921047942247242, -0.00010921046487055719, -0.00010921050852630287, -0.00010921049397438765, -0.00010921050852630287]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0010263307485729456, -0.0010263309814035892, -0.001026330515742302, -0.0010263306321576238, -0.0010263307485729456, -0.0010263309814035892, -0.001026331796310842, -0.001026331214234233, -0.001026331214234233]
-----------run 4 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.884953, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.071317, running mem acc: 0.250
[0.011 0.014 0.021 0.018 0.023 0.014 0.018 0.017 0.013 0.024]
no ratio: 0.0
on ratio: 0.9999887019692467
[(0, 987, 0, 0), (0, 988, 0, 988), (0, 989, 0, 1981), (0, 990, 0, 2960), (0, 987, 0, 3956), (0, 994, 0, 4925), (0, 983, 0, 5911), (0, 988, 0, 6889), (0, 986, 0, 7843), (0, 976, 0, 8851)]
[-0.039692534327507016, -0.010900319337844848, -0.016099681228399276, -0.013449028484523297, -0.017040452003479003, -0.018331249594688414, -0.019763897016644476, 0.004140503088012338, -0.007266157940030098, -0.005667093951255083]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00010921036300715059, -0.00010921035573119298, -0.00010921037755906582, -0.00010921039938693866, -0.00010921047942247242, -0.00010921046487055719, -0.00010921050852630287, -0.00010921049397438765, -0.00010921050852630287, -0.0001092105230782181]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0010263307485729456, -0.0010263309814035892, -0.001026330515742302, -0.0010263306321576238, -0.0010263307485729456, -0.0010263309814035892, -0.001026331796310842, -0.001026331214234233, -0.001026331214234233, -0.0010263314470648766]
-----------run 4-----------avg_end_acc 0.017300000000000003-----------train time 171.52006721496582
----------- Total 5 run: 30675.75603580475s -----------
----------- Avg_End_Acc (0.01572, 0.005637892938022492) Avg_End_Fgt (0.004620000000000001, 0.0016667922878848833) Avg_Acc (0.013865182539682542, 0.001959169129364833) Avg_Bwtp (0.0017955555555555559, 0.0023513145256600387) Avg_Fwt (0.012813333333333335, 0.0010895732056222316)-----------
