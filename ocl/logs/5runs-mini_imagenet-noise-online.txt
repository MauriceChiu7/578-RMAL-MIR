Namespace(num_runs=5, seed=0, val_size=0.1, num_val=3, num_runs_val=3, error_analysis=True, verbose=True, store=False, save_path=None, agent='ER', update='random', retrieve='MIR', optimizer='SGD', learning_rate=0.1, epoch=1, batch=10, test_batch=128, weight_decay=0, num_tasks=10, fix_order=False, plot_sample=False, data='mini_imagenet', cl_type='ni', ns_factor=[0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6], ns_type='noise', ns_task=(1, 1, 2, 2, 2, 2), online=True, mem_size=5000, eps_mem_batch=10, lambda_=100, alpha=0.9, fisher_update_after=50, subsample=50, gss_mem_strength=10, gss_batch_size=10, k=5, aser_type='asvm', n_smp_cls=2.0, stm_capacity=1000, classifier_chill=0.01, log_alpha=-300, minlr=0.0005, clip=10.0, mem_epoch=70, labels_trick=False, separated_softmax=False, kd_trick=False, kd_trick_star=False, review_trick=False, ncm_trick=False, mem_iters=1, min_delta=0.0, patience=0, cumulative_delta=False, temp=0.07, buffer_tracker=False, warmup=4, head='mlp', budget=1.0, cuda=True)
Setting up data stream
data setup time: 1.355684757232666
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 0 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.366927, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.513958, running mem acc: 0.100
==>>> it: 101, avg. loss: 5.015291, running train acc: 0.026
==>>> it: 101, mem avg. loss: 3.286449, running mem acc: 0.251
==>>> it: 201, avg. loss: 4.846656, running train acc: 0.029
==>>> it: 201, mem avg. loss: 3.306516, running mem acc: 0.234
==>>> it: 301, avg. loss: 4.766414, running train acc: 0.033
==>>> it: 301, mem avg. loss: 3.289099, running mem acc: 0.236
==>>> it: 401, avg. loss: 4.668641, running train acc: 0.035
==>>> it: 401, mem avg. loss: 3.367958, running mem acc: 0.221
[0.08  0.069 0.061 0.074 0.076 0.059 0.057 0.045 0.05  0.045]
no ratio: 0.0
on ratio: 0.0
[(0, 920, 0, 0)]
[-0.0029484526887536048]
[0]
[nan]
[-7.531164737883955e-05]
[nan]
[-0.0012383618159219623]
-----------run 0 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.488788, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.719564, running mem acc: 0.150
==>>> it: 101, avg. loss: 4.290670, running train acc: 0.058
==>>> it: 101, mem avg. loss: 3.602458, running mem acc: 0.175
==>>> it: 201, avg. loss: 4.218487, running train acc: 0.064
==>>> it: 201, mem avg. loss: 3.590934, running mem acc: 0.171
==>>> it: 301, avg. loss: 4.183652, running train acc: 0.072
==>>> it: 301, mem avg. loss: 3.579139, running mem acc: 0.173
==>>> it: 401, avg. loss: 4.152327, running train acc: 0.075
==>>> it: 401, mem avg. loss: 3.542025, running mem acc: 0.178
[0.075 0.084 0.116 0.1   0.08  0.063 0.052 0.055 0.053 0.049]
no ratio: 0.0
on ratio: 0.9998919035779915
[(0, 920, 0, 0), (0, 916, 0, 925)]
[-0.0029484526887536048, 0.0004351024678908289]
[0, nan]
[nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05]
[nan, nan]
[-0.0012383618159219623, -0.001238374039530754]
-----------run 0 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.501975, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.790915, running mem acc: 0.300
==>>> it: 101, avg. loss: 4.086847, running train acc: 0.092
==>>> it: 101, mem avg. loss: 3.442289, running mem acc: 0.203
==>>> it: 201, avg. loss: 3.990155, running train acc: 0.098
==>>> it: 201, mem avg. loss: 3.455688, running mem acc: 0.200
==>>> it: 301, avg. loss: 3.949727, running train acc: 0.105
==>>> it: 301, mem avg. loss: 3.439577, running mem acc: 0.204
==>>> it: 401, avg. loss: 3.929968, running train acc: 0.109
==>>> it: 401, mem avg. loss: 3.438194, running mem acc: 0.201
[0.142 0.142 0.15  0.147 0.139 0.116 0.117 0.102 0.088 0.065]
no ratio: 0.0
on ratio: 0.9999417283375095
[(0, 920, 0, 0), (0, 916, 0, 925), (0, 850, 0, 1716)]
[-0.0029484526887536048, 0.0004351024678908289, -0.005924082051962614]
[0, nan, nan]
[nan, nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05, -7.531663140980527e-05]
[nan, nan, nan]
[-0.0012383618159219623, -0.001238374039530754, -0.0012383877765387297]
-----------run 0 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.965797, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.132742, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.856655, running train acc: 0.118
==>>> it: 101, mem avg. loss: 3.311791, running mem acc: 0.225
==>>> it: 201, avg. loss: 3.813181, running train acc: 0.128
==>>> it: 201, mem avg. loss: 3.275528, running mem acc: 0.222
==>>> it: 301, avg. loss: 3.774124, running train acc: 0.134
==>>> it: 301, mem avg. loss: 3.285041, running mem acc: 0.219
==>>> it: 401, avg. loss: 3.760955, running train acc: 0.138
==>>> it: 401, mem avg. loss: 3.259301, running mem acc: 0.224
[0.086 0.095 0.105 0.102 0.078 0.062 0.054 0.046 0.036 0.029]
no ratio: 0.0
on ratio: 0.9999631553737888
[(0, 920, 0, 0), (0, 916, 0, 925), (0, 850, 0, 1716), (0, 898, 0, 2714)]
[-0.0029484526887536048, 0.0004351024678908289, -0.005924082051962614, -0.02157464572787285]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05, -7.531663140980527e-05, -7.531910523539409e-05]
[nan, nan, nan, nan]
[-0.0012383618159219623, -0.001238374039530754, -0.0012383877765387297, -0.001238403026945889]
-----------run 0 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.554932, running train acc: 0.150
==>>> it: 1, mem avg. loss: 2.902280, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.748324, running train acc: 0.147
==>>> it: 101, mem avg. loss: 3.135032, running mem acc: 0.250
==>>> it: 201, avg. loss: 3.752112, running train acc: 0.139
==>>> it: 201, mem avg. loss: 3.137635, running mem acc: 0.234
==>>> it: 301, avg. loss: 3.699952, running train acc: 0.141
==>>> it: 301, mem avg. loss: 3.130732, running mem acc: 0.240
==>>> it: 401, avg. loss: 3.675888, running train acc: 0.143
==>>> it: 401, mem avg. loss: 3.119053, running mem acc: 0.245
[0.178 0.181 0.165 0.181 0.179 0.132 0.124 0.132 0.088 0.071]
no ratio: 0.0
on ratio: 0.9999696519073776
[(0, 920, 0, 0), (0, 916, 0, 925), (0, 850, 0, 1716), (0, 898, 0, 2714), (0, 821, 0, 3295)]
[-0.0029484526887536048, 0.0004351024678908289, -0.005924082051962614, -0.02157464572787285, -0.007702742256224155]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05, -7.531663140980527e-05, -7.531910523539409e-05, -7.532162999268621e-05]
[nan, nan, nan, nan, nan]
[-0.0012383618159219623, -0.001238374039530754, -0.0012383877765387297, -0.001238403026945889, -0.0012384200235828757]
-----------run 0 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.598530, running train acc: 0.250
==>>> it: 1, mem avg. loss: 3.210382, running mem acc: 0.200
==>>> it: 101, avg. loss: 3.573798, running train acc: 0.166
==>>> it: 101, mem avg. loss: 3.124359, running mem acc: 0.260
==>>> it: 201, avg. loss: 3.621356, running train acc: 0.160
==>>> it: 201, mem avg. loss: 3.015206, running mem acc: 0.268
==>>> it: 301, avg. loss: 3.595289, running train acc: 0.166
==>>> it: 301, mem avg. loss: 2.998681, running mem acc: 0.273
==>>> it: 401, avg. loss: 3.589752, running train acc: 0.164
==>>> it: 401, mem avg. loss: 3.006340, running mem acc: 0.271
[0.183 0.222 0.221 0.2   0.192 0.163 0.151 0.139 0.108 0.084]
no ratio: 0.0
on ratio: 0.999974887622109
[(0, 920, 0, 0), (0, 916, 0, 925), (0, 850, 0, 1716), (0, 898, 0, 2714), (0, 821, 0, 3295), (0, 837, 0, 3982)]
[-0.0029484526887536048, 0.0004351024678908289, -0.005924082051962614, -0.02157464572787285, -0.007702742256224155, -0.009956833943724631]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05, -7.531663140980527e-05, -7.531910523539409e-05, -7.532162999268621e-05, -7.532385643571615e-05]
[nan, nan, nan, nan, nan, nan]
[-0.0012383618159219623, -0.001238374039530754, -0.0012383877765387297, -0.001238403026945889, -0.0012384200235828757, -0.0012384330620989203]
-----------run 0 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.045118, running train acc: 0.150
==>>> it: 1, mem avg. loss: 2.754883, running mem acc: 0.350
==>>> it: 101, avg. loss: 3.528221, running train acc: 0.190
==>>> it: 101, mem avg. loss: 2.827885, running mem acc: 0.302
==>>> it: 201, avg. loss: 3.522994, running train acc: 0.180
==>>> it: 201, mem avg. loss: 2.902072, running mem acc: 0.285
==>>> it: 301, avg. loss: 3.523947, running train acc: 0.179
==>>> it: 301, mem avg. loss: 2.875363, running mem acc: 0.293
==>>> it: 401, avg. loss: 3.533375, running train acc: 0.178
==>>> it: 401, mem avg. loss: 2.854158, running mem acc: 0.295
[0.192 0.204 0.213 0.208 0.188 0.174 0.181 0.184 0.155 0.115]
no ratio: 0.0
on ratio: 0.9999792578457197
[(0, 920, 0, 0), (0, 916, 0, 925), (0, 850, 0, 1716), (0, 898, 0, 2714), (0, 821, 0, 3295), (0, 837, 0, 3982), (0, 819, 0, 4821)]
[-0.0029484526887536048, 0.0004351024678908289, -0.005924082051962614, -0.02157464572787285, -0.007702742256224155, -0.009956833943724631, -0.009022896721959115]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05, -7.531663140980527e-05, -7.531910523539409e-05, -7.532162999268621e-05, -7.532385643571615e-05, -7.532652671216056e-05]
[nan, nan, nan, nan, nan, nan, nan]
[-0.0012383618159219623, -0.001238374039530754, -0.0012383877765387297, -0.001238403026945889, -0.0012384200235828757, -0.0012384330620989203, -0.001238447381183505]
-----------run 0 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.779056, running train acc: 0.150
==>>> it: 1, mem avg. loss: 2.956845, running mem acc: 0.350
==>>> it: 101, avg. loss: 3.504760, running train acc: 0.163
==>>> it: 101, mem avg. loss: 2.797258, running mem acc: 0.290
==>>> it: 201, avg. loss: 3.454821, running train acc: 0.181
==>>> it: 201, mem avg. loss: 2.803166, running mem acc: 0.300
==>>> it: 301, avg. loss: 3.463126, running train acc: 0.180
==>>> it: 301, mem avg. loss: 2.767453, running mem acc: 0.313
==>>> it: 401, avg. loss: 3.465252, running train acc: 0.183
==>>> it: 401, mem avg. loss: 2.763988, running mem acc: 0.314
[0.19  0.224 0.195 0.199 0.193 0.166 0.176 0.173 0.15  0.16 ]
no ratio: 0.0
on ratio: 0.9999823230984073
[(0, 920, 0, 0), (0, 916, 0, 925), (0, 850, 0, 1716), (0, 898, 0, 2714), (0, 821, 0, 3295), (0, 837, 0, 3982), (0, 819, 0, 4821), (0, 827, 0, 5657)]
[-0.0029484526887536048, 0.0004351024678908289, -0.005924082051962614, -0.02157464572787285, -0.007702742256224155, -0.009956833943724631, -0.009022896721959115, -0.005029336698353291]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05, -7.531663140980527e-05, -7.531910523539409e-05, -7.532162999268621e-05, -7.532385643571615e-05, -7.532652671216056e-05, -7.532879681093618e-05]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0012383618159219623, -0.001238374039530754, -0.0012383877765387297, -0.001238403026945889, -0.0012384200235828757, -0.0012384330620989203, -0.001238447381183505, -0.0012384664732962847]
-----------run 0 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.566508, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.565218, running mem acc: 0.350
==>>> it: 101, avg. loss: 3.435270, running train acc: 0.187
==>>> it: 101, mem avg. loss: 2.775104, running mem acc: 0.324
==>>> it: 201, avg. loss: 3.438095, running train acc: 0.191
==>>> it: 201, mem avg. loss: 2.729260, running mem acc: 0.322
==>>> it: 301, avg. loss: 3.455801, running train acc: 0.198
==>>> it: 301, mem avg. loss: 2.712966, running mem acc: 0.323
==>>> it: 401, avg. loss: 3.466111, running train acc: 0.196
==>>> it: 401, mem avg. loss: 2.676600, running mem acc: 0.327
[0.169 0.204 0.202 0.183 0.209 0.165 0.154 0.164 0.117 0.086]
no ratio: 0.0
on ratio: 0.9999847330575106
[(0, 920, 0, 0), (0, 916, 0, 925), (0, 850, 0, 1716), (0, 898, 0, 2714), (0, 821, 0, 3295), (0, 837, 0, 3982), (0, 819, 0, 4821), (0, 827, 0, 5657), (0, 883, 0, 6550)]
[-0.0029484526887536048, 0.0004351024678908289, -0.005924082051962614, -0.02157464572787285, -0.007702742256224155, -0.009956833943724631, -0.009022896721959115, -0.005029336698353291, 0.003396669102832675]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05, -7.531663140980527e-05, -7.531910523539409e-05, -7.532162999268621e-05, -7.532385643571615e-05, -7.532652671216056e-05, -7.532879681093618e-05, -7.533133612014353e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0012383618159219623, -0.001238374039530754, -0.0012383877765387297, -0.001238403026945889, -0.0012384200235828757, -0.0012384330620989203, -0.001238447381183505, -0.0012384664732962847, -0.0012384748551994562]
-----------run 0 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.378463, running train acc: 0.250
==>>> it: 1, mem avg. loss: 2.535133, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.466423, running train acc: 0.185
==>>> it: 101, mem avg. loss: 2.622957, running mem acc: 0.329
==>>> it: 201, avg. loss: 3.411845, running train acc: 0.199
==>>> it: 201, mem avg. loss: 2.616927, running mem acc: 0.337
==>>> it: 301, avg. loss: 3.420869, running train acc: 0.193
==>>> it: 301, mem avg. loss: 2.577119, running mem acc: 0.350
==>>> it: 401, avg. loss: 3.417553, running train acc: 0.195
==>>> it: 401, mem avg. loss: 2.580570, running mem acc: 0.350
[0.223 0.262 0.254 0.239 0.263 0.237 0.241 0.222 0.216 0.198]
no ratio: 0.0
on ratio: 0.9999853867399278
[(0, 920, 0, 0), (0, 916, 0, 925), (0, 850, 0, 1716), (0, 898, 0, 2714), (0, 821, 0, 3295), (0, 837, 0, 3982), (0, 819, 0, 4821), (0, 827, 0, 5657), (0, 883, 0, 6550), (0, 802, 0, 6843)]
[-0.0029484526887536048, 0.0004351024678908289, -0.005924082051962614, -0.02157464572787285, -0.007702742256224155, -0.009956833943724631, -0.009022896721959115, -0.005029336698353291, 0.003396669102832675, -0.004978051420301199]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-7.531164737883955e-05, -7.53141866880469e-05, -7.531663140980527e-05, -7.531910523539409e-05, -7.532162999268621e-05, -7.532385643571615e-05, -7.532652671216056e-05, -7.532879681093618e-05, -7.533133612014353e-05, -7.53330605220981e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0012383618159219623, -0.001238374039530754, -0.0012383877765387297, -0.001238403026945889, -0.0012384200235828757, -0.0012384330620989203, -0.001238447381183505, -0.0012384664732962847, -0.0012384748551994562, -0.0012384771835058928]
-----------run 0-----------avg_end_acc 0.2355-----------train time 265.4391324520111
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 1 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.545312, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.551831, running mem acc: 0.100
==>>> it: 101, avg. loss: 5.142096, running train acc: 0.017
==>>> it: 101, mem avg. loss: 3.093306, running mem acc: 0.253
==>>> it: 201, avg. loss: 4.924352, running train acc: 0.029
==>>> it: 201, mem avg. loss: 3.178990, running mem acc: 0.240
==>>> it: 301, avg. loss: 4.819396, running train acc: 0.033
==>>> it: 301, mem avg. loss: 3.257858, running mem acc: 0.228
==>>> it: 401, avg. loss: 4.718665, running train acc: 0.037
==>>> it: 401, mem avg. loss: 3.320011, running mem acc: 0.225
[0.057 0.076 0.064 0.059 0.079 0.067 0.075 0.067 0.058 0.072]
no ratio: 0.0
on ratio: 0.0
[(0, 943, 0, 0)]
[-0.008153536900877953]
[0]
[nan]
[-4.436251037986949e-05]
[nan]
[-0.0013915121089667082]
-----------run 1 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.283368, running train acc: 0.100
==>>> it: 1, mem avg. loss: 3.605784, running mem acc: 0.300
==>>> it: 101, avg. loss: 4.308896, running train acc: 0.065
==>>> it: 101, mem avg. loss: 3.588235, running mem acc: 0.192
==>>> it: 201, avg. loss: 4.288037, running train acc: 0.065
==>>> it: 201, mem avg. loss: 3.526132, running mem acc: 0.196
==>>> it: 301, avg. loss: 4.248713, running train acc: 0.075
==>>> it: 301, mem avg. loss: 3.512842, running mem acc: 0.198
==>>> it: 401, avg. loss: 4.228679, running train acc: 0.076
==>>> it: 401, mem avg. loss: 3.517613, running mem acc: 0.194
[0.081 0.084 0.09  0.072 0.073 0.071 0.066 0.068 0.048 0.054]
no ratio: 0.0
on ratio: 0.9998911979109999
[(0, 943, 0, 0), (0, 916, 0, 919)]
[-0.008153536900877953, -0.004614683732390404]
[0, nan]
[nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05]
[nan, nan]
[-0.0013915121089667082, -0.001391521655023098]
-----------run 1 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.894852, running train acc: 0.150
==>>> it: 1, mem avg. loss: 3.985247, running mem acc: 0.200
==>>> it: 101, avg. loss: 4.075356, running train acc: 0.089
==>>> it: 101, mem avg. loss: 3.455268, running mem acc: 0.212
==>>> it: 201, avg. loss: 4.005578, running train acc: 0.100
==>>> it: 201, mem avg. loss: 3.441811, running mem acc: 0.217
==>>> it: 301, avg. loss: 3.971015, running train acc: 0.102
==>>> it: 301, mem avg. loss: 3.423488, running mem acc: 0.216
==>>> it: 401, avg. loss: 3.940750, running train acc: 0.108
==>>> it: 401, mem avg. loss: 3.407500, running mem acc: 0.223
[0.047 0.057 0.045 0.036 0.042 0.034 0.034 0.037 0.019 0.022]
no ratio: 0.0
on ratio: 0.9999472601656031
[(0, 943, 0, 0), (0, 916, 0, 919), (0, 955, 0, 1896)]
[-0.008153536900877953, -0.004614683732390404, -0.0041402046903967855]
[0, nan, nan]
[nan, nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05, -4.4368232920533046e-05]
[nan, nan, nan]
[-0.0013915121089667082, -0.001391521655023098, -0.001391541794873774]
-----------run 1 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.497978, running train acc: 0.150
==>>> it: 1, mem avg. loss: 3.356862, running mem acc: 0.100
==>>> it: 101, avg. loss: 3.759637, running train acc: 0.135
==>>> it: 101, mem avg. loss: 3.443113, running mem acc: 0.192
==>>> it: 201, avg. loss: 3.809307, running train acc: 0.128
==>>> it: 201, mem avg. loss: 3.329390, running mem acc: 0.225
==>>> it: 301, avg. loss: 3.783569, running train acc: 0.130
==>>> it: 301, mem avg. loss: 3.318080, running mem acc: 0.227
==>>> it: 401, avg. loss: 3.754670, running train acc: 0.135
==>>> it: 401, mem avg. loss: 3.289116, running mem acc: 0.227
[0.141 0.153 0.146 0.133 0.138 0.12  0.111 0.079 0.073 0.066]
no ratio: 0.0
on ratio: 0.9999609390258193
[(0, 943, 0, 0), (0, 916, 0, 919), (0, 955, 0, 1896), (0, 867, 0, 2560)]
[-0.008153536900877953, -0.004614683732390404, -0.0041402046903967855, -0.006274079695343971]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05, -4.4368232920533046e-05, -4.4371186959324405e-05]
[nan, nan, nan, nan]
[-0.0013915121089667082, -0.001391521655023098, -0.001391541794873774, -0.0013915578601881862]
-----------run 1 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.227023, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.986037, running mem acc: 0.150
==>>> it: 101, avg. loss: 3.694120, running train acc: 0.153
==>>> it: 101, mem avg. loss: 3.102194, running mem acc: 0.251
==>>> it: 201, avg. loss: 3.630949, running train acc: 0.166
==>>> it: 201, mem avg. loss: 3.104714, running mem acc: 0.244
==>>> it: 301, avg. loss: 3.625972, running train acc: 0.169
==>>> it: 301, mem avg. loss: 3.104508, running mem acc: 0.244
==>>> it: 401, avg. loss: 3.620378, running train acc: 0.168
==>>> it: 401, mem avg. loss: 3.072272, running mem acc: 0.252
[0.135 0.154 0.13  0.128 0.157 0.14  0.137 0.162 0.134 0.096]
no ratio: 0.0
on ratio: 0.9999710405143205
[(0, 943, 0, 0), (0, 916, 0, 919), (0, 955, 0, 1896), (0, 867, 0, 2560), (0, 843, 0, 3453)]
[-0.008153536900877953, -0.004614683732390404, -0.0041402046903967855, -0.006274079695343971, -0.005326962001621723]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05, -4.4368232920533046e-05, -4.4371186959324405e-05, -4.437353345565498e-05]
[nan, nan, nan, nan, nan]
[-0.0013915121089667082, -0.001391521655023098, -0.001391541794873774, -0.0013915578601881862, -0.0013915685703977942]
-----------run 1 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.381080, running train acc: 0.200
==>>> it: 1, mem avg. loss: 3.278865, running mem acc: 0.300
==>>> it: 101, avg. loss: 3.563115, running train acc: 0.166
==>>> it: 101, mem avg. loss: 3.032284, running mem acc: 0.242
==>>> it: 201, avg. loss: 3.541295, running train acc: 0.171
==>>> it: 201, mem avg. loss: 3.057819, running mem acc: 0.247
==>>> it: 301, avg. loss: 3.519611, running train acc: 0.173
==>>> it: 301, mem avg. loss: 3.030320, running mem acc: 0.253
==>>> it: 401, avg. loss: 3.526420, running train acc: 0.174
==>>> it: 401, mem avg. loss: 2.984506, running mem acc: 0.264
[0.196 0.208 0.192 0.185 0.207 0.17  0.158 0.123 0.102 0.078]
no ratio: 0.0
on ratio: 0.9999750753969243
[(0, 943, 0, 0), (0, 916, 0, 919), (0, 955, 0, 1896), (0, 867, 0, 2560), (0, 843, 0, 3453), (0, 830, 0, 4012)]
[-0.008153536900877953, -0.004614683732390404, -0.0041402046903967855, -0.006274079695343971, -0.005326962001621723, -0.008669006042182445]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05, -4.4368232920533046e-05, -4.4371186959324405e-05, -4.437353345565498e-05, -4.4376090954756364e-05]
[nan, nan, nan, nan, nan, nan]
[-0.0013915121089667082, -0.001391521655023098, -0.001391541794873774, -0.0013915578601881862, -0.0013915685703977942, -0.001391583471558988]
-----------run 1 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.058392, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.428940, running mem acc: 0.350
==>>> it: 101, avg. loss: 3.479746, running train acc: 0.192
==>>> it: 101, mem avg. loss: 2.808091, running mem acc: 0.280
==>>> it: 201, avg. loss: 3.512167, running train acc: 0.183
==>>> it: 201, mem avg. loss: 2.834750, running mem acc: 0.289
==>>> it: 301, avg. loss: 3.478955, running train acc: 0.184
==>>> it: 301, mem avg. loss: 2.834067, running mem acc: 0.294
==>>> it: 401, avg. loss: 3.496283, running train acc: 0.181
==>>> it: 401, mem avg. loss: 2.823215, running mem acc: 0.296
[0.186 0.195 0.181 0.189 0.219 0.198 0.199 0.184 0.194 0.164]
no ratio: 0.0
on ratio: 0.9999793050640507
[(0, 943, 0, 0), (0, 916, 0, 919), (0, 955, 0, 1896), (0, 867, 0, 2560), (0, 843, 0, 3453), (0, 830, 0, 4012), (0, 801, 0, 4832)]
[-0.008153536900877953, -0.004614683732390404, -0.0041402046903967855, -0.006274079695343971, -0.005326962001621723, -0.008669006042182445, -0.010159875527024268]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05, -4.4368232920533046e-05, -4.4371186959324405e-05, -4.437353345565498e-05, -4.4376090954756364e-05, -4.4378804886946455e-05]
[nan, nan, nan, nan, nan, nan, nan]
[-0.0013915121089667082, -0.001391521655023098, -0.001391541794873774, -0.0013915578601881862, -0.0013915685703977942, -0.001391583471558988, -0.0013915987219661474]
-----------run 1 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.777373, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.063359, running mem acc: 0.400
==>>> it: 101, avg. loss: 3.445898, running train acc: 0.202
==>>> it: 101, mem avg. loss: 2.756884, running mem acc: 0.301
==>>> it: 201, avg. loss: 3.454097, running train acc: 0.201
==>>> it: 201, mem avg. loss: 2.761209, running mem acc: 0.297
==>>> it: 301, avg. loss: 3.446870, running train acc: 0.194
==>>> it: 301, mem avg. loss: 2.729777, running mem acc: 0.304
==>>> it: 401, avg. loss: 3.456958, running train acc: 0.191
==>>> it: 401, mem avg. loss: 2.718458, running mem acc: 0.313
[0.21  0.241 0.232 0.23  0.214 0.198 0.199 0.187 0.18  0.144]
no ratio: 0.0
on ratio: 0.9999817388287284
[(0, 943, 0, 0), (0, 916, 0, 919), (0, 955, 0, 1896), (0, 867, 0, 2560), (0, 843, 0, 3453), (0, 830, 0, 4012), (0, 801, 0, 4832), (0, 813, 0, 5476)]
[-0.008153536900877953, -0.004614683732390404, -0.0041402046903967855, -0.006274079695343971, -0.005326962001621723, -0.008669006042182445, -0.010159875527024268, -0.003739107606001198]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05, -4.4368232920533046e-05, -4.4371186959324405e-05, -4.437353345565498e-05, -4.4376090954756364e-05, -4.4378804886946455e-05, -4.438062387635e-05]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0013915121089667082, -0.001391521655023098, -0.001391541794873774, -0.0013915578601881862, -0.0013915685703977942, -0.001391583471558988, -0.0013915987219661474, -0.0013916025636717677]
-----------run 1 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.953088, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.599761, running mem acc: 0.400
==>>> it: 101, avg. loss: 3.440891, running train acc: 0.188
==>>> it: 101, mem avg. loss: 2.666754, running mem acc: 0.329
==>>> it: 201, avg. loss: 3.444412, running train acc: 0.189
==>>> it: 201, mem avg. loss: 2.629991, running mem acc: 0.334
==>>> it: 301, avg. loss: 3.461649, running train acc: 0.189
==>>> it: 301, mem avg. loss: 2.642025, running mem acc: 0.332
==>>> it: 401, avg. loss: 3.458241, running train acc: 0.188
==>>> it: 401, mem avg. loss: 2.644615, running mem acc: 0.329
[0.223 0.239 0.222 0.227 0.241 0.204 0.217 0.224 0.231 0.165]
no ratio: 0.0
on ratio: 0.9999838790282277
[(0, 943, 0, 0), (0, 916, 0, 919), (0, 955, 0, 1896), (0, 867, 0, 2560), (0, 843, 0, 3453), (0, 830, 0, 4012), (0, 801, 0, 4832), (0, 813, 0, 5476), (0, 769, 0, 6203)]
[-0.008153536900877953, -0.004614683732390404, -0.0041402046903967855, -0.006274079695343971, -0.005326962001621723, -0.008669006042182445, -0.010159875527024268, -0.003739107606001198, -0.007067844167351723]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05, -4.4368232920533046e-05, -4.4371186959324405e-05, -4.437353345565498e-05, -4.4376090954756364e-05, -4.4378804886946455e-05, -4.438062387635e-05, -4.4382806663634256e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0013915121089667082, -0.001391521655023098, -0.001391541794873774, -0.0013915578601881862, -0.0013915685703977942, -0.001391583471558988, -0.0013915987219661474, -0.0013916025636717677, -0.0013916128082200885]
-----------run 1 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.074996, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.450540, running mem acc: 0.300
==>>> it: 101, avg. loss: 3.367255, running train acc: 0.213
==>>> it: 101, mem avg. loss: 2.596107, running mem acc: 0.339
==>>> it: 201, avg. loss: 3.367689, running train acc: 0.210
==>>> it: 201, mem avg. loss: 2.570538, running mem acc: 0.351
==>>> it: 301, avg. loss: 3.376842, running train acc: 0.205
==>>> it: 301, mem avg. loss: 2.536113, running mem acc: 0.351
==>>> it: 401, avg. loss: 3.376422, running train acc: 0.206
==>>> it: 401, mem avg. loss: 2.501585, running mem acc: 0.360
[0.223 0.234 0.222 0.225 0.22  0.191 0.205 0.211 0.204 0.207]
no ratio: 0.0
on ratio: 0.9999858459186706
[(0, 943, 0, 0), (0, 916, 0, 919), (0, 955, 0, 1896), (0, 867, 0, 2560), (0, 843, 0, 3453), (0, 830, 0, 4012), (0, 801, 0, 4832), (0, 813, 0, 5476), (0, 769, 0, 6203), (0, 793, 0, 7065)]
[-0.008153536900877953, -0.004614683732390404, -0.0041402046903967855, -0.006274079695343971, -0.005326962001621723, -0.008669006042182445, -0.010159875527024268, -0.003739107606001198, -0.007067844167351723, -0.00904382449388504]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-4.436251037986949e-05, -4.4365071516949683e-05, -4.4368232920533046e-05, -4.4371186959324405e-05, -4.437353345565498e-05, -4.4376090954756364e-05, -4.4378804886946455e-05, -4.438062387635e-05, -4.4382806663634256e-05, -4.438492760527879e-05]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0013915121089667082, -0.001391521655023098, -0.001391541794873774, -0.0013915578601881862, -0.0013915685703977942, -0.001391583471558988, -0.0013915987219661474, -0.0013916025636717677, -0.0013916128082200885, -0.0013916236348450184]
-----------run 1-----------avg_end_acc 0.2142-----------train time 264.5596477985382
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 2 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.170889, running train acc: 0.050
==>>> it: 1, mem avg. loss: 2.526487, running mem acc: 0.100
==>>> it: 101, avg. loss: 5.170620, running train acc: 0.015
==>>> it: 101, mem avg. loss: 3.308273, running mem acc: 0.229
==>>> it: 201, avg. loss: 4.979343, running train acc: 0.019
==>>> it: 201, mem avg. loss: 3.251754, running mem acc: 0.228
==>>> it: 301, avg. loss: 4.845160, running train acc: 0.029
==>>> it: 301, mem avg. loss: 3.272795, running mem acc: 0.228
==>>> it: 401, avg. loss: 4.758813, running train acc: 0.032
==>>> it: 401, mem avg. loss: 3.298278, running mem acc: 0.228
[0.061 0.049 0.065 0.054 0.049 0.052 0.054 0.057 0.052 0.051]
no ratio: 0.0
on ratio: 0.0
[(0, 939, 0, 0)]
[0.011884676061570645]
[0]
[nan]
[0.00018465520406607538]
[nan]
[-0.00024391025363001972]
-----------run 2 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.439040, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.284011, running mem acc: 0.200
==>>> it: 101, avg. loss: 4.328448, running train acc: 0.069
==>>> it: 101, mem avg. loss: 3.501741, running mem acc: 0.206
==>>> it: 201, avg. loss: 4.307234, running train acc: 0.068
==>>> it: 201, mem avg. loss: 3.483247, running mem acc: 0.206
==>>> it: 301, avg. loss: 4.301789, running train acc: 0.068
==>>> it: 301, mem avg. loss: 3.476872, running mem acc: 0.206
==>>> it: 401, avg. loss: 4.268189, running train acc: 0.071
==>>> it: 401, mem avg. loss: 3.466014, running mem acc: 0.206
[0.094 0.078 0.097 0.076 0.083 0.077 0.091 0.089 0.079 0.1  ]
no ratio: 0.0
on ratio: 0.9998896369054188
[(0, 939, 0, 0), (0, 922, 0, 906)]
[0.011884676061570645, 0.014824435010552407]
[0, nan]
[nan, nan]
[0.00018465520406607538, 0.00018465235189069062]
[nan, nan]
[-0.00024391025363001972, -0.00024392812338192016]
-----------run 2 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.744661, running train acc: 0.250
==>>> it: 1, mem avg. loss: 3.312682, running mem acc: 0.250
==>>> it: 101, avg. loss: 4.068710, running train acc: 0.107
==>>> it: 101, mem avg. loss: 3.319205, running mem acc: 0.233
==>>> it: 201, avg. loss: 4.066476, running train acc: 0.096
==>>> it: 201, mem avg. loss: 3.368185, running mem acc: 0.219
==>>> it: 301, avg. loss: 4.046396, running train acc: 0.099
==>>> it: 301, mem avg. loss: 3.361646, running mem acc: 0.226
==>>> it: 401, avg. loss: 4.032376, running train acc: 0.102
==>>> it: 401, mem avg. loss: 3.360070, running mem acc: 0.223
[0.119 0.113 0.108 0.102 0.098 0.108 0.102 0.09  0.081 0.097]
no ratio: 0.0
on ratio: 0.9999434421130027
[(0, 939, 0, 0), (0, 922, 0, 906), (0, 892, 0, 1768)]
[0.011884676061570645, 0.014824435010552407, 0.021827941715717315]
[0, nan, nan]
[nan, nan, nan]
[0.00018465520406607538, 0.00018465235189069062, 0.00018464983440935612]
[nan, nan, nan]
[-0.00024391025363001972, -0.00024392812338192016, -0.000243943024543114]
-----------run 2 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.130019, running train acc: 0.050
==>>> it: 1, mem avg. loss: 2.819342, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.907767, running train acc: 0.111
==>>> it: 101, mem avg. loss: 3.342126, running mem acc: 0.202
==>>> it: 201, avg. loss: 3.905557, running train acc: 0.122
==>>> it: 201, mem avg. loss: 3.299797, running mem acc: 0.230
==>>> it: 301, avg. loss: 3.912184, running train acc: 0.122
==>>> it: 301, mem avg. loss: 3.298161, running mem acc: 0.225
==>>> it: 401, avg. loss: 3.887707, running train acc: 0.123
==>>> it: 401, mem avg. loss: 3.279102, running mem acc: 0.229
[0.152 0.126 0.137 0.12  0.129 0.136 0.107 0.1   0.073 0.078]
no ratio: 0.0
on ratio: 0.9999613167769139
[(0, 939, 0, 0), (0, 922, 0, 906), (0, 892, 0, 1768), (0, 880, 0, 2585)]
[0.011884676061570645, 0.014824435010552407, 0.021827941715717315, 0.022587841510772706]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[0.00018465520406607538, 0.00018465235189069062, 0.00018464983440935612, 0.000184647215064615]
[nan, nan, nan, nan]
[-0.00024391025363001972, -0.00024392812338192016, -0.000243943024543114, -0.0002439624076941982]
-----------run 2 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.772034, running train acc: 0.150
==>>> it: 1, mem avg. loss: 3.397384, running mem acc: 0.300
==>>> it: 101, avg. loss: 3.827763, running train acc: 0.112
==>>> it: 101, mem avg. loss: 3.318407, running mem acc: 0.236
==>>> it: 201, avg. loss: 3.718324, running train acc: 0.134
==>>> it: 201, mem avg. loss: 3.294988, running mem acc: 0.227
==>>> it: 301, avg. loss: 3.700008, running train acc: 0.140
==>>> it: 301, mem avg. loss: 3.231848, running mem acc: 0.238
==>>> it: 401, avg. loss: 3.705598, running train acc: 0.141
==>>> it: 401, mem avg. loss: 3.214044, running mem acc: 0.240
[0.17  0.167 0.15  0.161 0.139 0.148 0.131 0.106 0.081 0.081]
no ratio: 0.0
on ratio: 0.9999701679544166
[(0, 939, 0, 0), (0, 922, 0, 906), (0, 892, 0, 1768), (0, 880, 0, 2585), (0, 861, 0, 3352)]
[0.011884676061570645, 0.014824435010552407, 0.021827941715717315, 0.022587841510772706, 0.026036780416965485]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[0.00018465520406607538, 0.00018465235189069062, 0.00018464983440935612, 0.000184647215064615, 0.0001846451632445678]
[nan, nan, nan, nan, nan]
[-0.00024391025363001972, -0.00024392812338192016, -0.000243943024543114, -0.0002439624076941982, -0.00024397223023697734]
-----------run 2 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.564738, running train acc: 0.100
==>>> it: 1, mem avg. loss: 3.013166, running mem acc: 0.300
==>>> it: 101, avg. loss: 3.636699, running train acc: 0.163
==>>> it: 101, mem avg. loss: 3.137588, running mem acc: 0.239
==>>> it: 201, avg. loss: 3.662793, running train acc: 0.157
==>>> it: 201, mem avg. loss: 3.112765, running mem acc: 0.256
==>>> it: 301, avg. loss: 3.641934, running train acc: 0.160
==>>> it: 301, mem avg. loss: 3.064867, running mem acc: 0.264
==>>> it: 401, avg. loss: 3.620693, running train acc: 0.163
==>>> it: 401, mem avg. loss: 3.047489, running mem acc: 0.267
[0.202 0.195 0.198 0.189 0.187 0.179 0.182 0.152 0.136 0.117]
no ratio: 0.0
on ratio: 0.9999751805614158
[(0, 939, 0, 0), (0, 922, 0, 906), (0, 892, 0, 1768), (0, 880, 0, 2585), (0, 861, 0, 3352), (0, 821, 0, 4029)]
[0.011884676061570645, 0.014824435010552407, 0.021827941715717315, 0.022587841510772706, 0.026036780416965485, 0.018957972675561905]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[0.00018465520406607538, 0.00018465235189069062, 0.00018464983440935612, 0.000184647215064615, 0.0001846451632445678, 0.0001846426457632333]
[nan, nan, nan, nan, nan, nan]
[-0.00024391025363001972, -0.00024392812338192016, -0.000243943024543114, -0.0002439624076941982, -0.00024397223023697734, -0.00024398027744609863]
-----------run 2 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.788027, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.015842, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.597885, running train acc: 0.170
==>>> it: 101, mem avg. loss: 2.963724, running mem acc: 0.285
==>>> it: 201, avg. loss: 3.573599, running train acc: 0.171
==>>> it: 201, mem avg. loss: 2.938300, running mem acc: 0.297
==>>> it: 301, avg. loss: 3.521698, running train acc: 0.178
==>>> it: 301, mem avg. loss: 2.942695, running mem acc: 0.292
==>>> it: 401, avg. loss: 3.486810, running train acc: 0.179
==>>> it: 401, mem avg. loss: 2.930549, running mem acc: 0.287
[0.233 0.215 0.202 0.193 0.196 0.204 0.199 0.167 0.17  0.142]
no ratio: 0.0
on ratio: 0.9999789787895986
[(0, 939, 0, 0), (0, 922, 0, 906), (0, 892, 0, 1768), (0, 880, 0, 2585), (0, 861, 0, 3352), (0, 821, 0, 4029), (0, 801, 0, 4757)]
[0.011884676061570645, 0.014824435010552407, 0.021827941715717315, 0.022587841510772706, 0.026036780416965485, 0.018957972675561905, 0.021287561923265457]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[0.00018465520406607538, 0.00018465235189069062, 0.00018464983440935612, 0.000184647215064615, 0.0001846451632445678, 0.0001846426457632333, 0.00018464047752786428]
[nan, nan, nan, nan, nan, nan, nan]
[-0.00024391025363001972, -0.00024392812338192016, -0.000243943024543114, -0.0002439624076941982, -0.00024397223023697734, -0.00024398027744609863, -0.00024399383983109146]
-----------run 2 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.743284, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.770997, running mem acc: 0.200
==>>> it: 101, avg. loss: 3.489988, running train acc: 0.183
==>>> it: 101, mem avg. loss: 2.890143, running mem acc: 0.293
==>>> it: 201, avg. loss: 3.489804, running train acc: 0.186
==>>> it: 201, mem avg. loss: 2.826820, running mem acc: 0.311
==>>> it: 301, avg. loss: 3.465961, running train acc: 0.194
==>>> it: 301, mem avg. loss: 2.835001, running mem acc: 0.305
==>>> it: 401, avg. loss: 3.457769, running train acc: 0.193
==>>> it: 401, mem avg. loss: 2.808346, running mem acc: 0.312
[0.243 0.225 0.205 0.213 0.221 0.207 0.218 0.185 0.198 0.158]
no ratio: 0.0
on ratio: 0.9999817121120681
[(0, 939, 0, 0), (0, 922, 0, 906), (0, 892, 0, 1768), (0, 880, 0, 2585), (0, 861, 0, 3352), (0, 821, 0, 4029), (0, 801, 0, 4757), (0, 815, 0, 5468)]
[0.011884676061570645, 0.014824435010552407, 0.021827941715717315, 0.022587841510772706, 0.026036780416965485, 0.018957972675561905, 0.021287561923265457, 0.020565580353140833]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[0.00018465520406607538, 0.00018465235189069062, 0.00018464983440935612, 0.000184647215064615, 0.0001846451632445678, 0.0001846426457632333, 0.00018464047752786428, 0.0001846381783252582]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00024391025363001972, -0.00024392812338192016, -0.000243943024543114, -0.0002439624076941982, -0.00024397223023697734, -0.00024398027744609863, -0.00024399383983109146, -0.00024401269911322743]
-----------run 2 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.446209, running train acc: 0.200
==>>> it: 1, mem avg. loss: 2.547379, running mem acc: 0.400
==>>> it: 101, avg. loss: 3.426909, running train acc: 0.204
==>>> it: 101, mem avg. loss: 2.751316, running mem acc: 0.320
==>>> it: 201, avg. loss: 3.472133, running train acc: 0.197
==>>> it: 201, mem avg. loss: 2.690296, running mem acc: 0.334
==>>> it: 301, avg. loss: 3.445356, running train acc: 0.201
==>>> it: 301, mem avg. loss: 2.636807, running mem acc: 0.341
==>>> it: 401, avg. loss: 3.455310, running train acc: 0.199
==>>> it: 401, mem avg. loss: 2.622461, running mem acc: 0.342
[0.246 0.228 0.218 0.207 0.237 0.227 0.238 0.215 0.219 0.216]
no ratio: 0.0
on ratio: 0.9999838294982293
[(0, 939, 0, 0), (0, 922, 0, 906), (0, 892, 0, 1768), (0, 880, 0, 2585), (0, 861, 0, 3352), (0, 821, 0, 4029), (0, 801, 0, 4757), (0, 815, 0, 5468), (0, 781, 0, 6184)]
[0.011884676061570645, 0.014824435010552407, 0.021827941715717315, 0.022587841510772706, 0.026036780416965485, 0.018957972675561905, 0.021287561923265457, 0.020565580353140833, 0.029695202618837357]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[0.00018465520406607538, 0.00018465235189069062, 0.00018464983440935612, 0.000184647215064615, 0.0001846451632445678, 0.0001846426457632333, 0.00018464047752786428, 0.0001846381783252582, 0.0001846361847128719]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00024391025363001972, -0.00024392812338192016, -0.000243943024543114, -0.0002439624076941982, -0.00024397223023697734, -0.00024398027744609863, -0.00024399383983109146, -0.00024401269911322743, -0.00024402364215347916]
-----------run 2 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.402403, running train acc: 0.150
==>>> it: 1, mem avg. loss: 2.454132, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.464980, running train acc: 0.190
==>>> it: 101, mem avg. loss: 2.607786, running mem acc: 0.339
==>>> it: 201, avg. loss: 3.428734, running train acc: 0.194
==>>> it: 201, mem avg. loss: 2.590480, running mem acc: 0.338
==>>> it: 301, avg. loss: 3.427829, running train acc: 0.198
==>>> it: 301, mem avg. loss: 2.551801, running mem acc: 0.340
==>>> it: 401, avg. loss: 3.426222, running train acc: 0.197
==>>> it: 401, mem avg. loss: 2.531554, running mem acc: 0.347
[0.238 0.204 0.219 0.196 0.221 0.212 0.206 0.201 0.21  0.184]
no ratio: 0.0
on ratio: 0.9999859017918822
[(0, 939, 0, 0), (0, 922, 0, 906), (0, 892, 0, 1768), (0, 880, 0, 2585), (0, 861, 0, 3352), (0, 821, 0, 4029), (0, 801, 0, 4757), (0, 815, 0, 5468), (0, 781, 0, 6184), (0, 816, 0, 7093)]
[0.011884676061570645, 0.014824435010552407, 0.021827941715717315, 0.022587841510772706, 0.026036780416965485, 0.018957972675561905, 0.021287561923265457, 0.020565580353140833, 0.029695202618837357, 0.019677449628710746]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[0.00018465520406607538, 0.00018465235189069062, 0.00018464983440935612, 0.000184647215064615, 0.0001846451632445678, 0.0001846426457632333, 0.00018464047752786428, 0.0001846381783252582, 0.0001846361847128719, 0.00018463406013324857]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00024391025363001972, -0.00024392812338192016, -0.000243943024543114, -0.0002439624076941982, -0.00024397223023697734, -0.00024398027744609863, -0.00024399383983109146, -0.00024401269911322743, -0.00024402364215347916, -0.00024404376745224]
-----------run 2-----------avg_end_acc 0.2091-----------train time 263.3954281806946
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 3 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.673343, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.751984, running mem acc: 0.100
==>>> it: 101, avg. loss: 5.194115, running train acc: 0.022
==>>> it: 101, mem avg. loss: 3.070722, running mem acc: 0.257
==>>> it: 201, avg. loss: 4.968897, running train acc: 0.025
==>>> it: 201, mem avg. loss: 3.167541, running mem acc: 0.242
==>>> it: 301, avg. loss: 4.812376, running train acc: 0.031
==>>> it: 301, mem avg. loss: 3.258087, running mem acc: 0.228
==>>> it: 401, avg. loss: 4.710849, running train acc: 0.036
==>>> it: 401, mem avg. loss: 3.315966, running mem acc: 0.221
[0.051 0.06  0.048 0.042 0.059 0.055 0.063 0.056 0.052 0.059]
no ratio: 0.0
on ratio: 0.0
[(0, 949, 0, 0)]
[0.00040483274776488543]
[0]
[nan]
[3.5570189993450185e-06]
[nan]
[-0.0013037511380389333]
-----------run 3 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.741497, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.206033, running mem acc: 0.350
==>>> it: 101, avg. loss: 4.283534, running train acc: 0.068
==>>> it: 101, mem avg. loss: 3.558632, running mem acc: 0.180
==>>> it: 201, avg. loss: 4.252572, running train acc: 0.074
==>>> it: 201, mem avg. loss: 3.533416, running mem acc: 0.192
==>>> it: 301, avg. loss: 4.192437, running train acc: 0.076
==>>> it: 301, mem avg. loss: 3.554296, running mem acc: 0.188
==>>> it: 401, avg. loss: 4.173163, running train acc: 0.082
==>>> it: 401, mem avg. loss: 3.520570, running mem acc: 0.194
[0.112 0.104 0.09  0.108 0.124 0.106 0.107 0.084 0.079 0.089]
no ratio: 0.0
on ratio: 0.9998874000675599
[(0, 949, 0, 0), (0, 896, 0, 888)]
[0.00040483274776488543, -0.0026998923141509294]
[0, nan]
[nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06]
[nan, nan]
[-0.0013037511380389333, -0.001303770812228322]
-----------run 3 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.882352, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.703860, running mem acc: 0.150
==>>> it: 101, avg. loss: 3.949038, running train acc: 0.102
==>>> it: 101, mem avg. loss: 3.402231, running mem acc: 0.219
==>>> it: 201, avg. loss: 4.001911, running train acc: 0.098
==>>> it: 201, mem avg. loss: 3.403357, running mem acc: 0.208
==>>> it: 301, avg. loss: 3.997089, running train acc: 0.093
==>>> it: 301, mem avg. loss: 3.416906, running mem acc: 0.205
==>>> it: 401, avg. loss: 3.977642, running train acc: 0.098
==>>> it: 401, mem avg. loss: 3.419437, running mem acc: 0.206
[0.096 0.119 0.116 0.124 0.104 0.122 0.128 0.115 0.117 0.112]
no ratio: 0.0
on ratio: 0.999943980729371
[(0, 949, 0, 0), (0, 896, 0, 888), (0, 884, 0, 1785)]
[0.00040483274776488543, -0.0026998923141509294, -0.006446311634033918]
[0, nan, nan]
[nan, nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06, 3.551334202711587e-06]
[nan, nan, nan]
[-0.0013037511380389333, -0.001303770812228322, -0.001303776283748448]
-----------run 3 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.216941, running train acc: 0.150
==>>> it: 1, mem avg. loss: 3.345279, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.739145, running train acc: 0.136
==>>> it: 101, mem avg. loss: 3.388447, running mem acc: 0.210
==>>> it: 201, avg. loss: 3.780626, running train acc: 0.134
==>>> it: 201, mem avg. loss: 3.327601, running mem acc: 0.220
==>>> it: 301, avg. loss: 3.757586, running train acc: 0.140
==>>> it: 301, mem avg. loss: 3.309668, running mem acc: 0.220
==>>> it: 401, avg. loss: 3.742924, running train acc: 0.143
==>>> it: 401, mem avg. loss: 3.311924, running mem acc: 0.224
[0.136 0.136 0.117 0.136 0.115 0.117 0.119 0.089 0.074 0.088]
no ratio: 0.0
on ratio: 0.9999617019646893
[(0, 949, 0, 0), (0, 896, 0, 888), (0, 884, 0, 1785), (0, 864, 0, 2611)]
[0.00040483274776488543, -0.0026998923141509294, -0.006446311634033918, -0.006343717843294143]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06, 3.551334202711587e-06, 3.5487787499732804e-06]
[nan, nan, nan, nan]
[-0.0013037511380389333, -0.001303770812228322, -0.001303776283748448, -0.0013037893222644925]
-----------run 3 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.543213, running train acc: 0.200
==>>> it: 1, mem avg. loss: 3.388170, running mem acc: 0.200
==>>> it: 101, avg. loss: 3.711118, running train acc: 0.158
==>>> it: 101, mem avg. loss: 3.117339, running mem acc: 0.256
==>>> it: 201, avg. loss: 3.657743, running train acc: 0.162
==>>> it: 201, mem avg. loss: 3.109105, running mem acc: 0.247
==>>> it: 301, avg. loss: 3.654947, running train acc: 0.156
==>>> it: 301, mem avg. loss: 3.114141, running mem acc: 0.244
==>>> it: 401, avg. loss: 3.654368, running train acc: 0.155
==>>> it: 401, mem avg. loss: 3.123647, running mem acc: 0.243
[0.161 0.163 0.158 0.171 0.155 0.134 0.14  0.096 0.074 0.074]
no ratio: 0.0
on ratio: 0.9999701233903977
[(0, 949, 0, 0), (0, 896, 0, 888), (0, 884, 0, 1785), (0, 864, 0, 2611), (0, 845, 0, 3347)]
[0.00040483274776488543, -0.0026998923141509294, -0.006446311634033918, -0.006343717843294143, 0.014653402008116245]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06, 3.551334202711587e-06, 3.5487787499732804e-06, 3.546171001289622e-06]
[nan, nan, nan, nan, nan]
[-0.0013037511380389333, -0.001303770812228322, -0.001303776283748448, -0.0013037893222644925, -0.0013038000324741006]
-----------run 3 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.413161, running train acc: 0.100
==>>> it: 1, mem avg. loss: 3.018347, running mem acc: 0.300
==>>> it: 101, avg. loss: 3.508637, running train acc: 0.175
==>>> it: 101, mem avg. loss: 3.093172, running mem acc: 0.260
==>>> it: 201, avg. loss: 3.535687, running train acc: 0.168
==>>> it: 201, mem avg. loss: 2.987193, running mem acc: 0.270
==>>> it: 301, avg. loss: 3.531277, running train acc: 0.174
==>>> it: 301, mem avg. loss: 2.990177, running mem acc: 0.269
==>>> it: 401, avg. loss: 3.522305, running train acc: 0.175
==>>> it: 401, mem avg. loss: 2.947049, running mem acc: 0.277
[0.159 0.183 0.178 0.174 0.144 0.108 0.104 0.062 0.042 0.051]
no ratio: 0.0
on ratio: 0.9999759736671391
[(0, 949, 0, 0), (0, 896, 0, 888), (0, 884, 0, 1785), (0, 864, 0, 2611), (0, 845, 0, 3347), (0, 892, 0, 4162)]
[0.00040483274776488543, -0.0026998923141509294, -0.006446311634033918, -0.006343717843294143, 0.014653402008116245, 0.0003171005733311176]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06, 3.551334202711587e-06, 3.5487787499732804e-06, 3.546171001289622e-06, 3.5437124097370543e-06]
[nan, nan, nan, nan, nan, nan]
[-0.0013037511380389333, -0.001303770812228322, -0.001303776283748448, -0.0013037893222644925, -0.0013038000324741006, -0.001303812488913536]
-----------run 3 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.086322, running train acc: 0.100
==>>> it: 1, mem avg. loss: 3.217591, running mem acc: 0.150
==>>> it: 101, avg. loss: 3.605488, running train acc: 0.168
==>>> it: 101, mem avg. loss: 2.857602, running mem acc: 0.301
==>>> it: 201, avg. loss: 3.590902, running train acc: 0.163
==>>> it: 201, mem avg. loss: 2.794475, running mem acc: 0.308
==>>> it: 301, avg. loss: 3.576871, running train acc: 0.161
==>>> it: 301, mem avg. loss: 2.805998, running mem acc: 0.317
==>>> it: 401, avg. loss: 3.553364, running train acc: 0.167
==>>> it: 401, mem avg. loss: 2.814947, running mem acc: 0.311
[0.192 0.191 0.2   0.195 0.196 0.192 0.21  0.172 0.144 0.152]
no ratio: 0.0
on ratio: 0.9999793136261144
[(0, 949, 0, 0), (0, 896, 0, 888), (0, 884, 0, 1785), (0, 864, 0, 2611), (0, 845, 0, 3347), (0, 892, 0, 4162), (0, 790, 0, 4834)]
[0.00040483274776488543, -0.0026998923141509294, -0.006446311634033918, -0.006343717843294143, 0.014653402008116245, 0.0003171005733311176, -0.003466628521680832]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06, 3.551334202711587e-06, 3.5487787499732804e-06, 3.546171001289622e-06, 3.5437124097370543e-06, 3.541857267919113e-06]
[nan, nan, nan, nan, nan, nan, nan]
[-0.0013037511380389333, -0.001303770812228322, -0.001303776283748448, -0.0013037893222644925, -0.0013038000324741006, -0.001303812488913536, -0.0013038283213973045]
-----------run 3 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.454699, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.167957, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.471165, running train acc: 0.189
==>>> it: 101, mem avg. loss: 2.835682, running mem acc: 0.319
==>>> it: 201, avg. loss: 3.457748, running train acc: 0.200
==>>> it: 201, mem avg. loss: 2.765444, running mem acc: 0.314
==>>> it: 301, avg. loss: 3.475400, running train acc: 0.192
==>>> it: 301, mem avg. loss: 2.735950, running mem acc: 0.319
==>>> it: 401, avg. loss: 3.478170, running train acc: 0.194
==>>> it: 401, mem avg. loss: 2.732221, running mem acc: 0.324
[0.204 0.207 0.224 0.205 0.221 0.204 0.219 0.189 0.204 0.182]
no ratio: 0.0
on ratio: 0.9999818712496147
[(0, 949, 0, 0), (0, 896, 0, 888), (0, 884, 0, 1785), (0, 864, 0, 2611), (0, 845, 0, 3347), (0, 892, 0, 4162), (0, 790, 0, 4834), (0, 811, 0, 5516)]
[0.00040483274776488543, -0.0026998923141509294, -0.006446311634033918, -0.006343717843294143, 0.014653402008116245, 0.0003171005733311176, -0.003466628521680832, -0.003206501739099622]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06, 3.551334202711587e-06, 3.5487787499732804e-06, 3.546171001289622e-06, 3.5437124097370543e-06, 3.541857267919113e-06, 3.539361159710097e-06]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0013037511380389333, -0.001303770812228322, -0.001303776283748448, -0.0013037893222644925, -0.0013038000324741006, -0.001303812488913536, -0.0013038283213973045, -0.0013038459001109004]
-----------run 3 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.958080, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.369191, running mem acc: 0.450
==>>> it: 101, avg. loss: 3.443163, running train acc: 0.186
==>>> it: 101, mem avg. loss: 2.570762, running mem acc: 0.348
==>>> it: 201, avg. loss: 3.404528, running train acc: 0.201
==>>> it: 201, mem avg. loss: 2.584417, running mem acc: 0.347
==>>> it: 301, avg. loss: 3.412900, running train acc: 0.204
==>>> it: 301, mem avg. loss: 2.581414, running mem acc: 0.347
==>>> it: 401, avg. loss: 3.402738, running train acc: 0.206
==>>> it: 401, mem avg. loss: 2.605234, running mem acc: 0.340
[0.225 0.222 0.238 0.223 0.25  0.212 0.222 0.204 0.168 0.159]
no ratio: 0.0
on ratio: 0.9999838816266662
[(0, 949, 0, 0), (0, 896, 0, 888), (0, 884, 0, 1785), (0, 864, 0, 2611), (0, 845, 0, 3347), (0, 892, 0, 4162), (0, 790, 0, 4834), (0, 811, 0, 5516), (0, 832, 0, 6204)]
[0.00040483274776488543, -0.0026998923141509294, -0.006446311634033918, -0.006343717843294143, 0.014653402008116245, 0.0003171005733311176, -0.003466628521680832, -0.003206501739099622, -0.002051754283718765]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06, 3.551334202711587e-06, 3.5487787499732804e-06, 3.546171001289622e-06, 3.5437124097370543e-06, 3.541857267919113e-06, 3.539361159710097e-06, 3.5372900129004847e-06]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0013037511380389333, -0.001303770812228322, -0.001303776283748448, -0.0013037893222644925, -0.0013038000324741006, -0.001303812488913536, -0.0013038283213973045, -0.0013038459001109004, -0.00130385160446167]
-----------run 3 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.797707, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.952540, running mem acc: 0.300
==>>> it: 101, avg. loss: 3.401514, running train acc: 0.202
==>>> it: 101, mem avg. loss: 2.542592, running mem acc: 0.361
==>>> it: 201, avg. loss: 3.424228, running train acc: 0.201
==>>> it: 201, mem avg. loss: 2.519545, running mem acc: 0.366
==>>> it: 301, avg. loss: 3.406392, running train acc: 0.205
==>>> it: 301, mem avg. loss: 2.481120, running mem acc: 0.379
==>>> it: 401, avg. loss: 3.425165, running train acc: 0.198
==>>> it: 401, mem avg. loss: 2.458363, running mem acc: 0.386
[0.226 0.209 0.227 0.237 0.244 0.233 0.237 0.217 0.21  0.231]
no ratio: 0.0
on ratio: 0.9999856323903392
[(0, 949, 0, 0), (0, 896, 0, 888), (0, 884, 0, 1785), (0, 864, 0, 2611), (0, 845, 0, 3347), (0, 892, 0, 4162), (0, 790, 0, 4834), (0, 811, 0, 5516), (0, 832, 0, 6204), (0, 769, 0, 6960)]
[0.00040483274776488543, -0.0026998923141509294, -0.006446311634033918, -0.006343717843294143, 0.014653402008116245, 0.0003171005733311176, -0.003466628521680832, -0.003206501739099622, -0.002051754283718765, 0.001589831128017977]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[3.5570189993450185e-06, 3.553949454726535e-06, 3.551334202711587e-06, 3.5487787499732804e-06, 3.546171001289622e-06, 3.5437124097370543e-06, 3.541857267919113e-06, 3.539361159710097e-06, 3.5372900129004847e-06, 3.5351069982425543e-06]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0013037511380389333, -0.001303770812228322, -0.001303776283748448, -0.0013037893222644925, -0.0013038000324741006, -0.001303812488913536, -0.0013038283213973045, -0.0013038459001109004, -0.00130385160446167, -0.0013038659235462546]
-----------run 3-----------avg_end_acc 0.2271-----------train time 264.4152035713196
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 4 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 7.795618, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.705307, running mem acc: 0.100
==>>> it: 101, avg. loss: 5.131135, running train acc: 0.026
==>>> it: 101, mem avg. loss: 3.043801, running mem acc: 0.299
==>>> it: 201, avg. loss: 4.960015, running train acc: 0.031
==>>> it: 201, mem avg. loss: 3.104928, running mem acc: 0.288
==>>> it: 301, avg. loss: 4.825449, running train acc: 0.034
==>>> it: 301, mem avg. loss: 3.180945, running mem acc: 0.274
==>>> it: 401, avg. loss: 4.713183, running train acc: 0.041
==>>> it: 401, mem avg. loss: 3.263183, running mem acc: 0.259
[0.07  0.074 0.059 0.064 0.075 0.048 0.057 0.061 0.069 0.058]
no ratio: 0.0
on ratio: 0.0
[(0, 930, 0, 0)]
[-0.0015192372677847743]
[0]
[nan]
[-0.00011748724500648677]
[nan]
[-0.0007525754044763744]
-----------run 4 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.704317, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.749530, running mem acc: 0.250
==>>> it: 101, avg. loss: 4.303469, running train acc: 0.061
==>>> it: 101, mem avg. loss: 3.522171, running mem acc: 0.205
==>>> it: 201, avg. loss: 4.297803, running train acc: 0.060
==>>> it: 201, mem avg. loss: 3.460258, running mem acc: 0.209
==>>> it: 301, avg. loss: 4.238800, running train acc: 0.066
==>>> it: 301, mem avg. loss: 3.453967, running mem acc: 0.211
==>>> it: 401, avg. loss: 4.206355, running train acc: 0.070
==>>> it: 401, mem avg. loss: 3.436578, running mem acc: 0.216
[0.105 0.089 0.091 0.075 0.092 0.089 0.085 0.078 0.076 0.069]
no ratio: 0.0
on ratio: 0.9998882806390347
[(0, 930, 0, 0), (0, 911, 0, 895)]
[-0.0015192372677847743, -0.012663929037749768]
[0, nan]
[nan, nan]
[-0.00011748724500648677, -0.00011748984979931265]
[nan, nan]
[-0.0007525754044763744, -0.0007525871624238789]
-----------run 4 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.203503, running train acc: 0.100
==>>> it: 1, mem avg. loss: 3.640825, running mem acc: 0.150
==>>> it: 101, avg. loss: 3.965631, running train acc: 0.108
==>>> it: 101, mem avg. loss: 3.367345, running mem acc: 0.211
==>>> it: 201, avg. loss: 3.960741, running train acc: 0.102
==>>> it: 201, mem avg. loss: 3.375028, running mem acc: 0.202
==>>> it: 301, avg. loss: 3.951221, running train acc: 0.106
==>>> it: 301, mem avg. loss: 3.364639, running mem acc: 0.206
==>>> it: 401, avg. loss: 3.949799, running train acc: 0.106
==>>> it: 401, mem avg. loss: 3.345893, running mem acc: 0.213
[0.155 0.143 0.135 0.132 0.107 0.103 0.086 0.071 0.08  0.061]
no ratio: 0.0
on ratio: 0.999941249045297
[(0, 930, 0, 0), (0, 911, 0, 895), (0, 865, 0, 1702)]
[-0.0015192372677847743, -0.012663929037749768, -0.017040927708148957]
[0, nan, nan]
[nan, nan, nan]
[-0.00011748724500648677, -0.00011748984979931265, -0.00011749251279979944]
[nan, nan, nan]
[-0.0007525754044763744, -0.0007525871624238789, -0.0007526021800003946]
-----------run 4 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.617613, running train acc: 0.250
==>>> it: 1, mem avg. loss: 3.674433, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.820129, running train acc: 0.130
==>>> it: 101, mem avg. loss: 3.329160, running mem acc: 0.213
==>>> it: 201, avg. loss: 3.753984, running train acc: 0.142
==>>> it: 201, mem avg. loss: 3.330743, running mem acc: 0.210
==>>> it: 301, avg. loss: 3.750833, running train acc: 0.145
==>>> it: 301, mem avg. loss: 3.282469, running mem acc: 0.214
==>>> it: 401, avg. loss: 3.756658, running train acc: 0.142
==>>> it: 401, mem avg. loss: 3.247703, running mem acc: 0.223
[0.156 0.146 0.124 0.122 0.132 0.08  0.088 0.072 0.057 0.047]
no ratio: 0.0
on ratio: 0.9999611514704169
[(0, 930, 0, 0), (0, 911, 0, 895), (0, 865, 0, 1702), (0, 878, 0, 2574)]
[-0.0015192372677847743, -0.012663929037749768, -0.017040927708148957, -0.02460981546342373]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[-0.00011748724500648677, -0.00011748984979931265, -0.00011749251279979944, -0.00011749492114176974]
[nan, nan, nan, nan]
[-0.0007525754044763744, -0.0007525871624238789, -0.0007526021800003946, -0.000752620748244226]
-----------run 4 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.679158, running train acc: 0.200
==>>> it: 1, mem avg. loss: 3.375245, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.700338, running train acc: 0.137
==>>> it: 101, mem avg. loss: 3.135669, running mem acc: 0.237
==>>> it: 201, avg. loss: 3.694187, running train acc: 0.142
==>>> it: 201, mem avg. loss: 3.123274, running mem acc: 0.242
==>>> it: 301, avg. loss: 3.667360, running train acc: 0.151
==>>> it: 301, mem avg. loss: 3.089089, running mem acc: 0.253
==>>> it: 401, avg. loss: 3.635201, running train acc: 0.154
==>>> it: 401, mem avg. loss: 3.057073, running mem acc: 0.262
[0.208 0.182 0.187 0.183 0.189 0.172 0.156 0.138 0.11  0.079]
no ratio: 0.0
on ratio: 0.9999691367550384
[(0, 930, 0, 0), (0, 911, 0, 895), (0, 865, 0, 1702), (0, 878, 0, 2574), (0, 811, 0, 3240)]
[-0.0015192372677847743, -0.012663929037749768, -0.017040927708148957, -0.02460981546342373, -0.019276214748620988]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[-0.00011748724500648677, -0.00011748984979931265, -0.00011749251279979944, -0.00011749492114176974, -0.00011749765690183267]
[nan, nan, nan, nan, nan]
[-0.0007525754044763744, -0.0007525871624238789, -0.0007526021800003946, -0.000752620748244226, -0.0007526415283791721]
-----------run 4 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 2.931175, running train acc: 0.300
==>>> it: 1, mem avg. loss: 2.980787, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.504605, running train acc: 0.183
==>>> it: 101, mem avg. loss: 2.953204, running mem acc: 0.280
==>>> it: 201, avg. loss: 3.482771, running train acc: 0.190
==>>> it: 201, mem avg. loss: 2.964311, running mem acc: 0.278
==>>> it: 301, avg. loss: 3.495636, running train acc: 0.181
==>>> it: 301, mem avg. loss: 2.950687, running mem acc: 0.289
==>>> it: 401, avg. loss: 3.518868, running train acc: 0.179
==>>> it: 401, mem avg. loss: 2.942423, running mem acc: 0.291
[0.186 0.152 0.174 0.172 0.209 0.171 0.188 0.157 0.157 0.107]
no ratio: 0.0
on ratio: 0.999975651919846
[(0, 930, 0, 0), (0, 911, 0, 895), (0, 865, 0, 1702), (0, 878, 0, 2574), (0, 811, 0, 3240), (0, 829, 0, 4107)]
[-0.0015192372677847743, -0.012663929037749768, -0.017040927708148957, -0.02460981546342373, -0.019276214748620988, -0.02102183835208416]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[-0.00011748724500648677, -0.00011748984979931265, -0.00011749251279979944, -0.00011749492114176974, -0.00011749765690183267, -0.00011749993427656591]
[nan, nan, nan, nan, nan, nan]
[-0.0007525754044763744, -0.0007525871624238789, -0.0007526021800003946, -0.000752620748244226, -0.0007526415283791721, -0.0007526495610363781]
-----------run 4 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.218900, running train acc: 0.150
==>>> it: 1, mem avg. loss: 3.917803, running mem acc: 0.100
==>>> it: 101, avg. loss: 3.406726, running train acc: 0.214
==>>> it: 101, mem avg. loss: 3.041769, running mem acc: 0.253
==>>> it: 201, avg. loss: 3.421866, running train acc: 0.204
==>>> it: 201, mem avg. loss: 2.969857, running mem acc: 0.270
==>>> it: 301, avg. loss: 3.463893, running train acc: 0.193
==>>> it: 301, mem avg. loss: 2.934361, running mem acc: 0.275
==>>> it: 401, avg. loss: 3.435346, running train acc: 0.198
==>>> it: 401, mem avg. loss: 2.895977, running mem acc: 0.279
[0.246 0.22  0.213 0.205 0.219 0.189 0.168 0.161 0.141 0.107]
no ratio: 0.0
on ratio: 0.9999787600093455
[(0, 930, 0, 0), (0, 911, 0, 895), (0, 865, 0, 1702), (0, 878, 0, 2574), (0, 811, 0, 3240), (0, 829, 0, 4107), (0, 832, 0, 4708)]
[-0.0015192372677847743, -0.012663929037749768, -0.017040927708148957, -0.02460981546342373, -0.019276214748620988, -0.02102183835208416, -0.028717688694596292]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[-0.00011748724500648677, -0.00011748984979931265, -0.00011749251279979944, -0.00011749492114176974, -0.00011749765690183267, -0.00011749993427656591, -0.0001175020806840621]
[nan, nan, nan, nan, nan, nan, nan]
[-0.0007525754044763744, -0.0007525871624238789, -0.0007526021800003946, -0.000752620748244226, -0.0007526415283791721, -0.0007526495610363781, -0.0007526614936068654]
-----------run 4 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.179486, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.532827, running mem acc: 0.400
==>>> it: 101, avg. loss: 3.411491, running train acc: 0.190
==>>> it: 101, mem avg. loss: 2.816799, running mem acc: 0.305
==>>> it: 201, avg. loss: 3.404377, running train acc: 0.195
==>>> it: 201, mem avg. loss: 2.833267, running mem acc: 0.297
==>>> it: 301, avg. loss: 3.420392, running train acc: 0.200
==>>> it: 301, mem avg. loss: 2.788659, running mem acc: 0.307
==>>> it: 401, avg. loss: 3.423651, running train acc: 0.199
==>>> it: 401, mem avg. loss: 2.749258, running mem acc: 0.317
[0.235 0.216 0.214 0.216 0.224 0.213 0.198 0.217 0.185 0.144]
no ratio: 0.0
on ratio: 0.9999817654674422
[(0, 930, 0, 0), (0, 911, 0, 895), (0, 865, 0, 1702), (0, 878, 0, 2574), (0, 811, 0, 3240), (0, 829, 0, 4107), (0, 832, 0, 4708), (0, 783, 0, 5484)]
[-0.0015192372677847743, -0.012663929037749768, -0.017040927708148957, -0.02460981546342373, -0.019276214748620988, -0.02102183835208416, -0.028717688694596292, -0.03100155784189701]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00011748724500648677, -0.00011748984979931265, -0.00011749251279979944, -0.00011749492114176974, -0.00011749765690183267, -0.00011749993427656591, -0.0001175020806840621, -0.00011750388512155041]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0007525754044763744, -0.0007525871624238789, -0.0007526021800003946, -0.000752620748244226, -0.0007526415283791721, -0.0007526495610363781, -0.0007526614936068654, -0.0007526716217398643]
-----------run 4 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.306333, running train acc: 0.250
==>>> it: 1, mem avg. loss: 2.485614, running mem acc: 0.400
==>>> it: 101, avg. loss: 3.521595, running train acc: 0.187
==>>> it: 101, mem avg. loss: 2.572418, running mem acc: 0.342
==>>> it: 201, avg. loss: 3.484720, running train acc: 0.186
==>>> it: 201, mem avg. loss: 2.604927, running mem acc: 0.337
==>>> it: 301, avg. loss: 3.480223, running train acc: 0.186
==>>> it: 301, mem avg. loss: 2.621830, running mem acc: 0.342
==>>> it: 401, avg. loss: 3.467059, running train acc: 0.191
==>>> it: 401, mem avg. loss: 2.607107, running mem acc: 0.344
[0.242 0.231 0.241 0.221 0.243 0.225 0.213 0.216 0.196 0.157]
no ratio: 0.0
on ratio: 0.9999837875520824
[(0, 930, 0, 0), (0, 911, 0, 895), (0, 865, 0, 1702), (0, 878, 0, 2574), (0, 811, 0, 3240), (0, 829, 0, 4107), (0, 832, 0, 4708), (0, 783, 0, 5484), (0, 804, 0, 6168)]
[-0.0015192372677847743, -0.012663929037749768, -0.017040927708148957, -0.02460981546342373, -0.019276214748620988, -0.02102183835208416, -0.028717688694596292, -0.03100155784189701, -0.019308604419231414]
[0, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00011748724500648677, -0.00011748984979931265, -0.00011749251279979944, -0.00011749492114176974, -0.00011749765690183267, -0.00011749993427656591, -0.0001175020806840621, -0.00011750388512155041, -0.00011750635167118162]
[nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0007525754044763744, -0.0007525871624238789, -0.0007526021800003946, -0.000752620748244226, -0.0007526415283791721, -0.0007526495610363781, -0.0007526614936068654, -0.0007526716217398643, -0.0007526771514676511]
-----------run 4 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.616041, running train acc: 0.150
==>>> it: 1, mem avg. loss: 2.355995, running mem acc: 0.350
==>>> it: 101, avg. loss: 3.388926, running train acc: 0.214
==>>> it: 101, mem avg. loss: 2.553187, running mem acc: 0.349
==>>> it: 201, avg. loss: 3.377979, running train acc: 0.213
==>>> it: 201, mem avg. loss: 2.539115, running mem acc: 0.358
==>>> it: 301, avg. loss: 3.392310, running train acc: 0.210
==>>> it: 301, mem avg. loss: 2.505238, running mem acc: 0.368
==>>> it: 401, avg. loss: 3.370449, running train acc: 0.209
==>>> it: 401, mem avg. loss: 2.490248, running mem acc: 0.371
[0.256 0.244 0.224 0.232 0.239 0.239 0.212 0.215 0.222 0.2  ]
no ratio: 0.0
on ratio: 0.9999855430744098
[(0, 930, 0, 0), (0, 911, 0, 895), (0, 865, 0, 1702), (0, 878, 0, 2574), (0, 811, 0, 3240), (0, 829, 0, 4107), (0, 832, 0, 4708), (0, 783, 0, 5484), (0, 804, 0, 6168), (0, 800, 0, 6917)]
[-0.0015192372677847743, -0.012663929037749768, -0.017040927708148957, -0.02460981546342373, -0.019276214748620988, -0.02102183835208416, -0.028717688694596292, -0.03100155784189701, -0.019308604419231414, -0.016252788588404656]
[0, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.00011748724500648677, -0.00011748984979931265, -0.00011749251279979944, -0.00011749492114176974, -0.00011749765690183267, -0.00011749993427656591, -0.0001175020806840621, -0.00011750388512155041, -0.00011750635167118162, -0.00011750845442293212]
[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0007525754044763744, -0.0007525871624238789, -0.0007526021800003946, -0.000752620748244226, -0.0007526415283791721, -0.0007526495610363781, -0.0007526614936068654, -0.0007526716217398643, -0.0007526771514676511, -0.0007526874542236328]
-----------run 4-----------avg_end_acc 0.22830000000000003-----------train time 263.2923264503479
----------- Total 5 run: 1322.4576787948608s -----------
----------- Avg_End_Acc (0.22284, 0.013483236159974689) Avg_End_Fgt (0.00658, 0.008642909057710963) Avg_Acc (0.15894517460317462, 0.008197987244485403) Avg_Bwtp (0.07526666666666665, 0.008194215893040722) Avg_Fwt (0.09708444444444445, 0.007140934822259973)-----------
