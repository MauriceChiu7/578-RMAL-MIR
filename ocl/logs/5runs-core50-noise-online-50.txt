Namespace(num_runs=5, seed=0, val_size=0.1, num_val=3, num_runs_val=3, error_analysis=True, verbose=True, store=False, save_path=None, agent='ER', update='random', retrieve='MIR', optimizer='SGD', learning_rate=0.1, epoch=1, batch=10, test_batch=128, weight_decay=0, num_tasks=10, fix_order=False, plot_sample=False, data='core50', cl_type='ni', ns_factor=[0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6], ns_type='noise', ns_task=(1, 1, 2, 2, 2, 2), online=True, mem_size=5000, eps_mem_batch=10, lambda_=100, alpha=0.9, fisher_update_after=50, subsample=50, gss_mem_strength=10, gss_batch_size=10, k=5, aser_type='asvm', n_smp_cls=2.0, stm_capacity=1000, classifier_chill=0.01, log_alpha=-300, minlr=0.0005, clip=10.0, mem_epoch=70, labels_trick=False, separated_softmax=False, kd_trick=False, kd_trick_star=False, review_trick=False, ncm_trick=False, mem_iters=1, min_delta=0.0, patience=0, cumulative_delta=False, temp=0.07, buffer_tracker=False, warmup=4, head='mlp', budget=0.5, cuda=True)
Setting up data stream
Loading paths...
Loading LUP...
Loading labels...
data setup time: 0.4691798686981201
Loading test set...
buffer has 5000 slots
Loading data...
loading time 5.279415607452393
-----------run 0 training batch 0-------------
size: (13492, 128, 128, 3), (13492,)
==>>> it: 1, avg. loss: 12.275919, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.495718, running mem acc: 0.200
[0.02001245]
no ratio: 0.0
on ratio: 0.0
[(0, 44072, 0, 0)]
[-0.23138392563234347]
[0]
[nan]
[-6.834811210865155e-05]
[nan]
[-0.0030449863988906145]
Loading data...
loading time 5.360082626342773
-----------run 0 training batch 1-------------
size: (13496, 128, 128, 3), (13496,)
==>>> it: 1, avg. loss: 13.892807, running train acc: 0.050
==>>> it: 1, mem avg. loss: 9.945140, running mem acc: 0.300
==>>> it: 801, avg. loss: 5.590459, running train acc: 0.021
==>>> it: 801, mem avg. loss: 5.718923, running mem acc: 0.065
[0.01854487]
no ratio: 0.0
on ratio: 0.9999977343836731
[(0, 44072, 0, 0), (0, 0, 0, 44138)]
[-0.23138392563234347, 0]
[0, nan]
[nan, nan]
[-6.834811210865155e-05, -6.834813393652439e-05]
[nan, nan]
[-0.0030449863988906145, -0.0030449875630438328]
Loading data...
loading time 5.1537394523620605
-----------run 0 training batch 2-------------
size: (13488, 128, 128, 3), (13488,)
==>>> it: 1, avg. loss: 4.561456, running train acc: 0.050
==>>> it: 1, mem avg. loss: 6.582056, running mem acc: 0.000
==>>> it: 501, avg. loss: 4.735295, running train acc: 0.018
==>>> it: 501, mem avg. loss: 6.121836, running mem acc: 0.026
==>>> it: 801, avg. loss: 4.555225, running train acc: 0.019
==>>> it: 801, mem avg. loss: 5.823499, running mem acc: 0.023
==>>> it: 901, avg. loss: 4.548491, running train acc: 0.018
==>>> it: 901, mem avg. loss: 5.849108, running mem acc: 0.024
==>>> it: 1301, avg. loss: 4.482286, running train acc: 0.015
==>>> it: 1301, mem avg. loss: 5.740764, running mem acc: 0.026
[0.01965668]
no ratio: 0.0
on ratio: 0.9999977318142538
[(0, 44072, 0, 0), (0, 0, 0, 44138), (0, 0, 0, 44088)]
[-0.23138392563234347, 0, 0]
[0, nan, nan]
[nan, nan, nan]
[-6.834811210865155e-05, -6.834813393652439e-05, -6.834813393652439e-05]
[nan, nan, nan]
[-0.0030449863988906145, -0.0030449875630438328, -0.0030449875630438328]
Loading data...
loading time 5.569323778152466
-----------run 0 training batch 3-------------
size: (13495, 128, 128, 3), (13495,)
==>>> it: 1, avg. loss: 4.862079, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.725782, running mem acc: 0.000
==>>> it: 201, avg. loss: 4.729146, running train acc: 0.038
==>>> it: 201, mem avg. loss: 9.424796, running mem acc: 0.022
==>>> it: 301, avg. loss: 4.609832, running train acc: 0.039
==>>> it: 301, mem avg. loss: 9.108807, running mem acc: 0.019
==>>> it: 501, avg. loss: 4.411578, running train acc: 0.033
==>>> it: 501, mem avg. loss: 9.001945, running mem acc: 0.022
==>>> it: 601, avg. loss: 4.346567, running train acc: 0.028
==>>> it: 601, mem avg. loss: 8.875692, running mem acc: 0.022
==>>> it: 801, avg. loss: 4.251763, running train acc: 0.022
==>>> it: 801, mem avg. loss: 8.833335, running mem acc: 0.022
==>>> it: 1101, avg. loss: 4.195537, running train acc: 0.017
==>>> it: 1101, mem avg. loss: 8.618813, running mem acc: 0.024
==>>> it: 1201, avg. loss: 4.188665, running train acc: 0.016
==>>> it: 1201, mem avg. loss: 8.510503, running mem acc: 0.024
[0.01614338]
no ratio: 0.0
on ratio: 0.9999977399138004
[(0, 44072, 0, 0), (0, 0, 0, 44138), (0, 0, 0, 44088), (0, 0, 0, 44246)]
[-0.23138392563234347, 0, 0, 0]
[0, nan, nan, nan]
[nan, nan, nan, nan]
[-6.834811210865155e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05]
[nan, nan, nan, nan]
[-0.0030449863988906145, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328]
Loading data...
loading time 5.4079461097717285
-----------run 0 training batch 4-------------
size: (13491, 128, 128, 3), (13491,)
==>>> it: 1, avg. loss: 5.239136, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.256942, running mem acc: 0.000
==>>> it: 101, avg. loss: 4.812349, running train acc: 0.011
==>>> it: 101, mem avg. loss: 4.633949, running mem acc: 0.013
==>>> it: 1301, avg. loss: 4.153864, running train acc: 0.003
==>>> it: 1301, mem avg. loss: 4.525238, running mem acc: 0.020
[0.01907854]
no ratio: 0.0
on ratio: 0.9999977331510788
[(0, 44072, 0, 0), (0, 0, 0, 44138), (0, 0, 0, 44088), (0, 0, 0, 44246), (0, 0, 0, 44114)]
[-0.23138392563234347, 0, 0, 0, 0]
[0, nan, nan, nan, nan]
[nan, nan, nan, nan, nan]
[-6.834811210865155e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05]
[nan, nan, nan, nan, nan]
[-0.0030449863988906145, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328]
Loading data...
loading time 5.193921327590942
-----------run 0 training batch 5-------------
size: (13491, 128, 128, 3), (13491,)
==>>> it: 1, avg. loss: 5.042814, running train acc: 0.050
==>>> it: 1, mem avg. loss: 4.252863, running mem acc: 0.050
==>>> it: 501, avg. loss: 4.773209, running train acc: 0.015
==>>> it: 501, mem avg. loss: 4.467592, running mem acc: 0.017
==>>> it: 601, avg. loss: 4.711106, running train acc: 0.014
==>>> it: 601, mem avg. loss: 4.477701, running mem acc: 0.019
==>>> it: 801, avg. loss: 4.586312, running train acc: 0.011
==>>> it: 801, mem avg. loss: 4.477308, running mem acc: 0.019
==>>> it: 901, avg. loss: 4.566706, running train acc: 0.010
==>>> it: 901, mem avg. loss: 4.474480, running mem acc: 0.019
[0.02007916]
no ratio: 0.0
on ratio: 0.9999977308363457
[(0, 44072, 0, 0), (0, 0, 0, 44138), (0, 0, 0, 44088), (0, 0, 0, 44246), (0, 0, 0, 44114), (0, 0, 0, 44069)]
[-0.23138392563234347, 0, 0, 0, 0, 0]
[0, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan]
[-6.834811210865155e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05]
[nan, nan, nan, nan, nan, nan]
[-0.0030449863988906145, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328]
Loading data...
loading time 5.398337364196777
-----------run 0 training batch 6-------------
size: (13470, 128, 128, 3), (13470,)
==>>> it: 1, avg. loss: 4.537344, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.536276, running mem acc: 0.100
==>>> it: 201, avg. loss: 4.717088, running train acc: 0.000
==>>> it: 201, mem avg. loss: 4.431308, running mem acc: 0.007
==>>> it: 301, avg. loss: 4.705358, running train acc: 0.000
==>>> it: 301, mem avg. loss: 4.363933, running mem acc: 0.010
==>>> it: 801, avg. loss: 4.440993, running train acc: 0.000
==>>> it: 801, mem avg. loss: 4.373371, running mem acc: 0.011
==>>> it: 901, avg. loss: 4.416351, running train acc: 0.000
==>>> it: 901, mem avg. loss: 4.370350, running mem acc: 0.012
==>>> it: 1001, avg. loss: 4.454021, running train acc: 0.000
==>>> it: 1001, mem avg. loss: 4.362787, running mem acc: 0.012
[0.01965668]
no ratio: 0.0
on ratio: 0.9999977318142538
[(0, 44072, 0, 0), (0, 0, 0, 44138), (0, 0, 0, 44088), (0, 0, 0, 44246), (0, 0, 0, 44114), (0, 0, 0, 44069), (0, 0, 0, 44088)]
[-0.23138392563234347, 0, 0, 0, 0, 0, 0]
[0, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan]
[-6.834811210865155e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05]
[nan, nan, nan, nan, nan, nan, nan]
[-0.0030449863988906145, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328]
Loading data...
loading time 5.1190855503082275
-----------run 0 training batch 7-------------
size: (13486, 128, 128, 3), (13486,)
==>>> it: 1, avg. loss: 5.285083, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.656804, running mem acc: 0.000
==>>> it: 601, avg. loss: 4.700932, running train acc: 0.022
==>>> it: 601, mem avg. loss: 4.687556, running mem acc: 0.015
==>>> it: 801, avg. loss: 4.668019, running train acc: 0.017
==>>> it: 801, mem avg. loss: 4.651222, running mem acc: 0.014
==>>> it: 1201, avg. loss: 4.581757, running train acc: 0.021
==>>> it: 1201, mem avg. loss: 4.571807, running mem acc: 0.015
[0.02063506]
no ratio: 0.0
on ratio: 0.9999977295483391
[(0, 44072, 0, 0), (0, 0, 0, 44138), (0, 0, 0, 44088), (0, 0, 0, 44246), (0, 0, 0, 44114), (0, 0, 0, 44069), (0, 0, 0, 44088), (0, 0, 0, 44044)]
[-0.23138392563234347, 0, 0, 0, 0, 0, 0, 0]
[0, nan, nan, nan, nan, nan, nan, nan]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-6.834811210865155e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05, -6.834813393652439e-05]
[nan, nan, nan, nan, nan, nan, nan, nan]
[-0.0030449863988906145, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328, -0.0030449875630438328]
-----------run 0-----------avg_end_acc 0.020635061816241217-----------train time 1040.0979180335999
Loading test set...
buffer has 5000 slots
Loading data...
loading time 5.182502508163452
-----------run 1 training batch 0-------------
size: (13488, 128, 128, 3), (13488,)
==>>> it: 1, avg. loss: 8.310175, running train acc: 0.050
==>>> it: 1, mem avg. loss: 2.555923, running mem acc: 0.300
[0.01996798]
no ratio: 0.0
on ratio: 0.0
[(0, 44074, 0, 0)]
[-1.529909082974423]
[0]
[nan]
[4.444910246093059e-06]
[nan]
[-0.0013167548459023237]
Loading data...
loading time 5.762168884277344
-----------run 1 training batch 1-------------
size: (13495, 128, 128, 3), (13495,)
==>>> it: 1, avg. loss: 28.696277, running train acc: 0.000
==>>> it: 1, mem avg. loss: 13.913164, running mem acc: 0.200
[0.02134662]
no ratio: 0.0
on ratio: 0.9999977278975555
[(0, 44074, 0, 0), (0, 0, 0, 44012)]
[-1.529909082974423, 0]
[0, nan]
[nan, nan]
[4.444910246093059e-06, 4.44485931438976e-06]
[nan, nan]
[-0.0013167548459023237, -0.0013167555443942547]
Loading data...
loading time 5.34639310836792
-----------run 1 training batch 2-------------
size: (13491, 128, 128, 3), (13491,)
==>>> it: 1, avg. loss: 15.324402, running train acc: 0.000
==>>> it: 1, mem avg. loss: 5.401527, running mem acc: 0.200
==>>> it: 201, avg. loss: 6.826929, running train acc: 0.009
==>>> it: 201, mem avg. loss: 4.212582, running mem acc: 0.051
==>>> it: 501, avg. loss: 5.638512, running train acc: 0.010
==>>> it: 501, mem avg. loss: 4.201608, running mem acc: 0.031
==>>> it: 601, avg. loss: 5.486158, running train acc: 0.009
==>>> it: 601, mem avg. loss: 4.207520, running mem acc: 0.028
==>>> it: 1201, avg. loss: 4.943770, running train acc: 0.005
==>>> it: 1201, mem avg. loss: 4.198624, running mem acc: 0.022
==>>> it: 1301, avg. loss: 4.910486, running train acc: 0.007
==>>> it: 1301, mem avg. loss: 4.199456, running mem acc: 0.021
[0.01747754]
no ratio: 0.0
on ratio: 0.9999977368448449
[(0, 44074, 0, 0), (0, 0, 0, 44012), (0, 0, 0, 44186)]
[-1.529909082974423, 0, 0]
[0, nan, nan]
[nan, nan, nan]
[4.444910246093059e-06, 4.44485931438976e-06, 4.444850674190093e-06]
[nan, nan, nan]
[-0.0013167548459023237, -0.0013167555443942547, -0.0013167571742087603]
Loading data...
loading time 4.944761276245117
-----------run 1 training batch 3-------------
size: (13486, 128, 128, 3), (13486,)
==>>> it: 1, avg. loss: 4.638969, running train acc: 0.000
==>>> it: 1, mem avg. loss: 4.378864, running mem acc: 0.000
==>>> it: 101, avg. loss: 4.601863, running train acc: 0.044
==>>> it: 101, mem avg. loss: 4.441145, running mem acc: 0.015
==>>> it: 201, avg. loss: 4.683310, running train acc: 0.041
==>>> it: 201, mem avg. loss: 4.480894, running mem acc: 0.013
==>>> it: 401, avg. loss: 4.638080, running train acc: 0.049
==>>> it: 401, mem avg. loss: 4.446647, running mem acc: 0.020
==>>> it: 501, avg. loss: 4.576467, running train acc: 0.049
==>>> it: 501, mem avg. loss: 4.431196, running mem acc: 0.021
==>>> it: 801, avg. loss: 4.716219, running train acc: 0.046
==>>> it: 801, mem avg. loss: 4.426688, running mem acc: 0.027
