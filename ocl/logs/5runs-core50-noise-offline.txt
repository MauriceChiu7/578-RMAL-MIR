Namespace(num_runs=5, seed=0, val_size=0.1, num_val=3, num_runs_val=3, error_analysis=True, verbose=True, store=False, save_path=None, agent='ER', update='random', retrieve='MIR', optimizer='SGD', learning_rate=0.1, epoch=1, batch=10, test_batch=128, weight_decay=0, num_tasks=10, fix_order=False, plot_sample=False, data='core50', cl_type='ni', ns_factor=[0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6], ns_type='noise', ns_task=(1, 1, 2, 2, 2, 2), online=False, mem_size=5000, eps_mem_batch=10, lambda_=100, alpha=0.9, fisher_update_after=50, subsample=50, gss_mem_strength=10, gss_batch_size=10, k=5, aser_type='asvm', n_smp_cls=2.0, stm_capacity=1000, classifier_chill=0.01, log_alpha=-300, minlr=0.0005, clip=10.0, mem_epoch=70, labels_trick=False, separated_softmax=False, kd_trick=False, kd_trick_star=False, review_trick=False, ncm_trick=False, mem_iters=1, min_delta=0.0, patience=0, cumulative_delta=False, temp=0.07, buffer_tracker=False, warmup=4, head='mlp', budget=1.0, cuda=True)
Setting up data stream
Loading paths...
Loading LUP...
Loading labels...
data setup time: 0.5203351974487305
Loading test set...
buffer has 5000 slots
Loading data...
loading time 14.63441252708435
Loading data...
loading time 14.210375547409058
Loading data...
loading time 12.715760469436646
Loading data...
loading time 13.632940292358398
Loading data...
loading time 13.297114849090576
Loading data...
loading time 14.357815265655518
Loading data...
loading time 14.166406154632568
Loading data...
loading time 12.618113040924072
Training Start
----------run 0 training-------------
size: (107909, 128, 128, 3), (107909,)
==>>> it: 1, avg. loss: 14.888034, running train acc: 0.050
==>>> it: 1, mem avg. loss: 11.657894, running mem acc: 0.200
==>>> it: 101, avg. loss: 5.353099, running train acc: 0.045
==>>> it: 101, mem avg. loss: 3.388463, running mem acc: 0.341
==>>> it: 201, avg. loss: 4.683339, running train acc: 0.047
==>>> it: 201, mem avg. loss: 2.980038, running mem acc: 0.361
==>>> it: 301, avg. loss: 4.369340, running train acc: 0.066
==>>> it: 301, mem avg. loss: 2.878512, running mem acc: 0.343
==>>> it: 401, avg. loss: 4.180258, running train acc: 0.076
==>>> it: 401, mem avg. loss: 2.834336, running mem acc: 0.338
==>>> it: 501, avg. loss: 4.076425, running train acc: 0.078
==>>> it: 501, mem avg. loss: 2.795698, running mem acc: 0.336
==>>> it: 601, avg. loss: 3.974885, running train acc: 0.085
==>>> it: 601, mem avg. loss: 2.798518, running mem acc: 0.321
==>>> it: 701, avg. loss: 3.886858, running train acc: 0.094
==>>> it: 701, mem avg. loss: 2.806211, running mem acc: 0.311
==>>> it: 801, avg. loss: 3.803764, running train acc: 0.104
==>>> it: 801, mem avg. loss: 2.817028, running mem acc: 0.304
==>>> it: 901, avg. loss: 3.735418, running train acc: 0.110
==>>> it: 901, mem avg. loss: 2.792073, running mem acc: 0.304
==>>> it: 1001, avg. loss: 3.658920, running train acc: 0.120
==>>> it: 1001, mem avg. loss: 2.763202, running mem acc: 0.307
==>>> it: 1101, avg. loss: 3.574417, running train acc: 0.133
==>>> it: 1101, mem avg. loss: 2.739110, running mem acc: 0.308
==>>> it: 1201, avg. loss: 3.502836, running train acc: 0.143
==>>> it: 1201, mem avg. loss: 2.701882, running mem acc: 0.312
==>>> it: 1301, avg. loss: 3.418233, running train acc: 0.158
==>>> it: 1301, mem avg. loss: 2.666710, running mem acc: 0.317
==>>> it: 1401, avg. loss: 3.340496, running train acc: 0.171
==>>> it: 1401, mem avg. loss: 2.626521, running mem acc: 0.323
==>>> it: 1501, avg. loss: 3.260209, running train acc: 0.186
==>>> it: 1501, mem avg. loss: 2.594515, running mem acc: 0.327
==>>> it: 1601, avg. loss: 3.184837, running train acc: 0.200
==>>> it: 1601, mem avg. loss: 2.556864, running mem acc: 0.332
==>>> it: 1701, avg. loss: 3.114386, running train acc: 0.213
==>>> it: 1701, mem avg. loss: 2.517041, running mem acc: 0.339
==>>> it: 1801, avg. loss: 3.037742, running train acc: 0.230
==>>> it: 1801, mem avg. loss: 2.475852, running mem acc: 0.348
==>>> it: 1901, avg. loss: 2.965021, running train acc: 0.244
==>>> it: 1901, mem avg. loss: 2.438131, running mem acc: 0.353
==>>> it: 2001, avg. loss: 2.896756, running train acc: 0.258
==>>> it: 2001, mem avg. loss: 2.394470, running mem acc: 0.361
==>>> it: 2101, avg. loss: 2.830431, running train acc: 0.273
==>>> it: 2101, mem avg. loss: 2.352509, running mem acc: 0.370
==>>> it: 2201, avg. loss: 2.768624, running train acc: 0.286
==>>> it: 2201, mem avg. loss: 2.312496, running mem acc: 0.378
==>>> it: 2301, avg. loss: 2.706537, running train acc: 0.300
==>>> it: 2301, mem avg. loss: 2.269582, running mem acc: 0.387
==>>> it: 2401, avg. loss: 2.649435, running train acc: 0.313
==>>> it: 2401, mem avg. loss: 2.228783, running mem acc: 0.396
==>>> it: 2501, avg. loss: 2.593932, running train acc: 0.326
==>>> it: 2501, mem avg. loss: 2.189280, running mem acc: 0.404
==>>> it: 2601, avg. loss: 2.538997, running train acc: 0.338
==>>> it: 2601, mem avg. loss: 2.148630, running mem acc: 0.413
==>>> it: 2701, avg. loss: 2.485062, running train acc: 0.350
==>>> it: 2701, mem avg. loss: 2.109418, running mem acc: 0.422
==>>> it: 2801, avg. loss: 2.433836, running train acc: 0.362
==>>> it: 2801, mem avg. loss: 2.072441, running mem acc: 0.431
==>>> it: 2901, avg. loss: 2.386507, running train acc: 0.373
==>>> it: 2901, mem avg. loss: 2.033420, running mem acc: 0.441
==>>> it: 3001, avg. loss: 2.340591, running train acc: 0.383
==>>> it: 3001, mem avg. loss: 1.996424, running mem acc: 0.449
==>>> it: 3101, avg. loss: 2.297564, running train acc: 0.393
==>>> it: 3101, mem avg. loss: 1.959757, running mem acc: 0.458
==>>> it: 3201, avg. loss: 2.252975, running train acc: 0.404
==>>> it: 3201, mem avg. loss: 1.922623, running mem acc: 0.468
==>>> it: 3301, avg. loss: 2.209595, running train acc: 0.414
==>>> it: 3301, mem avg. loss: 1.887355, running mem acc: 0.477
==>>> it: 3401, avg. loss: 2.169297, running train acc: 0.425
==>>> it: 3401, mem avg. loss: 1.852631, running mem acc: 0.485
==>>> it: 3501, avg. loss: 2.128399, running train acc: 0.434
==>>> it: 3501, mem avg. loss: 1.818986, running mem acc: 0.494
==>>> it: 3601, avg. loss: 2.089656, running train acc: 0.444
==>>> it: 3601, mem avg. loss: 1.784768, running mem acc: 0.503
==>>> it: 3701, avg. loss: 2.052652, running train acc: 0.453
==>>> it: 3701, mem avg. loss: 1.752533, running mem acc: 0.511
==>>> it: 3801, avg. loss: 2.016657, running train acc: 0.463
==>>> it: 3801, mem avg. loss: 1.720850, running mem acc: 0.520
==>>> it: 3901, avg. loss: 1.979946, running train acc: 0.472
==>>> it: 3901, mem avg. loss: 1.690746, running mem acc: 0.527
==>>> it: 4001, avg. loss: 1.946626, running train acc: 0.481
==>>> it: 4001, mem avg. loss: 1.662297, running mem acc: 0.535
==>>> it: 4101, avg. loss: 1.914398, running train acc: 0.489
==>>> it: 4101, mem avg. loss: 1.631950, running mem acc: 0.543
==>>> it: 4201, avg. loss: 1.882320, running train acc: 0.497
==>>> it: 4201, mem avg. loss: 1.603541, running mem acc: 0.550
==>>> it: 4301, avg. loss: 1.851402, running train acc: 0.504
==>>> it: 4301, mem avg. loss: 1.576489, running mem acc: 0.558
==>>> it: 4401, avg. loss: 1.822731, running train acc: 0.512
==>>> it: 4401, mem avg. loss: 1.548871, running mem acc: 0.565
==>>> it: 4501, avg. loss: 1.796583, running train acc: 0.518
==>>> it: 4501, mem avg. loss: 1.524075, running mem acc: 0.571
==>>> it: 4601, avg. loss: 1.770162, running train acc: 0.525
==>>> it: 4601, mem avg. loss: 1.498545, running mem acc: 0.578
==>>> it: 4701, avg. loss: 1.743887, running train acc: 0.531
==>>> it: 4701, mem avg. loss: 1.473710, running mem acc: 0.585
==>>> it: 4801, avg. loss: 1.718744, running train acc: 0.538
==>>> it: 4801, mem avg. loss: 1.448844, running mem acc: 0.592
==>>> it: 4901, avg. loss: 1.691925, running train acc: 0.545
==>>> it: 4901, mem avg. loss: 1.424598, running mem acc: 0.599
==>>> it: 5001, avg. loss: 1.669769, running train acc: 0.550
==>>> it: 5001, mem avg. loss: 1.401913, running mem acc: 0.605
==>>> it: 5101, avg. loss: 1.646232, running train acc: 0.556
==>>> it: 5101, mem avg. loss: 1.379387, running mem acc: 0.612
==>>> it: 5201, avg. loss: 1.623646, running train acc: 0.562
==>>> it: 5201, mem avg. loss: 1.357608, running mem acc: 0.618
==>>> it: 5301, avg. loss: 1.601648, running train acc: 0.568
==>>> it: 5301, mem avg. loss: 1.336361, running mem acc: 0.624
==>>> it: 5401, avg. loss: 1.580380, running train acc: 0.573
==>>> it: 5401, mem avg. loss: 1.316128, running mem acc: 0.629
==>>> it: 5501, avg. loss: 1.559685, running train acc: 0.579
==>>> it: 5501, mem avg. loss: 1.296730, running mem acc: 0.635
==>>> it: 5601, avg. loss: 1.538579, running train acc: 0.584
==>>> it: 5601, mem avg. loss: 1.277086, running mem acc: 0.640
==>>> it: 5701, avg. loss: 1.518286, running train acc: 0.590
==>>> it: 5701, mem avg. loss: 1.258057, running mem acc: 0.646
==>>> it: 5801, avg. loss: 1.497371, running train acc: 0.595
==>>> it: 5801, mem avg. loss: 1.239873, running mem acc: 0.651
==>>> it: 5901, avg. loss: 1.477944, running train acc: 0.600
==>>> it: 5901, mem avg. loss: 1.221664, running mem acc: 0.656
==>>> it: 6001, avg. loss: 1.458711, running train acc: 0.605
==>>> it: 6001, mem avg. loss: 1.204363, running mem acc: 0.661
==>>> it: 6101, avg. loss: 1.440450, running train acc: 0.609
==>>> it: 6101, mem avg. loss: 1.187202, running mem acc: 0.666
==>>> it: 6201, avg. loss: 1.421927, running train acc: 0.614
==>>> it: 6201, mem avg. loss: 1.170488, running mem acc: 0.671
==>>> it: 6301, avg. loss: 1.405431, running train acc: 0.618
==>>> it: 6301, mem avg. loss: 1.155138, running mem acc: 0.675
==>>> it: 6401, avg. loss: 1.388816, running train acc: 0.623
==>>> it: 6401, mem avg. loss: 1.139328, running mem acc: 0.679
==>>> it: 6501, avg. loss: 1.373424, running train acc: 0.626
==>>> it: 6501, mem avg. loss: 1.124274, running mem acc: 0.683
==>>> it: 6601, avg. loss: 1.357726, running train acc: 0.630
==>>> it: 6601, mem avg. loss: 1.109157, running mem acc: 0.688
==>>> it: 6701, avg. loss: 1.341769, running train acc: 0.634
==>>> it: 6701, mem avg. loss: 1.094433, running mem acc: 0.692
==>>> it: 6801, avg. loss: 1.325939, running train acc: 0.639
==>>> it: 6801, mem avg. loss: 1.080415, running mem acc: 0.696
==>>> it: 6901, avg. loss: 1.311357, running train acc: 0.642
==>>> it: 6901, mem avg. loss: 1.066560, running mem acc: 0.700
==>>> it: 7001, avg. loss: 1.296664, running train acc: 0.646
==>>> it: 7001, mem avg. loss: 1.053040, running mem acc: 0.704
==>>> it: 7101, avg. loss: 1.282808, running train acc: 0.650
==>>> it: 7101, mem avg. loss: 1.039909, running mem acc: 0.707
==>>> it: 7201, avg. loss: 1.269121, running train acc: 0.654
==>>> it: 7201, mem avg. loss: 1.026948, running mem acc: 0.711
==>>> it: 7301, avg. loss: 1.256003, running train acc: 0.657
==>>> it: 7301, mem avg. loss: 1.014610, running mem acc: 0.714
==>>> it: 7401, avg. loss: 1.243396, running train acc: 0.660
==>>> it: 7401, mem avg. loss: 1.002588, running mem acc: 0.718
==>>> it: 7501, avg. loss: 1.231176, running train acc: 0.663
==>>> it: 7501, mem avg. loss: 0.990910, running mem acc: 0.721
==>>> it: 7601, avg. loss: 1.218342, running train acc: 0.667
==>>> it: 7601, mem avg. loss: 0.979467, running mem acc: 0.724
==>>> it: 7701, avg. loss: 1.205734, running train acc: 0.670
==>>> it: 7701, mem avg. loss: 0.968121, running mem acc: 0.728
==>>> it: 7801, avg. loss: 1.193667, running train acc: 0.673
==>>> it: 7801, mem avg. loss: 0.957041, running mem acc: 0.731
==>>> it: 7901, avg. loss: 1.181454, running train acc: 0.677
==>>> it: 7901, mem avg. loss: 0.945773, running mem acc: 0.734
==>>> it: 8001, avg. loss: 1.170205, running train acc: 0.680
==>>> it: 8001, mem avg. loss: 0.935307, running mem acc: 0.737
==>>> it: 8101, avg. loss: 1.159189, running train acc: 0.683
==>>> it: 8101, mem avg. loss: 0.925018, running mem acc: 0.740
==>>> it: 8201, avg. loss: 1.148124, running train acc: 0.686
==>>> it: 8201, mem avg. loss: 0.914707, running mem acc: 0.743
==>>> it: 8301, avg. loss: 1.137134, running train acc: 0.688
==>>> it: 8301, mem avg. loss: 0.904786, running mem acc: 0.746
==>>> it: 8401, avg. loss: 1.125950, running train acc: 0.691
==>>> it: 8401, mem avg. loss: 0.895251, running mem acc: 0.748
==>>> it: 8501, avg. loss: 1.115358, running train acc: 0.694
==>>> it: 8501, mem avg. loss: 0.885732, running mem acc: 0.751
==>>> it: 8601, avg. loss: 1.105151, running train acc: 0.697
==>>> it: 8601, mem avg. loss: 0.876360, running mem acc: 0.754
==>>> it: 8701, avg. loss: 1.094627, running train acc: 0.700
==>>> it: 8701, mem avg. loss: 0.867224, running mem acc: 0.756
==>>> it: 8801, avg. loss: 1.084306, running train acc: 0.702
==>>> it: 8801, mem avg. loss: 0.858206, running mem acc: 0.759
==>>> it: 8901, avg. loss: 1.074073, running train acc: 0.705
==>>> it: 8901, mem avg. loss: 0.849229, running mem acc: 0.761
==>>> it: 9001, avg. loss: 1.064488, running train acc: 0.708
==>>> it: 9001, mem avg. loss: 0.840591, running mem acc: 0.764
==>>> it: 9101, avg. loss: 1.055134, running train acc: 0.710
==>>> it: 9101, mem avg. loss: 0.832120, running mem acc: 0.766
==>>> it: 9201, avg. loss: 1.045616, running train acc: 0.713
==>>> it: 9201, mem avg. loss: 0.823831, running mem acc: 0.769
==>>> it: 9301, avg. loss: 1.036315, running train acc: 0.715
==>>> it: 9301, mem avg. loss: 0.815695, running mem acc: 0.771
==>>> it: 9401, avg. loss: 1.027562, running train acc: 0.717
==>>> it: 9401, mem avg. loss: 0.807891, running mem acc: 0.773
==>>> it: 9501, avg. loss: 1.018869, running train acc: 0.720
==>>> it: 9501, mem avg. loss: 0.799961, running mem acc: 0.775
==>>> it: 9601, avg. loss: 1.010223, running train acc: 0.722
==>>> it: 9601, mem avg. loss: 0.792155, running mem acc: 0.778
==>>> it: 9701, avg. loss: 1.001949, running train acc: 0.724
==>>> it: 9701, mem avg. loss: 0.784628, running mem acc: 0.780
==>>> it: 9801, avg. loss: 0.993374, running train acc: 0.726
==>>> it: 9801, mem avg. loss: 0.777203, running mem acc: 0.782
==>>> it: 9901, avg. loss: 0.984867, running train acc: 0.729
==>>> it: 9901, mem avg. loss: 0.769865, running mem acc: 0.784
==>>> it: 10001, avg. loss: 0.976686, running train acc: 0.731
==>>> it: 10001, mem avg. loss: 0.762855, running mem acc: 0.786
==>>> it: 10101, avg. loss: 0.968741, running train acc: 0.733
==>>> it: 10101, mem avg. loss: 0.755967, running mem acc: 0.788
==>>> it: 10201, avg. loss: 0.960744, running train acc: 0.735
==>>> it: 10201, mem avg. loss: 0.749095, running mem acc: 0.790
==>>> it: 10301, avg. loss: 0.953240, running train acc: 0.737
==>>> it: 10301, mem avg. loss: 0.742494, running mem acc: 0.792
==>>> it: 10401, avg. loss: 0.945396, running train acc: 0.739
==>>> it: 10401, mem avg. loss: 0.735788, running mem acc: 0.793
==>>> it: 10501, avg. loss: 0.938197, running train acc: 0.741
==>>> it: 10501, mem avg. loss: 0.729206, running mem acc: 0.795
==>>> it: 10601, avg. loss: 0.930892, running train acc: 0.743
==>>> it: 10601, mem avg. loss: 0.723019, running mem acc: 0.797
==>>> it: 10701, avg. loss: 0.923607, running train acc: 0.745
==>>> it: 10701, mem avg. loss: 0.716971, running mem acc: 0.799
[0.34783866]
no ratio: 0.0
on ratio: 0.0
[(0, 29329, 0, 0)]
[-0.04697879016091897]
[0]
[nan]
[-6.828660116298124e-05]
[nan]
[-0.0030452965293079615]
Loading test set...
buffer has 5000 slots
Loading data...
loading time 5.220020532608032
Loading data...
loading time 5.623095512390137
Loading data...
loading time 5.306897163391113
Loading data...
loading time 4.851908206939697
Loading data...
loading time 5.4410881996154785
Loading data...
loading time 5.316094398498535
Loading data...
loading time 5.359348773956299
Loading data...
loading time 5.076460123062134
Training Start
----------run 1 training-------------
size: (107909, 128, 128, 3), (107909,)
==>>> it: 1, avg. loss: 15.432864, running train acc: 0.000
==>>> it: 1, mem avg. loss: 8.515553, running mem acc: 0.200
==>>> it: 101, avg. loss: 5.670830, running train acc: 0.044
==>>> it: 101, mem avg. loss: 3.205662, running mem acc: 0.356
==>>> it: 201, avg. loss: 4.785716, running train acc: 0.059
==>>> it: 201, mem avg. loss: 2.919085, running mem acc: 0.347
==>>> it: 301, avg. loss: 4.442473, running train acc: 0.069
==>>> it: 301, mem avg. loss: 2.804546, running mem acc: 0.353
==>>> it: 401, avg. loss: 4.222711, running train acc: 0.080
==>>> it: 401, mem avg. loss: 2.804241, running mem acc: 0.339
==>>> it: 501, avg. loss: 4.092912, running train acc: 0.085
==>>> it: 501, mem avg. loss: 2.786874, running mem acc: 0.336
==>>> it: 601, avg. loss: 3.987534, running train acc: 0.089
==>>> it: 601, mem avg. loss: 2.772491, running mem acc: 0.332
==>>> it: 701, avg. loss: 3.870333, running train acc: 0.103
==>>> it: 701, mem avg. loss: 2.763801, running mem acc: 0.327
==>>> it: 801, avg. loss: 3.772337, running train acc: 0.114
==>>> it: 801, mem avg. loss: 2.728471, running mem acc: 0.328
==>>> it: 901, avg. loss: 3.666823, running train acc: 0.126
==>>> it: 901, mem avg. loss: 2.715855, running mem acc: 0.322
==>>> it: 1001, avg. loss: 3.575810, running train acc: 0.139
==>>> it: 1001, mem avg. loss: 2.692019, running mem acc: 0.323
==>>> it: 1101, avg. loss: 3.493827, running train acc: 0.152
==>>> it: 1101, mem avg. loss: 2.664316, running mem acc: 0.325
==>>> it: 1201, avg. loss: 3.402482, running train acc: 0.167
==>>> it: 1201, mem avg. loss: 2.623259, running mem acc: 0.330
==>>> it: 1301, avg. loss: 3.312056, running train acc: 0.183
==>>> it: 1301, mem avg. loss: 2.585980, running mem acc: 0.333
==>>> it: 1401, avg. loss: 3.227333, running train acc: 0.198
==>>> it: 1401, mem avg. loss: 2.550239, running mem acc: 0.337
==>>> it: 1501, avg. loss: 3.150430, running train acc: 0.213
==>>> it: 1501, mem avg. loss: 2.510696, running mem acc: 0.342
==>>> it: 1601, avg. loss: 3.074442, running train acc: 0.228
==>>> it: 1601, mem avg. loss: 2.477371, running mem acc: 0.348
==>>> it: 1701, avg. loss: 3.003093, running train acc: 0.241
==>>> it: 1701, mem avg. loss: 2.444090, running mem acc: 0.354
==>>> it: 1801, avg. loss: 2.936110, running train acc: 0.254
==>>> it: 1801, mem avg. loss: 2.404208, running mem acc: 0.362
==>>> it: 1901, avg. loss: 2.868267, running train acc: 0.269
==>>> it: 1901, mem avg. loss: 2.365763, running mem acc: 0.369
==>>> it: 2001, avg. loss: 2.808247, running train acc: 0.281
==>>> it: 2001, mem avg. loss: 2.328786, running mem acc: 0.376
==>>> it: 2101, avg. loss: 2.748788, running train acc: 0.295
==>>> it: 2101, mem avg. loss: 2.290506, running mem acc: 0.384
==>>> it: 2201, avg. loss: 2.691532, running train acc: 0.307
==>>> it: 2201, mem avg. loss: 2.254890, running mem acc: 0.390
==>>> it: 2301, avg. loss: 2.639480, running train acc: 0.318
==>>> it: 2301, mem avg. loss: 2.217700, running mem acc: 0.397
==>>> it: 2401, avg. loss: 2.585296, running train acc: 0.330
==>>> it: 2401, mem avg. loss: 2.181457, running mem acc: 0.404
==>>> it: 2501, avg. loss: 2.535399, running train acc: 0.341
==>>> it: 2501, mem avg. loss: 2.145085, running mem acc: 0.412
==>>> it: 2601, avg. loss: 2.488479, running train acc: 0.351
==>>> it: 2601, mem avg. loss: 2.110687, running mem acc: 0.419
==>>> it: 2701, avg. loss: 2.441533, running train acc: 0.362
==>>> it: 2701, mem avg. loss: 2.075230, running mem acc: 0.427
==>>> it: 2801, avg. loss: 2.398305, running train acc: 0.372
==>>> it: 2801, mem avg. loss: 2.041662, running mem acc: 0.435
==>>> it: 2901, avg. loss: 2.352735, running train acc: 0.383
==>>> it: 2901, mem avg. loss: 2.008101, running mem acc: 0.443
==>>> it: 3001, avg. loss: 2.311079, running train acc: 0.393
==>>> it: 3001, mem avg. loss: 1.975394, running mem acc: 0.451
==>>> it: 3101, avg. loss: 2.267459, running train acc: 0.403
==>>> it: 3101, mem avg. loss: 1.941233, running mem acc: 0.459
==>>> it: 3201, avg. loss: 2.226106, running train acc: 0.413
==>>> it: 3201, mem avg. loss: 1.910424, running mem acc: 0.467
==>>> it: 3301, avg. loss: 2.189161, running train acc: 0.422
==>>> it: 3301, mem avg. loss: 1.881039, running mem acc: 0.474
==>>> it: 3401, avg. loss: 2.149848, running train acc: 0.431
==>>> it: 3401, mem avg. loss: 1.850088, running mem acc: 0.482
==>>> it: 3501, avg. loss: 2.116560, running train acc: 0.440
==>>> it: 3501, mem avg. loss: 1.819357, running mem acc: 0.490
==>>> it: 3601, avg. loss: 2.080905, running train acc: 0.448
==>>> it: 3601, mem avg. loss: 1.788582, running mem acc: 0.498
==>>> it: 3701, avg. loss: 2.044508, running train acc: 0.457
==>>> it: 3701, mem avg. loss: 1.759777, running mem acc: 0.505
==>>> it: 3801, avg. loss: 2.014732, running train acc: 0.465
==>>> it: 3801, mem avg. loss: 1.729773, running mem acc: 0.513
==>>> it: 3901, avg. loss: 1.981193, running train acc: 0.473
==>>> it: 3901, mem avg. loss: 1.700715, running mem acc: 0.521
==>>> it: 4001, avg. loss: 1.949647, running train acc: 0.480
==>>> it: 4001, mem avg. loss: 1.672329, running mem acc: 0.529
==>>> it: 4101, avg. loss: 1.919666, running train acc: 0.487
==>>> it: 4101, mem avg. loss: 1.646348, running mem acc: 0.535
==>>> it: 4201, avg. loss: 1.892611, running train acc: 0.494
==>>> it: 4201, mem avg. loss: 1.620715, running mem acc: 0.542
==>>> it: 4301, avg. loss: 1.863756, running train acc: 0.501
==>>> it: 4301, mem avg. loss: 1.593993, running mem acc: 0.550
==>>> it: 4401, avg. loss: 1.836945, running train acc: 0.508
==>>> it: 4401, mem avg. loss: 1.569130, running mem acc: 0.556
==>>> it: 4501, avg. loss: 1.810666, running train acc: 0.514
==>>> it: 4501, mem avg. loss: 1.542731, running mem acc: 0.564
==>>> it: 4601, avg. loss: 1.785304, running train acc: 0.521
==>>> it: 4601, mem avg. loss: 1.518201, running mem acc: 0.571
==>>> it: 4701, avg. loss: 1.759798, running train acc: 0.527
==>>> it: 4701, mem avg. loss: 1.494040, running mem acc: 0.577
==>>> it: 4801, avg. loss: 1.733925, running train acc: 0.534
==>>> it: 4801, mem avg. loss: 1.471656, running mem acc: 0.583
==>>> it: 4901, avg. loss: 1.710384, running train acc: 0.539
==>>> it: 4901, mem avg. loss: 1.448516, running mem acc: 0.589
==>>> it: 5001, avg. loss: 1.687307, running train acc: 0.545
==>>> it: 5001, mem avg. loss: 1.426331, running mem acc: 0.596
==>>> it: 5101, avg. loss: 1.664684, running train acc: 0.551
==>>> it: 5101, mem avg. loss: 1.404880, running mem acc: 0.601
==>>> it: 5201, avg. loss: 1.642813, running train acc: 0.556
==>>> it: 5201, mem avg. loss: 1.383878, running mem acc: 0.607
==>>> it: 5301, avg. loss: 1.621719, running train acc: 0.562
==>>> it: 5301, mem avg. loss: 1.363248, running mem acc: 0.613
==>>> it: 5401, avg. loss: 1.601337, running train acc: 0.567
==>>> it: 5401, mem avg. loss: 1.342995, running mem acc: 0.619
==>>> it: 5501, avg. loss: 1.581401, running train acc: 0.572
==>>> it: 5501, mem avg. loss: 1.323721, running mem acc: 0.624
==>>> it: 5601, avg. loss: 1.561955, running train acc: 0.577
==>>> it: 5601, mem avg. loss: 1.304415, running mem acc: 0.629
==>>> it: 5701, avg. loss: 1.542219, running train acc: 0.582
==>>> it: 5701, mem avg. loss: 1.285553, running mem acc: 0.635
==>>> it: 5801, avg. loss: 1.522687, running train acc: 0.587
==>>> it: 5801, mem avg. loss: 1.267501, running mem acc: 0.640
==>>> it: 5901, avg. loss: 1.503912, running train acc: 0.591
==>>> it: 5901, mem avg. loss: 1.250045, running mem acc: 0.645
==>>> it: 6001, avg. loss: 1.486806, running train acc: 0.596
==>>> it: 6001, mem avg. loss: 1.232882, running mem acc: 0.650
==>>> it: 6101, avg. loss: 1.468998, running train acc: 0.600
==>>> it: 6101, mem avg. loss: 1.216112, running mem acc: 0.654
==>>> it: 6201, avg. loss: 1.452824, running train acc: 0.605
==>>> it: 6201, mem avg. loss: 1.200683, running mem acc: 0.658
==>>> it: 6301, avg. loss: 1.435737, running train acc: 0.609
==>>> it: 6301, mem avg. loss: 1.184558, running mem acc: 0.663
==>>> it: 6401, avg. loss: 1.418671, running train acc: 0.614
==>>> it: 6401, mem avg. loss: 1.168443, running mem acc: 0.668
==>>> it: 6501, avg. loss: 1.402725, running train acc: 0.618
==>>> it: 6501, mem avg. loss: 1.153358, running mem acc: 0.672
==>>> it: 6601, avg. loss: 1.387910, running train acc: 0.622
==>>> it: 6601, mem avg. loss: 1.138683, running mem acc: 0.676
==>>> it: 6701, avg. loss: 1.372447, running train acc: 0.626
==>>> it: 6701, mem avg. loss: 1.124071, running mem acc: 0.680
==>>> it: 6801, avg. loss: 1.357743, running train acc: 0.630
==>>> it: 6801, mem avg. loss: 1.110140, running mem acc: 0.684
==>>> it: 6901, avg. loss: 1.343385, running train acc: 0.633
==>>> it: 6901, mem avg. loss: 1.096277, running mem acc: 0.688
==>>> it: 7001, avg. loss: 1.328113, running train acc: 0.638
==>>> it: 7001, mem avg. loss: 1.082607, running mem acc: 0.692
==>>> it: 7101, avg. loss: 1.314509, running train acc: 0.641
==>>> it: 7101, mem avg. loss: 1.069295, running mem acc: 0.696
==>>> it: 7201, avg. loss: 1.301003, running train acc: 0.645
==>>> it: 7201, mem avg. loss: 1.055991, running mem acc: 0.700
==>>> it: 7301, avg. loss: 1.288043, running train acc: 0.648
==>>> it: 7301, mem avg. loss: 1.043645, running mem acc: 0.703
==>>> it: 7401, avg. loss: 1.275124, running train acc: 0.651
==>>> it: 7401, mem avg. loss: 1.031280, running mem acc: 0.707
==>>> it: 7501, avg. loss: 1.261640, running train acc: 0.655
==>>> it: 7501, mem avg. loss: 1.019047, running mem acc: 0.710
==>>> it: 7601, avg. loss: 1.248462, running train acc: 0.658
==>>> it: 7601, mem avg. loss: 1.006881, running mem acc: 0.714
==>>> it: 7701, avg. loss: 1.236187, running train acc: 0.662
==>>> it: 7701, mem avg. loss: 0.995263, running mem acc: 0.717
==>>> it: 7801, avg. loss: 1.224534, running train acc: 0.665
==>>> it: 7801, mem avg. loss: 0.983907, running mem acc: 0.720
==>>> it: 7901, avg. loss: 1.212398, running train acc: 0.668
==>>> it: 7901, mem avg. loss: 0.973159, running mem acc: 0.723
==>>> it: 8001, avg. loss: 1.200313, running train acc: 0.671
==>>> it: 8001, mem avg. loss: 0.962407, running mem acc: 0.726
==>>> it: 8101, avg. loss: 1.188502, running train acc: 0.674
==>>> it: 8101, mem avg. loss: 0.951763, running mem acc: 0.729
==>>> it: 8201, avg. loss: 1.177347, running train acc: 0.677
==>>> it: 8201, mem avg. loss: 0.941277, running mem acc: 0.732
==>>> it: 8301, avg. loss: 1.165995, running train acc: 0.680
==>>> it: 8301, mem avg. loss: 0.931035, running mem acc: 0.735
==>>> it: 8401, avg. loss: 1.155299, running train acc: 0.683
==>>> it: 8401, mem avg. loss: 0.921044, running mem acc: 0.738
==>>> it: 8501, avg. loss: 1.144685, running train acc: 0.686
==>>> it: 8501, mem avg. loss: 0.911343, running mem acc: 0.741
==>>> it: 8601, avg. loss: 1.134719, running train acc: 0.688
==>>> it: 8601, mem avg. loss: 0.901813, running mem acc: 0.744
==>>> it: 8701, avg. loss: 1.124193, running train acc: 0.691
==>>> it: 8701, mem avg. loss: 0.892475, running mem acc: 0.746
==>>> it: 8801, avg. loss: 1.114430, running train acc: 0.694
==>>> it: 8801, mem avg. loss: 0.883326, running mem acc: 0.749
==>>> it: 8901, avg. loss: 1.105114, running train acc: 0.696
==>>> it: 8901, mem avg. loss: 0.874578, running mem acc: 0.752
==>>> it: 9001, avg. loss: 1.095266, running train acc: 0.699
==>>> it: 9001, mem avg. loss: 0.865669, running mem acc: 0.754
==>>> it: 9101, avg. loss: 1.086126, running train acc: 0.701
==>>> it: 9101, mem avg. loss: 0.857138, running mem acc: 0.757
==>>> it: 9201, avg. loss: 1.076697, running train acc: 0.704
==>>> it: 9201, mem avg. loss: 0.848849, running mem acc: 0.759
==>>> it: 9301, avg. loss: 1.067470, running train acc: 0.706
==>>> it: 9301, mem avg. loss: 0.840531, running mem acc: 0.761
==>>> it: 9401, avg. loss: 1.058288, running train acc: 0.709
==>>> it: 9401, mem avg. loss: 0.832462, running mem acc: 0.764
==>>> it: 9501, avg. loss: 1.049909, running train acc: 0.711
==>>> it: 9501, mem avg. loss: 0.824703, running mem acc: 0.766
==>>> it: 9601, avg. loss: 1.041303, running train acc: 0.713
==>>> it: 9601, mem avg. loss: 0.817084, running mem acc: 0.768
==>>> it: 9701, avg. loss: 1.032407, running train acc: 0.716
==>>> it: 9701, mem avg. loss: 0.809352, running mem acc: 0.770
==>>> it: 9801, avg. loss: 1.023919, running train acc: 0.718
==>>> it: 9801, mem avg. loss: 0.801949, running mem acc: 0.772
==>>> it: 9901, avg. loss: 1.015948, running train acc: 0.720
==>>> it: 9901, mem avg. loss: 0.794675, running mem acc: 0.774
==>>> it: 10001, avg. loss: 1.007581, running train acc: 0.722
==>>> it: 10001, mem avg. loss: 0.787325, running mem acc: 0.776
==>>> it: 10101, avg. loss: 0.999711, running train acc: 0.724
==>>> it: 10101, mem avg. loss: 0.780330, running mem acc: 0.778
==>>> it: 10201, avg. loss: 0.991484, running train acc: 0.727
==>>> it: 10201, mem avg. loss: 0.773178, running mem acc: 0.781
==>>> it: 10301, avg. loss: 0.983403, running train acc: 0.729
==>>> it: 10301, mem avg. loss: 0.766262, running mem acc: 0.782
==>>> it: 10401, avg. loss: 0.975707, running train acc: 0.731
==>>> it: 10401, mem avg. loss: 0.759644, running mem acc: 0.784
==>>> it: 10501, avg. loss: 0.967814, running train acc: 0.733
==>>> it: 10501, mem avg. loss: 0.753063, running mem acc: 0.786
==>>> it: 10601, avg. loss: 0.960416, running train acc: 0.735
==>>> it: 10601, mem avg. loss: 0.746479, running mem acc: 0.788
==>>> it: 10701, avg. loss: 0.952914, running train acc: 0.737
==>>> it: 10701, mem avg. loss: 0.740192, running mem acc: 0.790
[0.33127279]
no ratio: 0.0
on ratio: 0.0
[(0, 30074, 0, 0)]
[-0.023013621876166193]
[0]
[nan]
[-2.070162008749321e-05]
[nan]
[-9.35554489842616e-05]
Loading test set...
buffer has 5000 slots
Loading data...
loading time 4.85053277015686
Loading data...
loading time 5.014880418777466
Loading data...
loading time 4.938662767410278
Loading data...
loading time 5.278808116912842
Loading data...
loading time 5.294091701507568
Loading data...
loading time 5.110446929931641
Loading data...
loading time 5.3830506801605225
Loading data...
loading time 5.205192565917969
Training Start
----------run 2 training-------------
size: (107909, 128, 128, 3), (107909,)
==>>> it: 1, avg. loss: 13.469354, running train acc: 0.050
==>>> it: 1, mem avg. loss: 6.754054, running mem acc: 0.200
==>>> it: 101, avg. loss: 5.445614, running train acc: 0.033
==>>> it: 101, mem avg. loss: 3.549844, running mem acc: 0.290
==>>> it: 201, avg. loss: 4.697916, running train acc: 0.054
==>>> it: 201, mem avg. loss: 3.032665, running mem acc: 0.345
==>>> it: 301, avg. loss: 4.340844, running train acc: 0.069
==>>> it: 301, mem avg. loss: 2.895957, running mem acc: 0.344
==>>> it: 401, avg. loss: 4.120885, running train acc: 0.085
==>>> it: 401, mem avg. loss: 2.859179, running mem acc: 0.335
==>>> it: 501, avg. loss: 3.964526, running train acc: 0.096
==>>> it: 501, mem avg. loss: 2.766543, running mem acc: 0.342
==>>> it: 601, avg. loss: 3.816485, running train acc: 0.111
==>>> it: 601, mem avg. loss: 2.727827, running mem acc: 0.337
==>>> it: 701, avg. loss: 3.714873, running train acc: 0.122
==>>> it: 701, mem avg. loss: 2.698097, running mem acc: 0.337
==>>> it: 801, avg. loss: 3.606523, running train acc: 0.140
==>>> it: 801, mem avg. loss: 2.659044, running mem acc: 0.338
==>>> it: 901, avg. loss: 3.510146, running train acc: 0.154
==>>> it: 901, mem avg. loss: 2.624363, running mem acc: 0.339
==>>> it: 1001, avg. loss: 3.416678, running train acc: 0.169
==>>> it: 1001, mem avg. loss: 2.596171, running mem acc: 0.341
==>>> it: 1101, avg. loss: 3.321712, running train acc: 0.184
==>>> it: 1101, mem avg. loss: 2.561906, running mem acc: 0.346
==>>> it: 1201, avg. loss: 3.234321, running train acc: 0.199
==>>> it: 1201, mem avg. loss: 2.521412, running mem acc: 0.349
==>>> it: 1301, avg. loss: 3.153069, running train acc: 0.213
==>>> it: 1301, mem avg. loss: 2.482818, running mem acc: 0.354
==>>> it: 1401, avg. loss: 3.078051, running train acc: 0.226
==>>> it: 1401, mem avg. loss: 2.446601, running mem acc: 0.359
==>>> it: 1501, avg. loss: 3.001347, running train acc: 0.241
==>>> it: 1501, mem avg. loss: 2.412877, running mem acc: 0.364
==>>> it: 1601, avg. loss: 2.930878, running train acc: 0.254
==>>> it: 1601, mem avg. loss: 2.373885, running mem acc: 0.371
==>>> it: 1701, avg. loss: 2.857725, running train acc: 0.270
==>>> it: 1701, mem avg. loss: 2.344660, running mem acc: 0.376
==>>> it: 1801, avg. loss: 2.794409, running train acc: 0.282
==>>> it: 1801, mem avg. loss: 2.308134, running mem acc: 0.381
==>>> it: 1901, avg. loss: 2.731201, running train acc: 0.296
==>>> it: 1901, mem avg. loss: 2.268567, running mem acc: 0.390
==>>> it: 2001, avg. loss: 2.669366, running train acc: 0.310
==>>> it: 2001, mem avg. loss: 2.233132, running mem acc: 0.396
==>>> it: 2101, avg. loss: 2.608686, running train acc: 0.323
==>>> it: 2101, mem avg. loss: 2.198194, running mem acc: 0.403
==>>> it: 2201, avg. loss: 2.553057, running train acc: 0.335
==>>> it: 2201, mem avg. loss: 2.159554, running mem acc: 0.411
==>>> it: 2301, avg. loss: 2.498909, running train acc: 0.347
==>>> it: 2301, mem avg. loss: 2.122979, running mem acc: 0.419
==>>> it: 2401, avg. loss: 2.447393, running train acc: 0.358
==>>> it: 2401, mem avg. loss: 2.090193, running mem acc: 0.426
==>>> it: 2501, avg. loss: 2.400703, running train acc: 0.369
==>>> it: 2501, mem avg. loss: 2.055952, running mem acc: 0.434
==>>> it: 2601, avg. loss: 2.353992, running train acc: 0.379
==>>> it: 2601, mem avg. loss: 2.023063, running mem acc: 0.441
==>>> it: 2701, avg. loss: 2.308195, running train acc: 0.390
==>>> it: 2701, mem avg. loss: 1.988019, running mem acc: 0.449
==>>> it: 2801, avg. loss: 2.263494, running train acc: 0.400
==>>> it: 2801, mem avg. loss: 1.956142, running mem acc: 0.456
==>>> it: 2901, avg. loss: 2.218820, running train acc: 0.411
==>>> it: 2901, mem avg. loss: 1.922882, running mem acc: 0.464
==>>> it: 3001, avg. loss: 2.176830, running train acc: 0.421
==>>> it: 3001, mem avg. loss: 1.892306, running mem acc: 0.471
==>>> it: 3101, avg. loss: 2.135091, running train acc: 0.432
==>>> it: 3101, mem avg. loss: 1.860227, running mem acc: 0.478
==>>> it: 3201, avg. loss: 2.095396, running train acc: 0.441
==>>> it: 3201, mem avg. loss: 1.827383, running mem acc: 0.487
==>>> it: 3301, avg. loss: 2.060296, running train acc: 0.450
==>>> it: 3301, mem avg. loss: 1.797415, running mem acc: 0.495
==>>> it: 3401, avg. loss: 2.023377, running train acc: 0.459
==>>> it: 3401, mem avg. loss: 1.768405, running mem acc: 0.502
==>>> it: 3501, avg. loss: 1.987811, running train acc: 0.468
==>>> it: 3501, mem avg. loss: 1.736512, running mem acc: 0.510
==>>> it: 3601, avg. loss: 1.954185, running train acc: 0.476
==>>> it: 3601, mem avg. loss: 1.707399, running mem acc: 0.518
==>>> it: 3701, avg. loss: 1.922243, running train acc: 0.484
==>>> it: 3701, mem avg. loss: 1.680196, running mem acc: 0.525
==>>> it: 3801, avg. loss: 1.889897, running train acc: 0.492
==>>> it: 3801, mem avg. loss: 1.651774, running mem acc: 0.533
==>>> it: 3901, avg. loss: 1.860349, running train acc: 0.499
==>>> it: 3901, mem avg. loss: 1.623663, running mem acc: 0.540
==>>> it: 4001, avg. loss: 1.829541, running train acc: 0.507
==>>> it: 4001, mem avg. loss: 1.596743, running mem acc: 0.548
==>>> it: 4101, avg. loss: 1.801053, running train acc: 0.514
==>>> it: 4101, mem avg. loss: 1.569245, running mem acc: 0.555
==>>> it: 4201, avg. loss: 1.772754, running train acc: 0.521
==>>> it: 4201, mem avg. loss: 1.544200, running mem acc: 0.562
==>>> it: 4301, avg. loss: 1.746111, running train acc: 0.528
==>>> it: 4301, mem avg. loss: 1.519204, running mem acc: 0.569
==>>> it: 4401, avg. loss: 1.720637, running train acc: 0.534
==>>> it: 4401, mem avg. loss: 1.494795, running mem acc: 0.576
==>>> it: 4501, avg. loss: 1.694355, running train acc: 0.541
==>>> it: 4501, mem avg. loss: 1.470952, running mem acc: 0.582
==>>> it: 4601, avg. loss: 1.668174, running train acc: 0.548
==>>> it: 4601, mem avg. loss: 1.445976, running mem acc: 0.589
==>>> it: 4701, avg. loss: 1.641956, running train acc: 0.554
==>>> it: 4701, mem avg. loss: 1.423471, running mem acc: 0.595
==>>> it: 4801, avg. loss: 1.617834, running train acc: 0.560
==>>> it: 4801, mem avg. loss: 1.400843, running mem acc: 0.601
==>>> it: 4901, avg. loss: 1.594841, running train acc: 0.566
==>>> it: 4901, mem avg. loss: 1.379859, running mem acc: 0.607
==>>> it: 5001, avg. loss: 1.573954, running train acc: 0.572
==>>> it: 5001, mem avg. loss: 1.359416, running mem acc: 0.613
==>>> it: 5101, avg. loss: 1.553604, running train acc: 0.577
==>>> it: 5101, mem avg. loss: 1.338111, running mem acc: 0.619
==>>> it: 5201, avg. loss: 1.534407, running train acc: 0.582
==>>> it: 5201, mem avg. loss: 1.318139, running mem acc: 0.624
==>>> it: 5301, avg. loss: 1.514889, running train acc: 0.587
==>>> it: 5301, mem avg. loss: 1.298020, running mem acc: 0.630
==>>> it: 5401, avg. loss: 1.494277, running train acc: 0.593
==>>> it: 5401, mem avg. loss: 1.278957, running mem acc: 0.635
==>>> it: 5501, avg. loss: 1.474771, running train acc: 0.598
==>>> it: 5501, mem avg. loss: 1.260532, running mem acc: 0.641
==>>> it: 5601, avg. loss: 1.455621, running train acc: 0.602
==>>> it: 5601, mem avg. loss: 1.242344, running mem acc: 0.646
==>>> it: 5701, avg. loss: 1.438175, running train acc: 0.607
==>>> it: 5701, mem avg. loss: 1.224879, running mem acc: 0.651
==>>> it: 5801, avg. loss: 1.421308, running train acc: 0.611
==>>> it: 5801, mem avg. loss: 1.207366, running mem acc: 0.655
==>>> it: 5901, avg. loss: 1.403484, running train acc: 0.616
==>>> it: 5901, mem avg. loss: 1.190337, running mem acc: 0.660
==>>> it: 6001, avg. loss: 1.386939, running train acc: 0.620
==>>> it: 6001, mem avg. loss: 1.174072, running mem acc: 0.665
==>>> it: 6101, avg. loss: 1.369747, running train acc: 0.625
==>>> it: 6101, mem avg. loss: 1.157695, running mem acc: 0.669
==>>> it: 6201, avg. loss: 1.354585, running train acc: 0.629
==>>> it: 6201, mem avg. loss: 1.142823, running mem acc: 0.674
==>>> it: 6301, avg. loss: 1.338637, running train acc: 0.633
==>>> it: 6301, mem avg. loss: 1.127967, running mem acc: 0.678
==>>> it: 6401, avg. loss: 1.323663, running train acc: 0.637
==>>> it: 6401, mem avg. loss: 1.113010, running mem acc: 0.682
==>>> it: 6501, avg. loss: 1.308695, running train acc: 0.641
==>>> it: 6501, mem avg. loss: 1.098624, running mem acc: 0.686
==>>> it: 6601, avg. loss: 1.293921, running train acc: 0.645
==>>> it: 6601, mem avg. loss: 1.084334, running mem acc: 0.690
==>>> it: 6701, avg. loss: 1.279361, running train acc: 0.648
==>>> it: 6701, mem avg. loss: 1.070321, running mem acc: 0.694
==>>> it: 6801, avg. loss: 1.266047, running train acc: 0.652
==>>> it: 6801, mem avg. loss: 1.056639, running mem acc: 0.698
==>>> it: 6901, avg. loss: 1.251386, running train acc: 0.656
==>>> it: 6901, mem avg. loss: 1.043346, running mem acc: 0.702
==>>> it: 7001, avg. loss: 1.237153, running train acc: 0.660
==>>> it: 7001, mem avg. loss: 1.030702, running mem acc: 0.705
==>>> it: 7101, avg. loss: 1.224477, running train acc: 0.663
==>>> it: 7101, mem avg. loss: 1.017840, running mem acc: 0.709
==>>> it: 7201, avg. loss: 1.211327, running train acc: 0.667
==>>> it: 7201, mem avg. loss: 1.005837, running mem acc: 0.713
==>>> it: 7301, avg. loss: 1.198711, running train acc: 0.670
==>>> it: 7301, mem avg. loss: 0.994135, running mem acc: 0.716
==>>> it: 7401, avg. loss: 1.186275, running train acc: 0.674
==>>> it: 7401, mem avg. loss: 0.982401, running mem acc: 0.719
==>>> it: 7501, avg. loss: 1.174435, running train acc: 0.677
==>>> it: 7501, mem avg. loss: 0.970860, running mem acc: 0.722
==>>> it: 7601, avg. loss: 1.163340, running train acc: 0.680
==>>> it: 7601, mem avg. loss: 0.959941, running mem acc: 0.726
==>>> it: 7701, avg. loss: 1.152210, running train acc: 0.682
==>>> it: 7701, mem avg. loss: 0.948908, running mem acc: 0.729
==>>> it: 7801, avg. loss: 1.141037, running train acc: 0.685
==>>> it: 7801, mem avg. loss: 0.938009, running mem acc: 0.732
==>>> it: 7901, avg. loss: 1.129876, running train acc: 0.688
==>>> it: 7901, mem avg. loss: 0.927252, running mem acc: 0.735
==>>> it: 8001, avg. loss: 1.119579, running train acc: 0.691
==>>> it: 8001, mem avg. loss: 0.917136, running mem acc: 0.738
==>>> it: 8101, avg. loss: 1.108940, running train acc: 0.694
==>>> it: 8101, mem avg. loss: 0.907031, running mem acc: 0.741
==>>> it: 8201, avg. loss: 1.098704, running train acc: 0.697
==>>> it: 8201, mem avg. loss: 0.897384, running mem acc: 0.744
==>>> it: 8301, avg. loss: 1.088577, running train acc: 0.699
==>>> it: 8301, mem avg. loss: 0.887680, running mem acc: 0.746
==>>> it: 8401, avg. loss: 1.078553, running train acc: 0.702
==>>> it: 8401, mem avg. loss: 0.878273, running mem acc: 0.749
==>>> it: 8501, avg. loss: 1.068179, running train acc: 0.704
==>>> it: 8501, mem avg. loss: 0.868996, running mem acc: 0.752
==>>> it: 8601, avg. loss: 1.058439, running train acc: 0.707
==>>> it: 8601, mem avg. loss: 0.859905, running mem acc: 0.754
==>>> it: 8701, avg. loss: 1.049066, running train acc: 0.710
==>>> it: 8701, mem avg. loss: 0.851104, running mem acc: 0.757
==>>> it: 8801, avg. loss: 1.040704, running train acc: 0.712
==>>> it: 8801, mem avg. loss: 0.842654, running mem acc: 0.759
==>>> it: 8901, avg. loss: 1.031358, running train acc: 0.714
==>>> it: 8901, mem avg. loss: 0.833948, running mem acc: 0.762
==>>> it: 9001, avg. loss: 1.022966, running train acc: 0.716
==>>> it: 9001, mem avg. loss: 0.825878, running mem acc: 0.764
==>>> it: 9101, avg. loss: 1.014119, running train acc: 0.719
==>>> it: 9101, mem avg. loss: 0.817760, running mem acc: 0.767
==>>> it: 9201, avg. loss: 1.006167, running train acc: 0.721
==>>> it: 9201, mem avg. loss: 0.809930, running mem acc: 0.769
==>>> it: 9301, avg. loss: 0.997334, running train acc: 0.723
==>>> it: 9301, mem avg. loss: 0.802114, running mem acc: 0.771
==>>> it: 9401, avg. loss: 0.988607, running train acc: 0.725
==>>> it: 9401, mem avg. loss: 0.794309, running mem acc: 0.773
==>>> it: 9501, avg. loss: 0.980002, running train acc: 0.728
==>>> it: 9501, mem avg. loss: 0.786569, running mem acc: 0.776
==>>> it: 9601, avg. loss: 0.972146, running train acc: 0.730
==>>> it: 9601, mem avg. loss: 0.779171, running mem acc: 0.778
==>>> it: 9701, avg. loss: 0.964034, running train acc: 0.732
==>>> it: 9701, mem avg. loss: 0.771965, running mem acc: 0.780
==>>> it: 9801, avg. loss: 0.956111, running train acc: 0.734
==>>> it: 9801, mem avg. loss: 0.764767, running mem acc: 0.782
==>>> it: 9901, avg. loss: 0.949044, running train acc: 0.736
==>>> it: 9901, mem avg. loss: 0.757945, running mem acc: 0.784
==>>> it: 10001, avg. loss: 0.941305, running train acc: 0.738
==>>> it: 10001, mem avg. loss: 0.750909, running mem acc: 0.786
==>>> it: 10101, avg. loss: 0.934376, running train acc: 0.740
==>>> it: 10101, mem avg. loss: 0.744324, running mem acc: 0.788
==>>> it: 10201, avg. loss: 0.926659, running train acc: 0.742
==>>> it: 10201, mem avg. loss: 0.737561, running mem acc: 0.789
==>>> it: 10301, avg. loss: 0.919887, running train acc: 0.744
==>>> it: 10301, mem avg. loss: 0.731256, running mem acc: 0.791
==>>> it: 10401, avg. loss: 0.912991, running train acc: 0.746
==>>> it: 10401, mem avg. loss: 0.724776, running mem acc: 0.793
==>>> it: 10501, avg. loss: 0.906130, running train acc: 0.748
==>>> it: 10501, mem avg. loss: 0.718473, running mem acc: 0.795
==>>> it: 10601, avg. loss: 0.899095, running train acc: 0.749
==>>> it: 10601, mem avg. loss: 0.712165, running mem acc: 0.797
==>>> it: 10701, avg. loss: 0.891999, running train acc: 0.751
==>>> it: 10701, mem avg. loss: 0.705951, running mem acc: 0.799
[0.35357556]
no ratio: 0.0
on ratio: 0.0
[(0, 29071, 0, 0)]
[0.027644412501762423]
[0]
[nan]
[4.049139897688292e-05]
[nan]
[-0.000872654898557812]
Loading test set...
buffer has 5000 slots
Loading data...
loading time 5.117130994796753
Loading data...
loading time 5.398241281509399
Loading data...
loading time 5.014197826385498
Loading data...
loading time 4.799806118011475
Loading data...
loading time 4.921010494232178
Loading data...
loading time 5.180999279022217
Loading data...
loading time 5.253671646118164
Loading data...
loading time 5.257310390472412
Training Start
----------run 3 training-------------
size: (107909, 128, 128, 3), (107909,)
==>>> it: 1, avg. loss: 19.453156, running train acc: 0.000
==>>> it: 1, mem avg. loss: 14.976027, running mem acc: 0.200
==>>> it: 101, avg. loss: 5.270624, running train acc: 0.029
==>>> it: 101, mem avg. loss: 3.032600, running mem acc: 0.352
==>>> it: 201, avg. loss: 4.596574, running train acc: 0.048
==>>> it: 201, mem avg. loss: 2.840586, running mem acc: 0.365
==>>> it: 301, avg. loss: 4.330289, running train acc: 0.061
==>>> it: 301, mem avg. loss: 2.771245, running mem acc: 0.364
==>>> it: 401, avg. loss: 4.153005, running train acc: 0.072
==>>> it: 401, mem avg. loss: 2.749843, running mem acc: 0.359
==>>> it: 501, avg. loss: 4.036408, running train acc: 0.078
==>>> it: 501, mem avg. loss: 2.723391, running mem acc: 0.356
==>>> it: 601, avg. loss: 3.940981, running train acc: 0.088
==>>> it: 601, mem avg. loss: 2.717046, running mem acc: 0.349
==>>> it: 701, avg. loss: 3.869368, running train acc: 0.096
==>>> it: 701, mem avg. loss: 2.688509, running mem acc: 0.349
==>>> it: 801, avg. loss: 3.779123, running train acc: 0.107
==>>> it: 801, mem avg. loss: 2.678474, running mem acc: 0.346
==>>> it: 901, avg. loss: 3.691521, running train acc: 0.121
==>>> it: 901, mem avg. loss: 2.655912, running mem acc: 0.346
==>>> it: 1001, avg. loss: 3.605391, running train acc: 0.133
==>>> it: 1001, mem avg. loss: 2.639682, running mem acc: 0.343
==>>> it: 1101, avg. loss: 3.518829, running train acc: 0.147
==>>> it: 1101, mem avg. loss: 2.613883, running mem acc: 0.344
==>>> it: 1201, avg. loss: 3.436861, running train acc: 0.160
==>>> it: 1201, mem avg. loss: 2.583144, running mem acc: 0.346
==>>> it: 1301, avg. loss: 3.353114, running train acc: 0.175
==>>> it: 1301, mem avg. loss: 2.553595, running mem acc: 0.346
==>>> it: 1401, avg. loss: 3.270754, running train acc: 0.189
==>>> it: 1401, mem avg. loss: 2.519116, running mem acc: 0.351
==>>> it: 1501, avg. loss: 3.199545, running train acc: 0.203
==>>> it: 1501, mem avg. loss: 2.481625, running mem acc: 0.356
==>>> it: 1601, avg. loss: 3.125634, running train acc: 0.218
==>>> it: 1601, mem avg. loss: 2.449046, running mem acc: 0.361
==>>> it: 1701, avg. loss: 3.049382, running train acc: 0.232
==>>> it: 1701, mem avg. loss: 2.413107, running mem acc: 0.366
==>>> it: 1801, avg. loss: 2.980870, running train acc: 0.245
==>>> it: 1801, mem avg. loss: 2.375557, running mem acc: 0.373
==>>> it: 1901, avg. loss: 2.919877, running train acc: 0.257
==>>> it: 1901, mem avg. loss: 2.341118, running mem acc: 0.378
==>>> it: 2001, avg. loss: 2.856103, running train acc: 0.270
==>>> it: 2001, mem avg. loss: 2.304768, running mem acc: 0.385
==>>> it: 2101, avg. loss: 2.789546, running train acc: 0.283
==>>> it: 2101, mem avg. loss: 2.263547, running mem acc: 0.393
==>>> it: 2201, avg. loss: 2.728608, running train acc: 0.297
==>>> it: 2201, mem avg. loss: 2.226298, running mem acc: 0.400
==>>> it: 2301, avg. loss: 2.670283, running train acc: 0.311
==>>> it: 2301, mem avg. loss: 2.192356, running mem acc: 0.406
==>>> it: 2401, avg. loss: 2.610112, running train acc: 0.324
==>>> it: 2401, mem avg. loss: 2.156129, running mem acc: 0.413
==>>> it: 2501, avg. loss: 2.554729, running train acc: 0.336
==>>> it: 2501, mem avg. loss: 2.120124, running mem acc: 0.421
==>>> it: 2601, avg. loss: 2.498393, running train acc: 0.349
==>>> it: 2601, mem avg. loss: 2.081700, running mem acc: 0.429
==>>> it: 2701, avg. loss: 2.445775, running train acc: 0.362
==>>> it: 2701, mem avg. loss: 2.045812, running mem acc: 0.438
==>>> it: 2801, avg. loss: 2.395587, running train acc: 0.373
==>>> it: 2801, mem avg. loss: 2.008967, running mem acc: 0.446
==>>> it: 2901, avg. loss: 2.346884, running train acc: 0.384
==>>> it: 2901, mem avg. loss: 1.970742, running mem acc: 0.456
==>>> it: 3001, avg. loss: 2.300664, running train acc: 0.395
==>>> it: 3001, mem avg. loss: 1.934996, running mem acc: 0.465
==>>> it: 3101, avg. loss: 2.253432, running train acc: 0.406
==>>> it: 3101, mem avg. loss: 1.901426, running mem acc: 0.472
==>>> it: 3201, avg. loss: 2.212465, running train acc: 0.416
==>>> it: 3201, mem avg. loss: 1.865275, running mem acc: 0.481
==>>> it: 3301, avg. loss: 2.172126, running train acc: 0.426
==>>> it: 3301, mem avg. loss: 1.829638, running mem acc: 0.490
==>>> it: 3401, avg. loss: 2.130546, running train acc: 0.436
==>>> it: 3401, mem avg. loss: 1.796250, running mem acc: 0.499
==>>> it: 3501, avg. loss: 2.091951, running train acc: 0.446
==>>> it: 3501, mem avg. loss: 1.763069, running mem acc: 0.508
==>>> it: 3601, avg. loss: 2.053806, running train acc: 0.455
==>>> it: 3601, mem avg. loss: 1.731765, running mem acc: 0.516
==>>> it: 3701, avg. loss: 2.016107, running train acc: 0.465
==>>> it: 3701, mem avg. loss: 1.701288, running mem acc: 0.524
==>>> it: 3801, avg. loss: 1.982995, running train acc: 0.473
==>>> it: 3801, mem avg. loss: 1.670390, running mem acc: 0.532
==>>> it: 3901, avg. loss: 1.949889, running train acc: 0.481
==>>> it: 3901, mem avg. loss: 1.640843, running mem acc: 0.540
==>>> it: 4001, avg. loss: 1.914925, running train acc: 0.490
==>>> it: 4001, mem avg. loss: 1.612709, running mem acc: 0.547
==>>> it: 4101, avg. loss: 1.883207, running train acc: 0.498
==>>> it: 4101, mem avg. loss: 1.584202, running mem acc: 0.555
==>>> it: 4201, avg. loss: 1.853613, running train acc: 0.506
==>>> it: 4201, mem avg. loss: 1.557912, running mem acc: 0.562
==>>> it: 4301, avg. loss: 1.825334, running train acc: 0.513
==>>> it: 4301, mem avg. loss: 1.531897, running mem acc: 0.569
==>>> it: 4401, avg. loss: 1.796684, running train acc: 0.520
==>>> it: 4401, mem avg. loss: 1.505497, running mem acc: 0.576
==>>> it: 4501, avg. loss: 1.768665, running train acc: 0.527
==>>> it: 4501, mem avg. loss: 1.479697, running mem acc: 0.583
==>>> it: 4601, avg. loss: 1.741732, running train acc: 0.534
==>>> it: 4601, mem avg. loss: 1.454251, running mem acc: 0.590
==>>> it: 4701, avg. loss: 1.715200, running train acc: 0.540
==>>> it: 4701, mem avg. loss: 1.429697, running mem acc: 0.597
==>>> it: 4801, avg. loss: 1.689702, running train acc: 0.547
==>>> it: 4801, mem avg. loss: 1.405606, running mem acc: 0.604
==>>> it: 4901, avg. loss: 1.665970, running train acc: 0.553
==>>> it: 4901, mem avg. loss: 1.382313, running mem acc: 0.611
==>>> it: 5001, avg. loss: 1.641436, running train acc: 0.559
==>>> it: 5001, mem avg. loss: 1.359938, running mem acc: 0.617
==>>> it: 5101, avg. loss: 1.620496, running train acc: 0.564
==>>> it: 5101, mem avg. loss: 1.338886, running mem acc: 0.623
==>>> it: 5201, avg. loss: 1.598910, running train acc: 0.570
==>>> it: 5201, mem avg. loss: 1.318060, running mem acc: 0.629
==>>> it: 5301, avg. loss: 1.577019, running train acc: 0.575
==>>> it: 5301, mem avg. loss: 1.297265, running mem acc: 0.635
==>>> it: 5401, avg. loss: 1.554681, running train acc: 0.581
==>>> it: 5401, mem avg. loss: 1.277681, running mem acc: 0.640
==>>> it: 5501, avg. loss: 1.533800, running train acc: 0.586
==>>> it: 5501, mem avg. loss: 1.259167, running mem acc: 0.645
==>>> it: 5601, avg. loss: 1.513661, running train acc: 0.592
==>>> it: 5601, mem avg. loss: 1.240272, running mem acc: 0.650
==>>> it: 5701, avg. loss: 1.494207, running train acc: 0.596
==>>> it: 5701, mem avg. loss: 1.222054, running mem acc: 0.655
==>>> it: 5801, avg. loss: 1.475065, running train acc: 0.601
==>>> it: 5801, mem avg. loss: 1.204550, running mem acc: 0.660
==>>> it: 5901, avg. loss: 1.456894, running train acc: 0.606
==>>> it: 5901, mem avg. loss: 1.187135, running mem acc: 0.665
==>>> it: 6001, avg. loss: 1.440311, running train acc: 0.611
==>>> it: 6001, mem avg. loss: 1.170810, running mem acc: 0.670
==>>> it: 6101, avg. loss: 1.422084, running train acc: 0.615
==>>> it: 6101, mem avg. loss: 1.154260, running mem acc: 0.675
==>>> it: 6201, avg. loss: 1.404854, running train acc: 0.619
==>>> it: 6201, mem avg. loss: 1.138142, running mem acc: 0.679
==>>> it: 6301, avg. loss: 1.386943, running train acc: 0.624
==>>> it: 6301, mem avg. loss: 1.122185, running mem acc: 0.684
==>>> it: 6401, avg. loss: 1.370558, running train acc: 0.628
==>>> it: 6401, mem avg. loss: 1.106783, running mem acc: 0.688
==>>> it: 6501, avg. loss: 1.354900, running train acc: 0.633
==>>> it: 6501, mem avg. loss: 1.091953, running mem acc: 0.692
==>>> it: 6601, avg. loss: 1.340426, running train acc: 0.636
==>>> it: 6601, mem avg. loss: 1.077673, running mem acc: 0.696
==>>> it: 6701, avg. loss: 1.325845, running train acc: 0.640
==>>> it: 6701, mem avg. loss: 1.063951, running mem acc: 0.700
==>>> it: 6801, avg. loss: 1.312002, running train acc: 0.644
==>>> it: 6801, mem avg. loss: 1.050360, running mem acc: 0.704
==>>> it: 6901, avg. loss: 1.298724, running train acc: 0.647
==>>> it: 6901, mem avg. loss: 1.036815, running mem acc: 0.708
==>>> it: 7001, avg. loss: 1.284838, running train acc: 0.651
==>>> it: 7001, mem avg. loss: 1.024209, running mem acc: 0.711
==>>> it: 7101, avg. loss: 1.270831, running train acc: 0.654
==>>> it: 7101, mem avg. loss: 1.011504, running mem acc: 0.715
==>>> it: 7201, avg. loss: 1.256916, running train acc: 0.658
==>>> it: 7201, mem avg. loss: 0.998993, running mem acc: 0.719
==>>> it: 7301, avg. loss: 1.243562, running train acc: 0.661
==>>> it: 7301, mem avg. loss: 0.987044, running mem acc: 0.722
==>>> it: 7401, avg. loss: 1.230305, running train acc: 0.665
==>>> it: 7401, mem avg. loss: 0.975130, running mem acc: 0.725
==>>> it: 7501, avg. loss: 1.217068, running train acc: 0.668
==>>> it: 7501, mem avg. loss: 0.963516, running mem acc: 0.729
==>>> it: 7601, avg. loss: 1.204270, running train acc: 0.672
==>>> it: 7601, mem avg. loss: 0.951880, running mem acc: 0.732
==>>> it: 7701, avg. loss: 1.192001, running train acc: 0.675
==>>> it: 7701, mem avg. loss: 0.941090, running mem acc: 0.735
==>>> it: 7801, avg. loss: 1.180017, running train acc: 0.678
==>>> it: 7801, mem avg. loss: 0.930523, running mem acc: 0.738
==>>> it: 7901, avg. loss: 1.168522, running train acc: 0.681
==>>> it: 7901, mem avg. loss: 0.920142, running mem acc: 0.741
==>>> it: 8001, avg. loss: 1.157207, running train acc: 0.684
==>>> it: 8001, mem avg. loss: 0.909982, running mem acc: 0.744
==>>> it: 8101, avg. loss: 1.145815, running train acc: 0.687
==>>> it: 8101, mem avg. loss: 0.899911, running mem acc: 0.747
==>>> it: 8201, avg. loss: 1.134525, running train acc: 0.690
==>>> it: 8201, mem avg. loss: 0.890112, running mem acc: 0.749
==>>> it: 8301, avg. loss: 1.123164, running train acc: 0.693
==>>> it: 8301, mem avg. loss: 0.880204, running mem acc: 0.752
==>>> it: 8401, avg. loss: 1.112656, running train acc: 0.696
==>>> it: 8401, mem avg. loss: 0.870835, running mem acc: 0.755
==>>> it: 8501, avg. loss: 1.102619, running train acc: 0.699
==>>> it: 8501, mem avg. loss: 0.861540, running mem acc: 0.757
==>>> it: 8601, avg. loss: 1.092311, running train acc: 0.701
==>>> it: 8601, mem avg. loss: 0.853026, running mem acc: 0.760
==>>> it: 8701, avg. loss: 1.082870, running train acc: 0.704
==>>> it: 8701, mem avg. loss: 0.844290, running mem acc: 0.762
==>>> it: 8801, avg. loss: 1.073044, running train acc: 0.706
==>>> it: 8801, mem avg. loss: 0.835774, running mem acc: 0.765
==>>> it: 8901, avg. loss: 1.063337, running train acc: 0.709
==>>> it: 8901, mem avg. loss: 0.827284, running mem acc: 0.767
==>>> it: 9001, avg. loss: 1.054106, running train acc: 0.711
==>>> it: 9001, mem avg. loss: 0.819100, running mem acc: 0.770
==>>> it: 9101, avg. loss: 1.044902, running train acc: 0.714
==>>> it: 9101, mem avg. loss: 0.810917, running mem acc: 0.772
==>>> it: 9201, avg. loss: 1.035909, running train acc: 0.716
==>>> it: 9201, mem avg. loss: 0.802965, running mem acc: 0.774
==>>> it: 9301, avg. loss: 1.026527, running train acc: 0.719
==>>> it: 9301, mem avg. loss: 0.795140, running mem acc: 0.776
==>>> it: 9401, avg. loss: 1.017155, running train acc: 0.721
==>>> it: 9401, mem avg. loss: 0.787463, running mem acc: 0.778
==>>> it: 9501, avg. loss: 1.008570, running train acc: 0.723
==>>> it: 9501, mem avg. loss: 0.779719, running mem acc: 0.781
==>>> it: 9601, avg. loss: 1.000133, running train acc: 0.726
==>>> it: 9601, mem avg. loss: 0.772627, running mem acc: 0.783
==>>> it: 9701, avg. loss: 0.991450, running train acc: 0.728
==>>> it: 9701, mem avg. loss: 0.765347, running mem acc: 0.785
==>>> it: 9801, avg. loss: 0.983113, running train acc: 0.730
==>>> it: 9801, mem avg. loss: 0.758098, running mem acc: 0.787
==>>> it: 9901, avg. loss: 0.975205, running train acc: 0.732
==>>> it: 9901, mem avg. loss: 0.751128, running mem acc: 0.789
==>>> it: 10001, avg. loss: 0.967348, running train acc: 0.734
==>>> it: 10001, mem avg. loss: 0.744097, running mem acc: 0.791
==>>> it: 10101, avg. loss: 0.959191, running train acc: 0.737
==>>> it: 10101, mem avg. loss: 0.737196, running mem acc: 0.793
==>>> it: 10201, avg. loss: 0.951472, running train acc: 0.739
==>>> it: 10201, mem avg. loss: 0.730545, running mem acc: 0.795
==>>> it: 10301, avg. loss: 0.944123, running train acc: 0.741
==>>> it: 10301, mem avg. loss: 0.724054, running mem acc: 0.797
==>>> it: 10401, avg. loss: 0.936851, running train acc: 0.743
==>>> it: 10401, mem avg. loss: 0.717711, running mem acc: 0.798
==>>> it: 10501, avg. loss: 0.929417, running train acc: 0.745
==>>> it: 10501, mem avg. loss: 0.711420, running mem acc: 0.800
==>>> it: 10601, avg. loss: 0.922439, running train acc: 0.746
==>>> it: 10601, mem avg. loss: 0.705213, running mem acc: 0.802
==>>> it: 10701, avg. loss: 0.915208, running train acc: 0.748
==>>> it: 10701, mem avg. loss: 0.699108, running mem acc: 0.804
[0.32077737]
no ratio: 0.0
on ratio: 0.0
[(0, 30546, 0, 0)]
[0.0370986531797989]
[0]
[nan]
[5.343644806998782e-05]
[nan]
[0.0027661072090268135]
Loading test set...
buffer has 5000 slots
Loading data...
loading time 5.24008846282959
Loading data...
loading time 5.225854873657227
Loading data...
loading time 5.405001640319824
Loading data...
loading time 4.908061742782593
Loading data...
loading time 5.306963205337524
Loading data...
loading time 5.466044902801514
Loading data...
loading time 5.0285727977752686
Loading data...
loading time 5.0818963050842285
Training Start
----------run 4 training-------------
size: (107909, 128, 128, 3), (107909,)
==>>> it: 1, avg. loss: 8.441302, running train acc: 0.050
==>>> it: 1, mem avg. loss: 2.363690, running mem acc: 0.300
==>>> it: 101, avg. loss: 5.251477, running train acc: 0.049
==>>> it: 101, mem avg. loss: 2.665745, running mem acc: 0.395
==>>> it: 201, avg. loss: 4.508681, running train acc: 0.063
==>>> it: 201, mem avg. loss: 2.599515, running mem acc: 0.374
==>>> it: 301, avg. loss: 4.172813, running train acc: 0.084
==>>> it: 301, mem avg. loss: 2.615062, running mem acc: 0.357
==>>> it: 401, avg. loss: 3.965551, running train acc: 0.096
==>>> it: 401, mem avg. loss: 2.584039, running mem acc: 0.356
==>>> it: 501, avg. loss: 3.795420, running train acc: 0.114
==>>> it: 501, mem avg. loss: 2.586716, running mem acc: 0.346
==>>> it: 601, avg. loss: 3.658774, running train acc: 0.130
==>>> it: 601, mem avg. loss: 2.584246, running mem acc: 0.340
==>>> it: 701, avg. loss: 3.531596, running train acc: 0.149
==>>> it: 701, mem avg. loss: 2.573438, running mem acc: 0.336
==>>> it: 801, avg. loss: 3.426569, running train acc: 0.164
==>>> it: 801, mem avg. loss: 2.544769, running mem acc: 0.340
==>>> it: 901, avg. loss: 3.323503, running train acc: 0.179
==>>> it: 901, mem avg. loss: 2.524366, running mem acc: 0.341
==>>> it: 1001, avg. loss: 3.221981, running train acc: 0.196
==>>> it: 1001, mem avg. loss: 2.494344, running mem acc: 0.342
==>>> it: 1101, avg. loss: 3.125147, running train acc: 0.212
==>>> it: 1101, mem avg. loss: 2.471924, running mem acc: 0.344
==>>> it: 1201, avg. loss: 3.039508, running train acc: 0.229
==>>> it: 1201, mem avg. loss: 2.436382, running mem acc: 0.350
==>>> it: 1301, avg. loss: 2.956539, running train acc: 0.245
==>>> it: 1301, mem avg. loss: 2.394779, running mem acc: 0.356
==>>> it: 1401, avg. loss: 2.874259, running train acc: 0.263
==>>> it: 1401, mem avg. loss: 2.357938, running mem acc: 0.362
==>>> it: 1501, avg. loss: 2.800395, running train acc: 0.278
==>>> it: 1501, mem avg. loss: 2.315979, running mem acc: 0.369
==>>> it: 1601, avg. loss: 2.728381, running train acc: 0.293
==>>> it: 1601, mem avg. loss: 2.276430, running mem acc: 0.377
==>>> it: 1701, avg. loss: 2.657062, running train acc: 0.309
==>>> it: 1701, mem avg. loss: 2.238695, running mem acc: 0.385
==>>> it: 1801, avg. loss: 2.594399, running train acc: 0.324
==>>> it: 1801, mem avg. loss: 2.198189, running mem acc: 0.393
==>>> it: 1901, avg. loss: 2.532823, running train acc: 0.338
==>>> it: 1901, mem avg. loss: 2.160228, running mem acc: 0.401
==>>> it: 2001, avg. loss: 2.476328, running train acc: 0.350
==>>> it: 2001, mem avg. loss: 2.121556, running mem acc: 0.410
==>>> it: 2101, avg. loss: 2.421603, running train acc: 0.362
==>>> it: 2101, mem avg. loss: 2.082704, running mem acc: 0.419
==>>> it: 2201, avg. loss: 2.364479, running train acc: 0.376
==>>> it: 2201, mem avg. loss: 2.047295, running mem acc: 0.427
==>>> it: 2301, avg. loss: 2.316552, running train acc: 0.387
==>>> it: 2301, mem avg. loss: 2.012310, running mem acc: 0.436
==>>> it: 2401, avg. loss: 2.268392, running train acc: 0.398
==>>> it: 2401, mem avg. loss: 1.980133, running mem acc: 0.442
==>>> it: 2501, avg. loss: 2.222423, running train acc: 0.408
==>>> it: 2501, mem avg. loss: 1.944328, running mem acc: 0.451
==>>> it: 2601, avg. loss: 2.175870, running train acc: 0.419
==>>> it: 2601, mem avg. loss: 1.910628, running mem acc: 0.459
==>>> it: 2701, avg. loss: 2.134361, running train acc: 0.430
==>>> it: 2701, mem avg. loss: 1.877584, running mem acc: 0.467
==>>> it: 2801, avg. loss: 2.092803, running train acc: 0.440
==>>> it: 2801, mem avg. loss: 1.843701, running mem acc: 0.476
==>>> it: 2901, avg. loss: 2.050410, running train acc: 0.450
==>>> it: 2901, mem avg. loss: 1.811353, running mem acc: 0.484
==>>> it: 3001, avg. loss: 2.013937, running train acc: 0.459
==>>> it: 3001, mem avg. loss: 1.778869, running mem acc: 0.493
==>>> it: 3101, avg. loss: 1.977041, running train acc: 0.468
==>>> it: 3101, mem avg. loss: 1.747145, running mem acc: 0.500
==>>> it: 3201, avg. loss: 1.941045, running train acc: 0.477
==>>> it: 3201, mem avg. loss: 1.717846, running mem acc: 0.508
==>>> it: 3301, avg. loss: 1.906531, running train acc: 0.486
==>>> it: 3301, mem avg. loss: 1.686823, running mem acc: 0.516
==>>> it: 3401, avg. loss: 1.873149, running train acc: 0.494
==>>> it: 3401, mem avg. loss: 1.657524, running mem acc: 0.524
==>>> it: 3501, avg. loss: 1.841832, running train acc: 0.501
==>>> it: 3501, mem avg. loss: 1.628311, running mem acc: 0.532
==>>> it: 3601, avg. loss: 1.809038, running train acc: 0.509
==>>> it: 3601, mem avg. loss: 1.597914, running mem acc: 0.540
==>>> it: 3701, avg. loss: 1.779508, running train acc: 0.517
==>>> it: 3701, mem avg. loss: 1.570554, running mem acc: 0.548
==>>> it: 3801, avg. loss: 1.750571, running train acc: 0.524
==>>> it: 3801, mem avg. loss: 1.542992, running mem acc: 0.555
==>>> it: 3901, avg. loss: 1.722452, running train acc: 0.531
==>>> it: 3901, mem avg. loss: 1.516347, running mem acc: 0.563
==>>> it: 4001, avg. loss: 1.695623, running train acc: 0.538
==>>> it: 4001, mem avg. loss: 1.489734, running mem acc: 0.570
==>>> it: 4101, avg. loss: 1.668583, running train acc: 0.545
==>>> it: 4101, mem avg. loss: 1.463522, running mem acc: 0.578
==>>> it: 4201, avg. loss: 1.643827, running train acc: 0.551
==>>> it: 4201, mem avg. loss: 1.439707, running mem acc: 0.585
==>>> it: 4301, avg. loss: 1.619204, running train acc: 0.557
==>>> it: 4301, mem avg. loss: 1.415906, running mem acc: 0.591
==>>> it: 4401, avg. loss: 1.595871, running train acc: 0.563
==>>> it: 4401, mem avg. loss: 1.392412, running mem acc: 0.598
==>>> it: 4501, avg. loss: 1.572169, running train acc: 0.569
==>>> it: 4501, mem avg. loss: 1.369920, running mem acc: 0.604
==>>> it: 4601, avg. loss: 1.549312, running train acc: 0.575
==>>> it: 4601, mem avg. loss: 1.348812, running mem acc: 0.610
==>>> it: 4701, avg. loss: 1.529459, running train acc: 0.580
==>>> it: 4701, mem avg. loss: 1.327304, running mem acc: 0.616
==>>> it: 4801, avg. loss: 1.507343, running train acc: 0.586
==>>> it: 4801, mem avg. loss: 1.305816, running mem acc: 0.622
==>>> it: 4901, avg. loss: 1.487323, running train acc: 0.591
==>>> it: 4901, mem avg. loss: 1.284852, running mem acc: 0.629
==>>> it: 5001, avg. loss: 1.466944, running train acc: 0.596
==>>> it: 5001, mem avg. loss: 1.265006, running mem acc: 0.634
==>>> it: 5101, avg. loss: 1.447792, running train acc: 0.601
==>>> it: 5101, mem avg. loss: 1.246035, running mem acc: 0.640
==>>> it: 5201, avg. loss: 1.428585, running train acc: 0.606
==>>> it: 5201, mem avg. loss: 1.227584, running mem acc: 0.645
==>>> it: 5301, avg. loss: 1.408073, running train acc: 0.612
==>>> it: 5301, mem avg. loss: 1.208781, running mem acc: 0.651
==>>> it: 5401, avg. loss: 1.389792, running train acc: 0.617
==>>> it: 5401, mem avg. loss: 1.190623, running mem acc: 0.656
==>>> it: 5501, avg. loss: 1.371572, running train acc: 0.621
==>>> it: 5501, mem avg. loss: 1.173438, running mem acc: 0.661
==>>> it: 5601, avg. loss: 1.354521, running train acc: 0.626
==>>> it: 5601, mem avg. loss: 1.156448, running mem acc: 0.665
==>>> it: 5701, avg. loss: 1.336973, running train acc: 0.630
==>>> it: 5701, mem avg. loss: 1.140240, running mem acc: 0.670
==>>> it: 5801, avg. loss: 1.320924, running train acc: 0.635
==>>> it: 5801, mem avg. loss: 1.123905, running mem acc: 0.675
==>>> it: 5901, avg. loss: 1.304955, running train acc: 0.639
==>>> it: 5901, mem avg. loss: 1.108150, running mem acc: 0.680
==>>> it: 6001, avg. loss: 1.289356, running train acc: 0.643
==>>> it: 6001, mem avg. loss: 1.093443, running mem acc: 0.684
==>>> it: 6101, avg. loss: 1.274362, running train acc: 0.647
==>>> it: 6101, mem avg. loss: 1.078612, running mem acc: 0.688
==>>> it: 6201, avg. loss: 1.259396, running train acc: 0.651
==>>> it: 6201, mem avg. loss: 1.063601, running mem acc: 0.693
==>>> it: 6301, avg. loss: 1.244110, running train acc: 0.655
==>>> it: 6301, mem avg. loss: 1.050034, running mem acc: 0.696
==>>> it: 6401, avg. loss: 1.230222, running train acc: 0.659
==>>> it: 6401, mem avg. loss: 1.036371, running mem acc: 0.700
==>>> it: 6501, avg. loss: 1.216782, running train acc: 0.662
==>>> it: 6501, mem avg. loss: 1.022797, running mem acc: 0.704
==>>> it: 6601, avg. loss: 1.203792, running train acc: 0.666
==>>> it: 6601, mem avg. loss: 1.010050, running mem acc: 0.708
==>>> it: 6701, avg. loss: 1.190309, running train acc: 0.670
==>>> it: 6701, mem avg. loss: 0.996823, running mem acc: 0.712
==>>> it: 6801, avg. loss: 1.177669, running train acc: 0.673
==>>> it: 6801, mem avg. loss: 0.984386, running mem acc: 0.715
==>>> it: 6901, avg. loss: 1.164976, running train acc: 0.676
==>>> it: 6901, mem avg. loss: 0.971959, running mem acc: 0.719
==>>> it: 7001, avg. loss: 1.152547, running train acc: 0.680
==>>> it: 7001, mem avg. loss: 0.960171, running mem acc: 0.722
==>>> it: 7101, avg. loss: 1.140829, running train acc: 0.683
==>>> it: 7101, mem avg. loss: 0.948543, running mem acc: 0.726
==>>> it: 7201, avg. loss: 1.129128, running train acc: 0.686
==>>> it: 7201, mem avg. loss: 0.937138, running mem acc: 0.729
==>>> it: 7301, avg. loss: 1.117552, running train acc: 0.689
==>>> it: 7301, mem avg. loss: 0.925811, running mem acc: 0.732
==>>> it: 7401, avg. loss: 1.105386, running train acc: 0.692
==>>> it: 7401, mem avg. loss: 0.914680, running mem acc: 0.735
==>>> it: 7501, avg. loss: 1.094529, running train acc: 0.695
==>>> it: 7501, mem avg. loss: 0.903786, running mem acc: 0.739
==>>> it: 7601, avg. loss: 1.083723, running train acc: 0.698
==>>> it: 7601, mem avg. loss: 0.893298, running mem acc: 0.742
==>>> it: 7701, avg. loss: 1.073372, running train acc: 0.701
==>>> it: 7701, mem avg. loss: 0.883378, running mem acc: 0.745
==>>> it: 7801, avg. loss: 1.062770, running train acc: 0.704
==>>> it: 7801, mem avg. loss: 0.873355, running mem acc: 0.748
==>>> it: 7901, avg. loss: 1.052676, running train acc: 0.706
==>>> it: 7901, mem avg. loss: 0.863508, running mem acc: 0.750
==>>> it: 8001, avg. loss: 1.042520, running train acc: 0.709
==>>> it: 8001, mem avg. loss: 0.853842, running mem acc: 0.753
==>>> it: 8101, avg. loss: 1.032328, running train acc: 0.712
==>>> it: 8101, mem avg. loss: 0.844570, running mem acc: 0.756
==>>> it: 8201, avg. loss: 1.023010, running train acc: 0.715
==>>> it: 8201, mem avg. loss: 0.835867, running mem acc: 0.758
==>>> it: 8301, avg. loss: 1.013229, running train acc: 0.717
==>>> it: 8301, mem avg. loss: 0.826569, running mem acc: 0.761
==>>> it: 8401, avg. loss: 1.004243, running train acc: 0.720
==>>> it: 8401, mem avg. loss: 0.817887, running mem acc: 0.764
==>>> it: 8501, avg. loss: 0.995271, running train acc: 0.722
==>>> it: 8501, mem avg. loss: 0.809313, running mem acc: 0.766
==>>> it: 8601, avg. loss: 0.986666, running train acc: 0.724
==>>> it: 8601, mem avg. loss: 0.800814, running mem acc: 0.769
==>>> it: 8701, avg. loss: 0.977696, running train acc: 0.727
==>>> it: 8701, mem avg. loss: 0.792518, running mem acc: 0.771
==>>> it: 8801, avg. loss: 0.969227, running train acc: 0.729
==>>> it: 8801, mem avg. loss: 0.784376, running mem acc: 0.774
==>>> it: 8901, avg. loss: 0.960241, running train acc: 0.732
==>>> it: 8901, mem avg. loss: 0.776280, running mem acc: 0.776
==>>> it: 9001, avg. loss: 0.951707, running train acc: 0.734
==>>> it: 9001, mem avg. loss: 0.768561, running mem acc: 0.778
==>>> it: 9101, avg. loss: 0.943322, running train acc: 0.736
==>>> it: 9101, mem avg. loss: 0.761020, running mem acc: 0.780
==>>> it: 9201, avg. loss: 0.935462, running train acc: 0.738
==>>> it: 9201, mem avg. loss: 0.753590, running mem acc: 0.782
==>>> it: 9301, avg. loss: 0.927525, running train acc: 0.740
==>>> it: 9301, mem avg. loss: 0.746248, running mem acc: 0.785
==>>> it: 9401, avg. loss: 0.920124, running train acc: 0.742
==>>> it: 9401, mem avg. loss: 0.739185, running mem acc: 0.787
==>>> it: 9501, avg. loss: 0.912265, running train acc: 0.744
==>>> it: 9501, mem avg. loss: 0.732183, running mem acc: 0.789
==>>> it: 9601, avg. loss: 0.904414, running train acc: 0.747
==>>> it: 9601, mem avg. loss: 0.725508, running mem acc: 0.791
==>>> it: 9701, avg. loss: 0.897252, running train acc: 0.749
==>>> it: 9701, mem avg. loss: 0.718713, running mem acc: 0.792
==>>> it: 9801, avg. loss: 0.889553, running train acc: 0.751
==>>> it: 9801, mem avg. loss: 0.711942, running mem acc: 0.794
==>>> it: 9901, avg. loss: 0.882876, running train acc: 0.752
==>>> it: 9901, mem avg. loss: 0.705445, running mem acc: 0.796
==>>> it: 10001, avg. loss: 0.875512, running train acc: 0.754
==>>> it: 10001, mem avg. loss: 0.699053, running mem acc: 0.798
==>>> it: 10101, avg. loss: 0.868782, running train acc: 0.756
==>>> it: 10101, mem avg. loss: 0.692942, running mem acc: 0.800
==>>> it: 10201, avg. loss: 0.861799, running train acc: 0.758
==>>> it: 10201, mem avg. loss: 0.686616, running mem acc: 0.802
==>>> it: 10301, avg. loss: 0.855451, running train acc: 0.760
==>>> it: 10301, mem avg. loss: 0.680559, running mem acc: 0.804
==>>> it: 10401, avg. loss: 0.848886, running train acc: 0.762
==>>> it: 10401, mem avg. loss: 0.674413, running mem acc: 0.805
==>>> it: 10501, avg. loss: 0.842534, running train acc: 0.764
==>>> it: 10501, mem avg. loss: 0.668434, running mem acc: 0.807
==>>> it: 10601, avg. loss: 0.836081, running train acc: 0.765
==>>> it: 10601, mem avg. loss: 0.662622, running mem acc: 0.809
==>>> it: 10701, avg. loss: 0.829783, running train acc: 0.767
==>>> it: 10701, mem avg. loss: 0.656896, running mem acc: 0.811
[0.30403362]
no ratio: 0.0
on ratio: 0.0
[(0, 31299, 0, 0)]
[0.005702209571262399]
[0]
[nan]
[1.9094757590210065e-05]
[nan]
[0.0005909538012929261]
----------- Total 5 run: 5528.305893421173s -----------
avg_end_acc 0.3314995997509561
